<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CodebuddyPersistentProjectState">
    <option name="additionalOptionsVisible" value="true" />
    <option name="autoFileSelection" value="true" />
    <option name="promptHistory">
      <list>
        <option value="ok i want to set up the current project to be a node web app using the files i already created" />
        <option value="yes" />
        <option value="ry again with the files ive given you" />
        <option value="ok in the llm directory, i want to install ollama" />
        <option value="what does ollama.js do" />
        <option value="wats the best way to integrate ollama into this current project" />
        <option value="youre allowed to creat files if needed" />
        <option value="llama2 is installed" />
        <option value="how do i modify the llama to act as a nasa instructor" />
        <option value="do i need to run the ollama.js as well?" />
        <option value="how do i start it then?" />
        <option value="ive made changes to the port used" />
        <option value="Error: listen tcp 127.0.0.1:11434: bind: address already in use&#10;" />
        <option value="whats wrong with my code" />
        <option value="please actually look at the codebase, something is wrong" />
        <option value="please document y code thoruohly to explain everythings use" />
        <option value="i didt ask you to touch the style sheet, i just want mroe comments" />
        <option value="anything wrong with my code?" />
        <option value="Chat error: Error: Failed to get response from Ollama: Request failed with status code 500&#10;    at OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:104:19)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:70:26&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed: &#10;pulling manifest &#10;pulling 8934d96d3f08... 100% ▕████████████████▏ 3.8 GB                         &#10;pulling 8c17c2ebb0ea... 100% ▕████████████████▏ 7.0 KB                         &#10;pulling 7c23fb36d801... 100% ▕████████████████▏ 4.8 KB                         &#10;pulling 2e0493f67d0c... 100% ▕████████████████▏   59 B                         &#10;pulling fa304d675061... 100% ▕████████████████▏   91 B                         &#10;pulling 42ba7f8a01dd... 100% ▕████████████████▏  557 B                         &#10;verifying sha256 digest &#10;writing manifest &#10;success &#10;&#10;Ollama service initialization complete&#10;Server initialization complete, ready to handle requests&#10;Server is running on http://localhost:3000&#10;[68hgjd] Starting chat request&#10;[68hgjd] Ollama chat error: {&#10;  message: 'Request failed with status code 500',&#10;  status: 500,&#10;  data: {&#10;    error: 'model requires more system memory (8.4 GiB) than is available (8.3 GiB)'&#10;  }&#10;}&#10;Chat error: Error: Ollama server error. The model might be overloaded or experiencing issues.&#10;    at OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:133:23)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:92:26&#10;[bngu8o] Starting chat request&#10;[bngu8o] Ollama chat error: { message: '', status: undefined, data: undefined }&#10;Chat error: Error: Could not connect to Ollama. Please ensure Ollama is running.&#10;    at OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:131:23)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:92:26&#10;&#10;&#10;" />
        <option value="what if i want to add lanceDB to this project" />
        <option value="/usr/bin/npm install&#10;npm error code E404&#10;npm error 404 Not Found - GET https://registry.npmjs.org/lance-db - Not found&#10;npm error 404&#10;npm error 404  'lance-db@^0.5.0' is not in this registry.&#10;npm error 404&#10;npm error 404 Note that you can also install from a&#10;npm error 404 tarball, folder, http url, or git url.&#10;npm error A complete log of this run can be found in: /home/barthmalemew/.npm/_logs/2024-11-02T20_05_43_695Z-debug-0.log&#10;&#10;Process finished with exit code 1&#10;" />
        <option value="but i want lance db" />
        <option value="/usr/bin/npm install&#10;npm error code ETARGET&#10;npm error notarget No matching version found for vectorizer@^1.2.0.&#10;npm error notarget In most cases you or one of your dependencies are requesting&#10;npm error notarget a package version that doesn't exist.&#10;npm error A complete log of this run can be found in: /home/barthmalemew/.npm/_logs/2024-11-02T20_08_30_028Z-debug-0.log&#10;&#10;Process finished with exit code 1&#10;" />
        <option value="cool, so ideally i want to store question in the db for the LLM to access, these will serve as the questions the LLM has with the user" />
        <option value="is anything redundat in my code" />
        <option value="I want the LLM itself to interpret if the answer is correct, like a teacher, and point our possible mistakes the user made" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed: &#10;pulling manifest &#10;pulling 8934d96d3f08... 100% ▕████████████████▏ 3.8 GB                         &#10;pulling 8c17c2ebb0ea... 100% ▕████████████████▏ 7.0 KB                         &#10;pulling 7c23fb36d801... 100% ▕████████████████▏ 4.8 KB                         &#10;pulling 2e0493f67d0c... 100% ▕████████████████▏   59 B                         &#10;pulling fa304d675061... 100% ▕████████████████▏   91 B                         &#10;pulling 42ba7f8a01dd... 100% ▕████████████████▏  557 B                         &#10;verifying sha256 digest &#10;writing manifest &#10;success &#10;&#10;Ollama service initialization complete&#10;Database initialization failed: Error: At least one record or a schema needs to be provided&#10;    at makeArrowTable (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/@lancedb/lancedb/dist/arrow.js:298:15)&#10;    at Connection.createTable (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/@lancedb/lancedb/dist/connection.js:116:48)&#10;    at DatabaseService.initialize (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/database.js:30:44)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:42:9)&#10;Server initialization failed: AppError: Database initialization failed&#10;    at DatabaseService.initialize (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/database.js:54:19)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:42:9) {&#10;  statusCode: 500,&#10;  status: 'error'&#10;}&#10;&#10;Process finished with exit code 1&#10;&#10;" />
        <option value="yes we need to give the database data" />
        <option value="how do i put data in the data.db" />
        <option value="cant i just manually enter data into the file?" />
        <option value="doesnt data bricks have tool to do this easier?" />
        <option value="please remove all code and files that arent needed for the project, like the insertsion script" />
        <option value="please remove files that arent usful" />
        <option value="are those all really needed?" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Server initialization failed: TypeError: ollamaService.initialize is not a function&#10;    at initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:17:29)&#10;    at Object.&lt;anonymous&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:50:1)&#10;    at Module._compile (node:internal/modules/cjs/loader:1469:14)&#10;    at Module._extensions..js (node:internal/modules/cjs/loader:1548:10)&#10;    at Module.load (node:internal/modules/cjs/loader:1288:32)&#10;    at Module._load (node:internal/modules/cjs/loader:1104:12)&#10;    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:174:12)&#10;    at node:internal/main/run_main_module:28:49&#10;&#10;Process finished with exit code 1&#10;&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Initializing Ollama service...&#10;Ollama initialization failed: Model llama2 not found in Ollama server&#10;Server initialization failed: Error: Failed to initialize Ollama service: Model llama2 not found in Ollama server&#10;    at OllamaService.initialize (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:32:19)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:26:9)&#10;Failed to initialize server: Error: Failed to initialize Ollama service: Model llama2 not found in Ollama server&#10;    at OllamaService.initialize (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:32:19)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:26:9)&#10;&#10;Process finished with exit code 1&#10;&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Initializing Ollama service...&#10;Model llama2 not found. Attempting to pull...&#10;Pulling model llama2...&#10;Successfully pulled model llama2&#10;Ollama service initialized with model: llama2&#10;Server initialization complete, ready to handle requests&#10;Server is running on http://localhost:3000&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:59:30)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:51:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '612',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '612',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [Timeout],&#10;        _idleStart: 6892,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 137,&#10;        [Symbol(triggerId)]: 135&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 612\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: false,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 612,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 20:48:44 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 0,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '612',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 612\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (7.8 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:59:30)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:51:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '612',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '612',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [TimersList],&#10;        _idleStart: 24960,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 175,&#10;        [Symbol(triggerId)]: 173&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 612\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: false,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 612,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 20:49:02 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 0,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '612',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 612\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (8.0 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;&#10;" />
        <option value="woah wtf why are pulling orca mini???" />
        <option value="      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '612',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '612',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [TimersList],&#10;        _idleStart: 49525,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 199,&#10;        [Symbol(triggerId)]: 197&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 612\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: false,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 612,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      _timeout: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 20:54:05 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 60000,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '612',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 612\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (7.1 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;Chat error: Error: Failed to get response from Ollama: Request failed with status code 500&#10;    at OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:106:19)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:60:26&#10;&#10;" />
        <option value="no utility files, they were needed before" />
        <option value="can you make sure llama2 process ends when the server stops?" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;This may take several minutes...&#10;Model llama2 pull completed: &#10;pulling manifest &#10;pulling 8934d96d3f08... 100% ▕████████████████▏ 3.8 GB                         &#10;pulling 8c17c2ebb0ea... 100% ▕████████████████▏ 7.0 KB                         &#10;pulling 7c23fb36d801... 100% ▕████████████████▏ 4.8 KB                         &#10;pulling 2e0493f67d0c... 100% ▕████████████████▏   59 B                         &#10;pulling fa304d675061... 100% ▕████████████████▏   91 B                         &#10;pulling 42ba7f8a01dd... 100% ▕████████████████▏  557 B                         &#10;verifying sha256 digest &#10;writing manifest &#10;success &#10;&#10;Successfully initialized with model: llama2&#10;Ollama service initialization complete&#10;Server initialization complete, ready to handle requests&#10;Server is running on http://localhost:3000&#10;Ollama chat error: AxiosError: socket hang up&#10;    at AxiosError.from (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:876:14)&#10;    at RedirectableRequest.handleRequestError (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3156:25)&#10;    at RedirectableRequest.emit (node:events:531:35)&#10;    at eventHandlers.&lt;computed&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/follow-redirects/index.js:49:24)&#10;    at ClientRequest.emit (node:events:519:28)&#10;    at emitErrorEvent (node:_http_client:108:11)&#10;    at Socket.socketOnEnd (node:_http_client:535:5)&#10;    at Socket.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:87:30)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:61:26 {&#10;  code: 'ECONNRESET',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 60000,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '612',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;  },&#10;  request: &lt;ref *1&gt; Writable {&#10;    _events: {&#10;      close: undefined,&#10;      error: [Function: handleRequestError],&#10;      prefinish: undefined,&#10;      finish: undefined,&#10;      drain: undefined,&#10;      response: [Function: handleResponse],&#10;      socket: [Array],&#10;      timeout: undefined,&#10;      abort: undefined&#10;    },&#10;    _writableState: WritableState {&#10;      highWaterMark: 16384,&#10;      length: 0,&#10;      corked: 0,&#10;      onwrite: [Function: bound onwrite],&#10;      writelen: 0,&#10;      bufferedIndex: 0,&#10;      pendingcb: 0,&#10;      [Symbol(kState)]: 17580812,&#10;      [Symbol(kBufferedValue)]: null&#10;    },&#10;    _maxListeners: undefined,&#10;    _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/api/generate',&#10;      method: 'POST',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '11434',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/api/generate'&#10;    },&#10;    _ended: true,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 612,&#10;    _requestBodyBuffers: [ [Object] ],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '612',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 612\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: true,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:11434/api/generate',&#10;    _timeout: null,&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: Error: socket hang up&#10;      at Socket.socketOnEnd (node:_http_client:535:25)&#10;      at Socket.emit (node:events:531:35)&#10;      at endReadableNT (node:internal/streams/readable:1696:12)&#10;      at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {&#10;    code: 'ECONNRESET'&#10;  }&#10;}&#10;Chat error: Error: Failed to get response from Ollama: socket hang up&#10;    at OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:107:19)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:61:26&#10;&#10;" />
        <option value="no added files" />
        <option value="bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;node:internal/modules/cjs/loader:1228&#10;  throw err;&#10;  ^&#10;&#10;Error: Cannot find module './utils/connectionManager'&#10;Require stack:&#10;- /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;    at Module._resolveFilename (node:internal/modules/cjs/loader:1225:15)&#10;    at Module._load (node:internal/modules/cjs/loader:1051:27)&#10;    at Module.require (node:internal/modules/cjs/loader:1311:19)&#10;    at require (node:internal/modules/helpers:179:18)&#10;    at Object.&lt;anonymous&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:9:27)&#10;    at Module._compile (node:internal/modules/cjs/loader:1469:14)&#10;    at Module._extensions..js (node:internal/modules/cjs/loader:1548:10)&#10;    at Module.load (node:internal/modules/cjs/loader:1288:32)&#10;    at Module._load (node:internal/modules/cjs/loader:1104:12)&#10;    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:174:12) {&#10;  code: 'MODULE_NOT_FOUND',&#10;  requireStack: [ '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js' ]&#10;}&#10;&#10;Node.js v20.17.0&#10;&#10;Process finished with exit code 1&#10;&#10;" />
        <option value="no i just said no adding files" />
        <option value="no i said no new files, just use script.js ollma and server" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;This may take several minutes...&#10;Model llama2 pull completed: &#10;pulling manifest &#10;pulling 8934d96d3f08... 100% ▕████████████████▏ 3.8 GB                         &#10;pulling 8c17c2ebb0ea... 100% ▕████████████████▏ 7.0 KB                         &#10;pulling 7c23fb36d801... 100% ▕████████████████▏ 4.8 KB                         &#10;pulling 2e0493f67d0c... 100% ▕████████████████▏   59 B                         &#10;pulling fa304d675061... 100% ▕████████████████▏   91 B                         &#10;pulling 42ba7f8a01dd... 100% ▕████████████████▏  557 B                         &#10;verifying sha256 digest &#10;writing manifest &#10;success &#10;&#10;Successfully initialized with model: llama2&#10;Ollama service initialization complete&#10;Server initialization complete, ready to handle requests&#10;Server is running on http://localhost:3000&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '999',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [Timeout],&#10;        _idleStart: 7695,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 186,&#10;        [Symbol(triggerId)]: 184&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 999\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: false,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 999,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 21:11:57 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 0,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      signal: [AbortSignal],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (7.0 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;Retry attempt 1 after error: Failed to get response from Ollama: Request failed with status code 500&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '999',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [Timeout],&#10;        _idleStart: 8727,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 206,&#10;        [Symbol(triggerId)]: 204&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 999\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: true,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 999,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 21:11:58 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 0,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      signal: [AbortSignal],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: true,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (7.0 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;Retry attempt 2 after error: Failed to get response from Ollama: Request failed with status code 500&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '999',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [TimersList],&#10;        _idleStart: 10749,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 227,&#10;        [Symbol(triggerId)]: 225&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 999\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: true,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 999,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 21:12:00 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 0,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      signal: [AbortSignal],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: true,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (7.1 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;Chat error: Error: Failed to get response from Ollama: Request failed with status code 500&#10;    at OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:135:19)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '999',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [TimersList],&#10;        _idleStart: 19579,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 265,&#10;        [Symbol(triggerId)]: 263&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 999\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: false,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 999,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 21:12:09 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 0,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      signal: [AbortSignal],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (7.1 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;Retry attempt 1 after error: Failed to get response from Ollama: Request failed with status code 500&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '999',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [TimersList],&#10;        _idleStart: 20606,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 285,&#10;        [Symbol(triggerId)]: 283&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 999\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: true,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 999,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 21:12:10 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 0,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      signal: [AbortSignal],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: true,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (7.2 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;Retry attempt 2 after error: Failed to get response from Ollama: Request failed with status code 500&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '999',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [TimersList],&#10;        _idleStart: 22632,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 305,&#10;        [Symbol(triggerId)]: 303&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 999\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: true,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 999,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 21:12:12 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 0,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      signal: [AbortSignal],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: true,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (7.2 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;Chat error: Error: Failed to get response from Ollama: Request failed with status code 500&#10;    at OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:135:19)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26&#10;&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;This may take several minutes...&#10;Model llama2 pull completed: &#10;pulling manifest &#10;pulling 8934d96d3f08... 100% ▕████████████████▏ 3.8 GB                         &#10;pulling 8c17c2ebb0ea... 100% ▕████████████████▏ 7.0 KB                         &#10;pulling 7c23fb36d801... 100% ▕████████████████▏ 4.8 KB                         &#10;pulling 2e0493f67d0c... 100% ▕████████████████▏   59 B                         &#10;pulling fa304d675061... 100% ▕████████████████▏   91 B                         &#10;pulling 42ba7f8a01dd... 100% ▕████████████████▏  557 B                         &#10;verifying sha256 digest &#10;writing manifest &#10;success &#10;&#10;Successfully initialized with model: llama2&#10;Ollama service initialization complete&#10;Server initialization complete, ready to handle requests&#10;Server is running on http://localhost:3000&#10;Ollama chat error: AxiosError: socket hang up&#10;    at AxiosError.from (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:876:14)&#10;    at RedirectableRequest.handleRequestError (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3156:25)&#10;    at RedirectableRequest.emit (node:events:519:28)&#10;    at eventHandlers.&lt;computed&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/follow-redirects/index.js:49:24)&#10;    at ClientRequest.emit (node:events:519:28)&#10;    at emitErrorEvent (node:_http_client:108:11)&#10;    at Socket.socketOnEnd (node:_http_client:535:5)&#10;    at Socket.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ECONNRESET',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: Hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; Writable {&#10;    _events: {&#10;      close: undefined,&#10;      error: [Function: handleRequestError],&#10;      prefinish: undefined,&#10;      finish: undefined,&#10;      drain: undefined,&#10;      response: [Function: handleResponse],&#10;      socket: [Function: handleRequestSocket]&#10;    },&#10;    _writableState: WritableState {&#10;      highWaterMark: 16384,&#10;      length: 0,&#10;      corked: 0,&#10;      onwrite: [Function: bound onwrite],&#10;      writelen: 0,&#10;      bufferedIndex: 0,&#10;      pendingcb: 0,&#10;      [Symbol(kState)]: 17580812,&#10;      [Symbol(kBufferedValue)]: null&#10;    },&#10;    _maxListeners: undefined,&#10;    _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/api/generate',&#10;      method: 'POST',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '11434',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/api/generate'&#10;    },&#10;    _ended: true,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 999,&#10;    _requestBodyBuffers: [ [Object] ],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:11434/api/generate',&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: Error: socket hang up&#10;      at Socket.socketOnEnd (node:_http_client:535:25)&#10;      at Socket.emit (node:events:531:35)&#10;      at endReadableNT (node:internal/streams/readable:1696:12)&#10;      at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {&#10;    code: 'ECONNRESET'&#10;  }&#10;}&#10;Retry attempt 1 after error: Failed to get response from Ollama: socket hang up&#10;Ollama chat error: AxiosError [AggregateError]&#10;    at AxiosError.from (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:876:14)&#10;    at RedirectableRequest.handleRequestError (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3156:25)&#10;    at RedirectableRequest.emit (node:events:519:28)&#10;    at eventHandlers.&lt;computed&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/follow-redirects/index.js:49:24)&#10;    at ClientRequest.emit (node:events:519:28)&#10;    at emitErrorEvent (node:_http_client:108:11)&#10;    at Socket.socketErrorListener (node:_http_client:511:5)&#10;    at Socket.emit (node:events:519:28)&#10;    at emitErrorNT (node:internal/streams/destroy:169:8)&#10;    at emitErrorCloseNT (node:internal/streams/destroy:128:3)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ECONNREFUSED',&#10;  errors: [&#10;    Error: connect ECONNREFUSED ::1:11434&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '::1',&#10;      port: 11434&#10;    },&#10;    Error: connect ECONNREFUSED 127.0.0.1:11434&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '127.0.0.1',&#10;      port: 11434&#10;    }&#10;  ],&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: Hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; Writable {&#10;    _events: {&#10;      close: undefined,&#10;      error: [Function: handleRequestError],&#10;      prefinish: undefined,&#10;      finish: undefined,&#10;      drain: undefined,&#10;      response: [Function: handleResponse],&#10;      socket: [Function: handleRequestSocket]&#10;    },&#10;    _writableState: WritableState {&#10;      highWaterMark: 16384,&#10;      length: 0,&#10;      corked: 0,&#10;      onwrite: [Function: bound onwrite],&#10;      writelen: 0,&#10;      bufferedIndex: 0,&#10;      pendingcb: 0,&#10;      [Symbol(kState)]: 17580812,&#10;      [Symbol(kBufferedValue)]: null&#10;    },&#10;    _maxListeners: undefined,&#10;    _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/api/generate',&#10;      method: 'POST',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '11434',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/api/generate'&#10;    },&#10;    _ended: false,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 999,&#10;    _requestBodyBuffers: [ [Object] ],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: false,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:11434/api/generate',&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: AggregateError [ECONNREFUSED]: &#10;      at internalConnectMultiple (node:net:1118:18)&#10;      at afterConnectMultiple (node:net:1685:7) {&#10;    code: 'ECONNREFUSED',&#10;    [errors]: [ [Error], [Error] ]&#10;  }&#10;}&#10;Retry attempt 2 after error: Could not connect to Ollama. Please ensure Ollama is running.&#10;Ollama chat error: AxiosError [AggregateError]&#10;    at AxiosError.from (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:876:14)&#10;    at RedirectableRequest.handleRequestError (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3156:25)&#10;    at RedirectableRequest.emit (node:events:519:28)&#10;    at eventHandlers.&lt;computed&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/follow-redirects/index.js:49:24)&#10;    at ClientRequest.emit (node:events:519:28)&#10;    at emitErrorEvent (node:_http_client:108:11)&#10;    at Socket.socketErrorListener (node:_http_client:511:5)&#10;    at Socket.emit (node:events:519:28)&#10;    at emitErrorNT (node:internal/streams/destroy:169:8)&#10;    at emitErrorCloseNT (node:internal/streams/destroy:128:3)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ECONNREFUSED',&#10;  errors: [&#10;    Error: connect ECONNREFUSED ::1:11434&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '::1',&#10;      port: 11434&#10;    },&#10;    Error: connect ECONNREFUSED 127.0.0.1:11434&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '127.0.0.1',&#10;      port: 11434&#10;    }&#10;  ],&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: Hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; Writable {&#10;    _events: {&#10;      close: undefined,&#10;      error: [Function: handleRequestError],&#10;      prefinish: undefined,&#10;      finish: undefined,&#10;      drain: undefined,&#10;      response: [Function: handleResponse],&#10;      socket: [Function: handleRequestSocket]&#10;    },&#10;    _writableState: WritableState {&#10;      highWaterMark: 16384,&#10;      length: 0,&#10;      corked: 0,&#10;      onwrite: [Function: bound onwrite],&#10;      writelen: 0,&#10;      bufferedIndex: 0,&#10;      pendingcb: 0,&#10;      [Symbol(kState)]: 17580812,&#10;      [Symbol(kBufferedValue)]: null&#10;    },&#10;    _maxListeners: undefined,&#10;    _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/api/generate',&#10;      method: 'POST',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '11434',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/api/generate'&#10;    },&#10;    _ended: false,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 999,&#10;    _requestBodyBuffers: [ [Object] ],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: false,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:11434/api/generate',&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: AggregateError [ECONNREFUSED]: &#10;      at internalConnectMultiple (node:net:1118:18)&#10;      at afterConnectMultiple (node:net:1685:7) {&#10;    code: 'ECONNREFUSED',&#10;    [errors]: [ [Error], [Error] ]&#10;  }&#10;}&#10;Chat error: Error: Could not connect to Ollama. Please ensure Ollama is running.&#10;    at OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:132:23)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26&#10;&#10;" />
        <option value="no for the last time no adding files" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed: &#10;pulling manifest &#10;pulling 8934d96d3f08... 100% ▕████████████████▏ 3.8 GB                         &#10;pulling 8c17c2ebb0ea... 100% ▕████████████████▏ 7.0 KB                         &#10;pulling 7c23fb36d801... 100% ▕████████████████▏ 4.8 KB                         &#10;pulling 2e0493f67d0c... 100% ▕████████████████▏   59 B                         &#10;pulling fa304d675061... 100% ▕████████████████▏   91 B                         &#10;pulling 42ba7f8a01dd... 100% ▕████████████████▏  557 B                         &#10;verifying sha256 digest &#10;writing manifest &#10;success &#10;&#10;Ollama service initialization complete&#10;Server initialization complete, ready to handle requests&#10;Server is running on http://localhost:3000&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:84:30)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:60:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 60000,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '612',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '612',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [TimersList],&#10;        _idleStart: 19701,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 208,&#10;        [Symbol(triggerId)]: 206&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 612\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: false,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 612,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      _timeout: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 21:29:29 GMT',&#10;      'content-length': '63'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 60000,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '612',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 612\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: { error: 'llama runner process has terminated: signal: killed' }&#10;  },&#10;  status: 500&#10;}&#10;Chat error: Error: Failed to get response from Ollama: Request failed with status code 500&#10;    at OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:104:19)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:60:26&#10;&#10;fix this without adding any files" />
        <option value="is it better to have the server.js launch llama2 or have it be a sepearte starting service?" />
        <option value="ok i now want to use the data in the lancedb files to to ask the user questions using the LLM" />
        <option value="/usr/bin/npm install&#10;npm error code ETARGET&#10;npm error notarget No matching version found for lancedb@^0.5.3.&#10;npm error notarget In most cases you or one of your dependencies are requesting&#10;npm error notarget a package version that doesn't exist.&#10;npm error A complete log of this run can be found in: /home/barthmalemew/.npm/_logs/2024-11-02T22_09_05_917Z-debug-0.log&#10;&#10;Process finished with exit code 1&#10;" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ npm install landcedb&#10;npm error code ETARGET&#10;npm error notarget No matching version found for lancedb@^0.3.3.&#10;npm error notarget In most cases you or one of your dependencies are requesting&#10;npm error notarget a package version that doesn't exist.&#10;npm error A complete log of this run can be found in: /home/barthmalemew/.npm/_logs/2024-11-02T22_12_42_681Z-debug-0.log&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ npm install landcedb&#10;" />
        <option value="how does it work now" />
        <option value="it should prompt the user with a quesition from the database on its sown" />
        <option value="pip install lancedb==0.3.3" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;node:internal/modules/cjs/loader:495&#10;      throw err;&#10;      ^&#10;&#10;Error: Cannot find module '/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/lancedb/index.js'. Please verify that the package.json has a valid &quot;main&quot; entry&#10;    at tryPackage (node:internal/modules/cjs/loader:487:19)&#10;    at Module._findPath (node:internal/modules/cjs/loader:771:18)&#10;    at Module._resolveFilename (node:internal/modules/cjs/loader:1211:27)&#10;    at Module._load (node:internal/modules/cjs/loader:1051:27)&#10;    at Module.require (node:internal/modules/cjs/loader:1311:19)&#10;    at require (node:internal/modules/helpers:179:18)&#10;    at Object.&lt;anonymous&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:12:17)&#10;    at Module._compile (node:internal/modules/cjs/loader:1469:14)&#10;    at Module._extensions..js (node:internal/modules/cjs/loader:1548:10)&#10;    at Module.load (node:internal/modules/cjs/loader:1288:32) {&#10;  code: 'MODULE_NOT_FOUND',&#10;  path: '/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/lancedb/package.json',&#10;  requestPath: 'lancedb'&#10;}&#10;&#10;Node.js v20.17.0&#10;&#10;Process finished with exit code 1&#10;&#10;" />
        <option value="node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;node:internal/modules/cjs/loader:495&#10;      throw err;&#10;      ^&#10;&#10;Error: Cannot find module '/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/lancedb/index.js'. Please verify that the package.json has a valid &quot;main&quot; entry&#10;    at tryPackage (node:internal/modules/cjs/loader:487:19)&#10;    at Module._findPath (node:internal/modules/cjs/loader:771:18)&#10;    at Module._resolveFilename (node:internal/modules/cjs/loader:1211:27)&#10;    at Module._load (node:internal/modules/cjs/loader:1051:27)&#10;    at Module.require (node:internal/modules/cjs/loader:1311:19)&#10;    at require (node:internal/modules/helpers:179:18)&#10;    at Object.&lt;anonymous&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:12:17)&#10;    at Module._compile (node:internal/modules/cjs/loader:1469:14)&#10;    at Module._extensions..js (node:internal/modules/cjs/loader:1548:10)&#10;    at Module.load (node:internal/modules/cjs/loader:1288:32) {&#10;  code: 'MODULE_NOT_FOUND',&#10;  path: '/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/lancedb/package.json',&#10;  requestPath: 'lancedb'&#10;}&#10;&#10;Node.js v20.17.0&#10;&#10;Process finished with exit code 1&#10;&#10;" />
        <option value="have a look over the code and suggest changes" />
        <option value="no please dont add files" />
        <option value="ok ignoreing style sheets look over my code. i want the server.js to start the llama2 model automatically by running he ollama.py" />
        <option value="rthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:76&#10;app.get('/api/random-question', checkServerReady, async (req, res) =&gt; {&#10;^&#10;&#10;ReferenceError: app is not defined&#10;    at Object.&lt;anonymous&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:76:1)&#10;    at Module._compile (node:internal/modules/cjs/loader:1469:14)&#10;    at Module._extensions..js (node:internal/modules/cjs/loader:1548:10)&#10;    at Module.load (node:internal/modules/cjs/loader:1288:32)&#10;    at Module._load (node:internal/modules/cjs/loader:1104:12)&#10;    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:174:12)&#10;    at node:internal/main/run_main_module:28:49&#10;&#10;Node.js v20.17.0&#10;&#10;Process finished with exit code 1&#10;&#10;" />
        <option value="please look at thep python script to make sure its ocrrect" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ pip install fastapu&#10;ERROR: Could not find a version that satisfies the requirement fastapu (from versions: none)&#10;ERROR: No matching distribution found for fastapu&#10;^CTraceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/bin/pip&quot;, line 8, in &lt;module&gt;&#10;    sys.exit(main())&#10;             ^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/cli/main.py&quot;, line 79, in main&#10;    return command.main(cmd_args)&#10;           ^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/cli/base_command.py&quot;, line 101, in main&#10;    return self._main(args)&#10;           ^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/cli/base_command.py&quot;, line 236, in _main&#10;    self.handle_pip_version_check(options)&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/cli/req_command.py&quot;, line 191, in handle_pip_version_check&#10;    pip_self_version_check(session, options)&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/self_outdated_check.py&quot;, line 230, in pip_self_version_check&#10;    upgrade_prompt = _self_version_check_logic(&#10;                     ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/self_outdated_check.py&quot;, line 193, in _self_version_check_logic&#10;    remote_version_str = get_remote_version()&#10;                         ^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/self_outdated_check.py&quot;, line 177, in _get_current_remote_pip_version&#10;    best_candidate = finder.find_best_candidate(&quot;pip&quot;).best_candidate&#10;                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/index/package_finder.py&quot;, line 890, in find_best_candidate&#10;    candidates = self.find_all_candidates(project_name)&#10;                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/index/package_finder.py&quot;, line 831, in find_all_candidates&#10;    page_candidates = list(page_candidates_it)&#10;                      ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/index/sources.py&quot;, line 134, in page_candidates&#10;    yield from self._candidates_from_page(self._link)&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/index/package_finder.py&quot;, line 798, in process_project_url&#10;    package_links = self.evaluate_links(&#10;                    ^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/index/package_finder.py&quot;, line 778, in evaluate_links&#10;    candidate = self.get_install_candidate(link_evaluator, link)&#10;                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/index/package_finder.py&quot;, line 759, in get_install_candidate&#10;    result, detail = link_evaluator.evaluate_link(link)&#10;                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/index/package_finder.py&quot;, line 238, in evaluate_link&#10;    supports_python = _check_link_requires_python(&#10;                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/index/package_finder.py&quot;, line 66, in _check_link_requires_python&#10;    is_compatible = check_requires_python(&#10;                    ^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/utils/packaging.py&quot;, line 31, in check_requires_python&#10;    requires_python_specifier = specifiers.SpecifierSet(requires_python)&#10;                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_vendor/packaging/specifiers.py&quot;, line 634, in __init__&#10;    parsed.add(Specifier(specifier))&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_vendor/packaging/specifiers.py&quot;, line 125, in __hash__&#10;    return hash(self._canonical_spec)&#10;                ^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_vendor/packaging/specifiers.py&quot;, line 122, in _canonical_spec&#10;    return self._spec[0], canonicalize_version(self._spec[1])&#10;                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_vendor/packaging/utils.py&quot;, line 60, in canonicalize_version&#10;    parts.append(re.sub(r&quot;(\.0)+$&quot;, &quot;&quot;, &quot;.&quot;.join(str(x) for x in parsed.release)))&#10;                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;KeyboardInterrupt&#10;&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ pip install fastapi&#10;Collecting fastapi&#10;  Obtaining dependency information for fastapi from https://files.pythonhosted.org/packages/99/f6/af0d1f58f86002be0cf1e2665cdd6f7a4a71cdc8a7a9438cdc9e3b5375fe/fastapi-0.115.4-py3-none-any.whl.metadata&#10;  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)&#10;Collecting starlette&lt;0.42.0,&gt;=0.40.0 (from fastapi)&#10;  Obtaining dependency information for starlette&lt;0.42.0,&gt;=0.40.0 from https://files.pythonhosted.org/packages/54/43/f185bfd0ca1d213beb4293bed51d92254df23d8ceaf6c0e17146d508a776/starlette-0.41.2-py3-none-any.whl.metadata&#10;  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)&#10;Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,&lt;3.0.0,&gt;=1.7.4 (from fastapi)&#10;  Obtaining dependency information for pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,&lt;3.0.0,&gt;=1.7.4 from https://files.pythonhosted.org/packages/df/e4/ba44652d562cbf0bf320e0f3810206149c8a4e99cdbf66da82e97ab53a15/pydantic-2.9.2-py3-none-any.whl.metadata&#10;  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)&#10;Collecting typing-extensions&gt;=4.8.0 (from fastapi)&#10;  Obtaining dependency information for typing-extensions&gt;=4.8.0 from https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl.metadata&#10;  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)&#10;Collecting annotated-types&gt;=0.6.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,&lt;3.0.0,&gt;=1.7.4-&gt;fastapi)&#10;  Obtaining dependency information for annotated-types&gt;=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata&#10;  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)&#10;Collecting pydantic-core==2.23.4 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,&lt;3.0.0,&gt;=1.7.4-&gt;fastapi)&#10;  Obtaining dependency information for pydantic-core==2.23.4 from https://files.pythonhosted.org/packages/06/c8/7d4b708f8d05a5cbfda3243aad468052c6e99de7d0937c9146c24d9f12e9/pydantic_core-2.23.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata&#10;  Using cached pydantic_core-2.23.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)&#10;Collecting anyio&lt;5,&gt;=3.4.0 (from starlette&lt;0.42.0,&gt;=0.40.0-&gt;fastapi)&#10;  Obtaining dependency information for anyio&lt;5,&gt;=3.4.0 from https://files.pythonhosted.org/packages/e4/f5/f2b75d2fc6f1a260f340f0e7c6a060f4dd2961cc16884ed851b0d18da06a/anyio-4.6.2.post1-py3-none-any.whl.metadata&#10;  Downloading anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)&#10;Collecting idna&gt;=2.8 (from anyio&lt;5,&gt;=3.4.0-&gt;starlette&lt;0.42.0,&gt;=0.40.0-&gt;fastapi)&#10;  Obtaining dependency information for idna&gt;=2.8 from https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl.metadata&#10;  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)&#10;Collecting sniffio&gt;=1.1 (from anyio&lt;5,&gt;=3.4.0-&gt;starlette&lt;0.42.0,&gt;=0.40.0-&gt;fastapi)&#10;  Obtaining dependency information for sniffio&gt;=1.1 from https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl.metadata&#10;  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)&#10;Downloading fastapi-0.115.4-py3-none-any.whl (94 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.7/94.7 kB 2.8 MB/s eta 0:00:00&#10;Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)&#10;Using cached pydantic_core-2.23.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)&#10;Downloading starlette-0.41.2-py3-none-any.whl (73 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.3/73.3 kB 3.7 MB/s eta 0:00:00&#10;Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)&#10;Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)&#10;Downloading anyio-4.6.2.post1-py3-none-any.whl (90 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.4/90.4 kB 3.2 MB/s eta 0:00:00&#10;Downloading idna-3.10-py3-none-any.whl (70 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.4/70.4 kB 5.7 MB/s eta 0:00:00&#10;Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)&#10;Installing collected packages: typing-extensions, sniffio, idna, annotated-types, pydantic-core, anyio, starlette, pydantic, fastapi&#10;Successfully installed annotated-types-0.7.0 anyio-4.6.2.post1 fastapi-0.115.4 idna-3.10 pydantic-2.9.2 pydantic-core-2.23.4 sniffio-1.3.1 starlette-0.41.2 typing-extensions-4.12.2&#10;&#10;[notice] A new release of pip is available: 23.2.1 -&gt; 24.3.1&#10;[notice] To update, run: pip install --upgrade pip&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ pip install lancedb&#10;Collecting lancedb&#10;  Obtaining dependency information for lancedb from https://files.pythonhosted.org/packages/e3/a8/67a8dc6fd7a57bde6b9f75b10368805db46b109adc7d634d7ce45bc655f3/lancedb-0.15.0-cp38-abi3-manylinux_2_28_x86_64.whl.metadata&#10;  Using cached lancedb-0.15.0-cp38-abi3-manylinux_2_28_x86_64.whl.metadata (4.8 kB)&#10;Collecting deprecation (from lancedb)&#10;  Obtaining dependency information for deprecation from https://files.pythonhosted.org/packages/02/c3/253a89ee03fc9b9682f1541728eb66db7db22148cd94f89ab22528cd1e1b/deprecation-2.1.0-py2.py3-none-any.whl.metadata&#10;  Using cached deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)&#10;Collecting pylance==0.19.1 (from lancedb)&#10;  Obtaining dependency information for pylance==0.19.1 from https://files.pythonhosted.org/packages/3b/af/3bf6d0c9dc52e2ae048c575249527f3c2cc8a4df85c94905900c719b42e0/pylance-0.19.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata&#10;  Using cached pylance-0.19.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (7.4 kB)&#10;Collecting requests&gt;=2.31.0 (from lancedb)&#10;  Obtaining dependency information for requests&gt;=2.31.0 from https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl.metadata&#10;  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)&#10;Collecting tqdm&gt;=4.27.0 (from lancedb)&#10;  Obtaining dependency information for tqdm&gt;=4.27.0 from https://files.pythonhosted.org/packages/41/73/02342de9c2d20922115f787e101527b831c0cffd2105c946c4a4826bcfd4/tqdm-4.66.6-py3-none-any.whl.metadata&#10;  Downloading tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)&#10;     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 1.2 MB/s eta 0:00:00&#10;Requirement already satisfied: pydantic&gt;=1.10 in ./venv/lib/python3.12/site-packages (from lancedb) (2.9.2)&#10;Collecting attrs&gt;=21.3.0 (from lancedb)&#10;  Obtaining dependency information for attrs&gt;=21.3.0 from https://files.pythonhosted.org/packages/6a/21/5b6702a7f963e95456c0de2d495f67bf5fd62840ac655dc451586d23d39a/attrs-24.2.0-py3-none-any.whl.metadata&#10;  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)&#10;Collecting packaging (from lancedb)&#10;  Obtaining dependency information for packaging from https://files.pythonhosted.org/packages/08/aa/cc0199a5f0ad350994d660967a8efb233fe0416e4639146c089643407ce6/packaging-24.1-py3-none-any.whl.metadata&#10;  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)&#10;Collecting cachetools (from lancedb)&#10;  Obtaining dependency information for cachetools from https://files.pythonhosted.org/packages/a4/07/14f8ad37f2d12a5ce41206c21820d8cb6561b728e51fad4530dff0552a67/cachetools-5.5.0-py3-none-any.whl.metadata&#10;  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)&#10;Collecting overrides&gt;=0.7 (from lancedb)&#10;  Obtaining dependency information for overrides&gt;=0.7 from https://files.pythonhosted.org/packages/2c/ab/fc8290c6a4c722e5514d80f62b2dc4c4df1a68a41d1364e625c35990fcf3/overrides-7.7.0-py3-none-any.whl.metadata&#10;  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)&#10;Collecting pyarrow&gt;=12 (from pylance==0.19.1-&gt;lancedb)&#10;  Obtaining dependency information for pyarrow&gt;=12 from https://files.pythonhosted.org/packages/8d/1f/9bb3b3a644892d631dbbe99053cdb5295092d2696b4bcd3d21f29624c689/pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata&#10;  Using cached pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)&#10;Collecting numpy&lt;2,&gt;=1.22 (from pylance==0.19.1-&gt;lancedb)&#10;  Obtaining dependency information for numpy&lt;2,&gt;=1.22 from https://files.pythonhosted.org/packages/0f/50/de23fde84e45f5c4fda2488c759b69990fd4512387a8632860f3ac9cd225/numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata&#10;  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)&#10;Requirement already satisfied: annotated-types&gt;=0.6.0 in ./venv/lib/python3.12/site-packages (from pydantic&gt;=1.10-&gt;lancedb) (0.7.0)&#10;Requirement already satisfied: pydantic-core==2.23.4 in ./venv/lib64/python3.12/site-packages (from pydantic&gt;=1.10-&gt;lancedb) (2.23.4)&#10;Requirement already satisfied: typing-extensions&gt;=4.6.1 in ./venv/lib/python3.12/site-packages (from pydantic&gt;=1.10-&gt;lancedb) (4.12.2)&#10;Collecting charset-normalizer&lt;4,&gt;=2 (from requests&gt;=2.31.0-&gt;lancedb)&#10;  Obtaining dependency information for charset-normalizer&lt;4,&gt;=2 from https://files.pythonhosted.org/packages/16/92/92a76dc2ff3a12e69ba94e7e05168d37d0345fa08c87e1fe24d0c2a42223/charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata&#10;  Downloading charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)&#10;Requirement already satisfied: idna&lt;4,&gt;=2.5 in ./venv/lib/python3.12/site-packages (from requests&gt;=2.31.0-&gt;lancedb) (3.10)&#10;Collecting urllib3&lt;3,&gt;=1.21.1 (from requests&gt;=2.31.0-&gt;lancedb)&#10;  Obtaining dependency information for urllib3&lt;3,&gt;=1.21.1 from https://files.pythonhosted.org/packages/ce/d9/5f4c13cecde62396b0d3fe530a50ccea91e7dfc1ccf0e09c228841bb5ba8/urllib3-2.2.3-py3-none-any.whl.metadata&#10;  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)&#10;Collecting certifi&gt;=2017.4.17 (from requests&gt;=2.31.0-&gt;lancedb)&#10;  Obtaining dependency information for certifi&gt;=2017.4.17 from https://files.pythonhosted.org/packages/12/90/3c9ff0512038035f59d279fddeb79f5f1eccd8859f06d6163c58798b9487/certifi-2024.8.30-py3-none-any.whl.metadata&#10;  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)&#10;Using cached lancedb-0.15.0-cp38-abi3-manylinux_2_28_x86_64.whl (27.1 MB)&#10;Using cached pylance-0.19.1-cp39-abi3-manylinux_2_28_x86_64.whl (30.4 MB)&#10;Using cached attrs-24.2.0-py3-none-any.whl (63 kB)&#10;Using cached overrides-7.7.0-py3-none-any.whl (17 kB)&#10;Using cached requests-2.32.3-py3-none-any.whl (64 kB)&#10;Downloading tqdm-4.66.6-py3-none-any.whl (78 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 10.7 MB/s eta 0:00:00&#10;Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)&#10;Using cached deprecation-2.1.0-py2.py3-none-any.whl (11 kB)&#10;Using cached packaging-24.1-py3-none-any.whl (53 kB)&#10;Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.3/167.3 kB 6.2 MB/s eta 0:00:00&#10;Downloading charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.8/143.8 kB 2.2 MB/s eta 0:00:00&#10;Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)&#10;Using cached pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.0 MB)&#10;Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.3/126.3 kB 2.8 MB/s eta 0:00:00&#10;Installing collected packages: urllib3, tqdm, pyarrow, packaging, overrides, numpy, charset-normalizer, certifi, cachetools, attrs, requests, pylance, deprecation, lancedb&#10;Successfully installed attrs-24.2.0 cachetools-5.5.0 certifi-2024.8.30 charset-normalizer-3.4.0 deprecation-2.1.0 lancedb-0.15.0 numpy-1.26.4 overrides-7.7.0 packaging-24.1 pyarrow-18.0.0 pylance-0.19.1 requests-2.32.3 tqdm-4.66.6 urllib3-2.2.3&#10;&#10;[notice] A new release of pip is available: 23.2.1 -&gt; 24.3.1&#10;[notice] To update, run: pip install --upgrade pip&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ pip install uvicorn&#10;Collecting uvicorn&#10;  Obtaining dependency information for uvicorn from https://files.pythonhosted.org/packages/eb/14/78bd0e95dd2444b6caacbca2b730671d4295ccb628ef58b81bee903629df/uvicorn-0.32.0-py3-none-any.whl.metadata&#10;  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)&#10;Collecting click&gt;=7.0 (from uvicorn)&#10;  Obtaining dependency information for click&gt;=7.0 from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata&#10;  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)&#10;Collecting h11&gt;=0.8 (from uvicorn)&#10;  Obtaining dependency information for h11&gt;=0.8 from https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata&#10;  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)&#10;Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.7/63.7 kB 2.6 MB/s eta 0:00:00&#10;Using cached click-8.1.7-py3-none-any.whl (97 kB)&#10;Using cached h11-0.14.0-py3-none-any.whl (58 kB)&#10;Installing collected packages: h11, click, uvicorn&#10;Successfully installed click-8.1.7 h11-0.14.0 uvicorn-0.32.0&#10;&#10;[notice] A new release of pip is available: 23.2.1 -&gt; 24.3.1&#10;[notice] To update, run: pip install --upgrade pip&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ ls&#10;node_modules  package.json  package-lock.json  venv  webapp&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ ls&#10;node_modules  package.json  package-lock.json  venv  webapp&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ cd webapp&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp$ ls&#10;data  public  server.js  services&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp$ cd services&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ ls&#10;ollama.py&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:148: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [90534]&#10;INFO:     Waiting for application startup.&#10;Initializing Ollama service...&#10;Failed to initialize Ollama service: Table space_training does not exist.Please first call db.create_table(space_training, data)&#10;ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 693, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 569, in __aenter__&#10;    await self._router.startup()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 670, in startup&#10;    await handler()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 150, in startup_event&#10;    await service.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 51, in initialize&#10;    self.table = await self.db.open_table('space_training')&#10;                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/db.py&quot;, line 450, in open_table&#10;    return LanceTable.open(self, name, index_cache_size=index_cache_size)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/table.py&quot;, line 1085, in open&#10;    raise FileNotFoundError(&#10;FileNotFoundError: Table space_training does not exist.Please first call db.create_table(space_training, data)&#10;&#10;ERROR:    Application startup failed. Exiting.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 16, in &lt;module&gt;&#10;    import aiohttp&#10;ModuleNotFoundError: No module named 'aiohttp'&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="make sure the file paths are correct accross the project" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:168: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:172: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;shutdown&quot;)&#10;INFO:     Started server process [92125]&#10;INFO:     Waiting for application startup.&#10;Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Failed to initialize Ollama service: object LanceTable can't be used in 'await' expression&#10;ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 693, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 569, in __aenter__&#10;    await self._router.startup()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 670, in startup&#10;    await handler()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 170, in startup_event&#10;    await service.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 59, in initialize&#10;    self.table = await self.db.open_table('space_training')&#10;                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;TypeError: object LanceTable can't be used in 'await' expression&#10;&#10;ERROR:    Application startup failed. Exiting.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:180: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:184: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;shutdown&quot;)&#10;INFO:     Started server process [92200]&#10;INFO:     Waiting for application startup.&#10;Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Successfully connected to Ollama service&#10;Failed to initialize Ollama service: object LanceTable can't be used in 'await' expression&#10;ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 693, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 569, in __aenter__&#10;    await self._router.startup()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 670, in startup&#10;    await handler()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 182, in startup_event&#10;    await service.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 60, in initialize&#10;    self.table = await self.db.open_table('space_training')&#10;                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;TypeError: object LanceTable can't be used in 'await' expression&#10;&#10;ERROR:    Application startup failed. Exiting.&#10;" />
        <option value="e /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;node:internal/modules/cjs/loader:1228&#10;  throw err;&#10;  ^&#10;&#10;Error: Cannot find module 'express-rate-limit'&#10;Require stack:&#10;- /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;    at Module._resolveFilename (node:internal/modules/cjs/loader:1225:15)&#10;    at Module._load (node:internal/modules/cjs/loader:1051:27)&#10;    at Module.require (node:internal/modules/cjs/loader:1311:19)&#10;    at require (node:internal/modules/helpers:179:18)&#10;    at Object.&lt;anonymous&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:22:19)&#10;    at Module._compile (node:internal/modules/cjs/loader:1469:14)&#10;    at Module._extensions..js (node:internal/modules/cjs/loader:1548:10)&#10;    at Module.load (node:internal/modules/cjs/loader:1288:32)&#10;    at Module._load (node:internal/modules/cjs/loader:1104:12)&#10;    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:174:12) {&#10;  code: 'MODULE_NOT_FOUND',&#10;  requireStack: [ '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js' ]&#10;}&#10;&#10;Node.js v20.17.0&#10;&#10;Process finished with exit code 1&#10;&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Starting Ollama service...&#10;Ollama service error: Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 6, in &lt;module&gt;&#10;&#10;Ollama service error:     from fastapi import FastAPI, HTTPException&#10;ModuleNotFoundError: No module named 'fastapi'&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;Server initialization complete, ready to handle requests&#10;Server is running on http://localhost:3000&#10;Starting Ollama service...&#10;Ollama service error: Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 6, in &lt;module&gt;&#10;&#10;Ollama service error:     from fastapi import FastAPI, HTTPException&#10;ModuleNotFoundError: No module named 'fastapi'&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;Starting Ollama service...&#10;Ollama service error: Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 6, in &lt;module&gt;&#10;&#10;Ollama service error:     from fastapi import FastAPI, HTTPException&#10;ModuleNotFoundError: No module named 'fastapi'&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;&#10;" />
        <option value="look at the html and css, make very few changes i just want to know whats wrong with it" />
        <option value="can you look at my index and style sheet to figure out wats wrong" />
        <option value="ok can you look at my style sheet and html please?" />
        <option value="  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/starlette/routing.py&quot;, line 734, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/usr/lib64/python3.12/contextlib.py&quot;, line 217, in __aexit__&#10;    await anext(self.gen)&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 184, in lifespan&#10;    service.db.close()&#10;    ^^^^^^^^^^^^^^^^&#10;AttributeError: 'LanceDBConnection' object has no attribute 'close'&#10;&#10;&#10;Ollama service error: ERROR:    Application shutdown failed. Exiting.&#10;&#10;Ollama service: Ollama service initialization complete&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;Server initialization complete, ready to handle requests&#10;Server is running on http://localhost:3000&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [99480]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Successfully connected to Ollama service&#10;&#10;Ollama service: Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;&#10;Ollama service error: ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/starlette/routing.py&quot;, line 734, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/usr/lib64/python3.12/contextlib.py&quot;, line 217, in __aexit__&#10;    await anext(self.gen)&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 184, in lifespan&#10;    service.db.close()&#10;    ^^^^^^^^^^^^^^^^&#10;AttributeError: 'LanceDBConnection' object has no attribute 'close'&#10;&#10;&#10;Ollama service error: ERROR:    Application shutdown failed. Exiting.&#10;&#10;Ollama service: Ollama service initialization complete&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [99507]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Successfully connected to Ollama service&#10;&#10;Ollama service: Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;&#10;Ollama service error: INFO:     Waiting for application shutdown.&#10;&#10;Ollama service error: ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/starlette/routing.py&quot;, line 734, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/usr/lib64/python3.12/contextlib.py&quot;, line 217, in __aexit__&#10;    await anext(self.gen)&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 184, in lifespan&#10;    service.db.close()&#10;    ^^^^^^^^^^^^^^^^&#10;AttributeError: 'LanceDBConnection' object has no attribute 'close'&#10;&#10;&#10;Ollama service error: ERROR:    Application shutdown failed. Exiting.&#10;&#10;Ollama service: Ollama service initialization complete&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [99537]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Successfully connected to Ollama service&#10;&#10;Ollama service: Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;&#10;Ollama service error: ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/starlette/routing.py&quot;, line 734, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/usr/lib64/python3.12/contextlib.py&quot;, line 217, in __aexit__&#10;    await anext(self.gen)&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 184, in lifespan&#10;    service.db.close()&#10;    ^^^^^^^^^^^^^^^^&#10;AttributeError: 'LanceDBConnection' object has no attribute 'close'&#10;&#10;&#10;Ollama service error: ERROR:    Application shutdown failed. Exiting.&#10;&#10;Ollama service: Ollama service initialization complete&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [99568]&#10;&#10;Ollama service error: INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Successfully connected to Ollama service&#10;&#10;Ollama service: Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;&#10;Ollama service error: ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/starlette/routing.py&quot;, line 734, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/usr/lib64/python3.12/contextlib.py&quot;, line 217, in __aexit__&#10;    await anext(self.gen)&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 184, in lifespan&#10;    service.db.close()&#10;    ^^^^^^^^^^^^^^^^&#10;AttributeError: 'LanceDBConnection' object has no attribute 'close'&#10;&#10;&#10;Ollama service error: ERROR:    Application shutdown failed. Exiting.&#10;&#10;Ollama service: Ollama service initialization complete&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [102035]&#10;&#10;Ollama service error: INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Successfully connected to Ollama service&#10;&#10;Ollama service: Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;Ollama service: Ollama service initialization complete&#10;Successfully connected to Ollama service&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255&#10;Ollama service error: :9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace&#10;&#10;Ollama service: INFO:     127.0.0.1:34560 - &quot;GET /health HTTP/1.1&quot; 200 OK&#10;&#10;Server is running on http://localhost:3000&#10;" />
        <option value="id prefer if you fix the error without mkaing mroe files" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [102362]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Successfully connected to Ollama service&#10;&#10;Ollama service: Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;Ollama service: Ollama service initialization complete&#10;Successfully connected to Ollama service&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace&#10;&#10;Ollama service: INFO:     127.0.0.1:57602 - &quot;GET /health HTTP/1.1&quot; 200 OK&#10;&#10;Server is running on http://localhost:3000&#10;" />
        <option value="no new files please" />
        <option value="no.. new files" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [102362]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Successfully connected to Ollama service&#10;&#10;Ollama service: Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;Ollama service: Ollama service initialization complete&#10;Successfully connected to Ollama service&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace&#10;&#10;Ollama service: INFO:     127.0.0.1:57602 - &quot;GET /health HTTP/1.1&quot; 200 OK&#10;&#10;Server is running on http://localhost:3000&#10;" />
        <option value="please stop trying to make mroe files youre waisting my credits" />
        <option value=" _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/health',&#10;      method: 'GET',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '5000',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/health'&#10;    },&#10;    _ended: true,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 0,&#10;    _requestBodyBuffers: [],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: true,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: false,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: 0,&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'GET /health HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:5000\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'GET',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/health',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:5000/health',&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: AggregateError [ECONNREFUSED]: &#10;      at internalConnectMultiple (node:net:1118:18)&#10;      at afterConnectMultiple (node:net:1685:7) {&#10;    code: 'ECONNREFUSED',&#10;    [errors]: [ [Error], [Error] ]&#10;  }&#10;}&#10;^CShutting down server...&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Database validation failed: External error: RuntimeError: Task was aborted&#10;Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;Ollama service error: ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/starlette/routing.py&quot;, line 743, in lifespan&#10;    await receive()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/uvicorn/lifespan/on.py&quot;, line 137, in receive&#10;    return await self.receive_queue.get()&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/usr/lib64/python3.12/asyncio/queues.py&quot;, line 158, in get&#10;    await getter&#10;asyncio.exceptions.CancelledError&#10;&#10;&#10;Ollama service: Startup error: Failed to initialize after 3 attempts: External error: RuntimeError: Task was aborted&#10;&#10;Ollama service exited with code 0&#10;&#10;Process finished with exit code 0&#10;&#10;" />
        <option value="iled to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Ollama service error: INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Database validation failed: External error: RuntimeError: Task was aborted&#10;Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;&#10;Ollama service error: INFO:     Waiting for application shutdown.&#10;&#10;Ollama service: Startup error: Failed to initialize after 3 attempts: External error: RuntimeError: Task was aborted&#10;Shutting down Ollama service...&#10;&#10;Ollama service error: INFO:     Application shutdown complete.&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Database validation failed: External error: RuntimeError: Task was aborted&#10;Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Database validation failed: External error: RuntimeError: Task was aborted&#10;Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Starting Ollama service...&#10;Port 5000 is in use, killing existing process...&#10;Ollama service exited with code null&#10;Ollama service crashed, attempting restart...&#10;Ollama service error: INFO:     Started server process [103209]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Database validation failed: External error: RuntimeError: Task was aborted&#10;Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [103260]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Database validation failed: External error: RuntimeError: Task was aborted&#10;Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Server initialization failed: AxiosError [AggregateError]&#10;    at AxiosError.from (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:876:14)&#10;    at RedirectableRequest.handleRequestError (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3156:25)&#10;    at RedirectableRequest.emit (node:events:519:28)&#10;    at eventHandlers.&lt;computed&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/follow-redirects/index.js:49:24)&#10;    at ClientRequest.emit (node:events:519:28)&#10;    at emitErrorEvent (node:_http_client:108:11)&#10;    at Socket.socketErrorListener (node:_http_client:511:5)&#10;    at Socket.emit (node:events:519:28)&#10;    at emitErrorNT (node:internal/streams/destroy:169:8)&#10;    at emitErrorCloseNT (node:internal/streams/destroy:128:3)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async Timeout.initializeServer [as _onTimeout] (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:114:26) {&#10;  code: 'ECONNREFUSED',&#10;  errors: [&#10;    Error: connect ECONNREFUSED ::1:5000&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '::1',&#10;      port: 5000&#10;    },&#10;    Error: connect ECONNREFUSED 127.0.0.1:5000&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '127.0.0.1',&#10;      port: 5000&#10;    }&#10;  ],&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': undefined,&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    baseURL: 'http://localhost:5000',&#10;    method: 'get',&#10;    url: '/health',&#10;    data: undefined&#10;  },&#10;  request: &lt;ref *1&gt; Writable {&#10;    _events: {&#10;      close: undefined,&#10;      error: [Function: handleRequestError],&#10;      prefinish: undefined,&#10;      finish: undefined,&#10;      drain: undefined,&#10;      response: [Function: handleResponse],&#10;      socket: [Function: handleRequestSocket]&#10;    },&#10;    _writableState: WritableState {&#10;      highWaterMark: 16384,&#10;      length: 0,&#10;      corked: 0,&#10;      onwrite: [Function: bound onwrite],&#10;      writelen: 0,&#10;      bufferedIndex: 0,&#10;      pendingcb: 0,&#10;      [Symbol(kState)]: 17580812,&#10;      [Symbol(kBufferedValue)]: null&#10;    },&#10;    _maxListeners: undefined,&#10;    _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/health',&#10;      method: 'GET',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '5000',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/health'&#10;    },&#10;    _ended: true,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 0,&#10;    _requestBodyBuffers: [],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: true,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: false,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: 0,&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'GET /health HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:5000\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'GET',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/health',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:5000/health',&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: AggregateError [ECONNREFUSED]: &#10;      at internalConnectMultiple (node:net:1118:18)&#10;      at afterConnectMultiple (node:net:1685:7) {&#10;    code: 'ECONNREFUSED',&#10;    [errors]: [ [Error], [Error] ]&#10;  }&#10;}&#10;^CShutting down server...&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Database validation failed: External error: RuntimeError: Task was aborted&#10;Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;Ollama service error: ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/starlette/routing.py&quot;, line 743, in lifespan&#10;    await receive()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/uvicorn/lifespan/on.py&quot;, line 137, in receive&#10;    return await self.receive_queue.get()&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/usr/lib64/python3.12/asyncio/queues.py&quot;, line 158, in get&#10;    await getter&#10;asyncio.exceptions.CancelledError&#10;&#10;&#10;Ollama service: Startup error: Failed to initialize after 3 attempts: External error: RuntimeError: Task was aborted&#10;&#10;Ollama service exited with code 0&#10;&#10;Process finished with exit code 0&#10;&#10;&#10;&#10;&#10;&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [103754]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Failed to initialize Ollama service: 'OllamaService' object has no attribute '_ensure_clean_database_state'&#10;&#10;Server initialization failed: AxiosError [AggregateError]&#10;    at AxiosError.from (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:876:14)&#10;    at RedirectableRequest.handleRequestError (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3156:25)&#10;    at RedirectableRequest.emit (node:events:519:28)&#10;    at eventHandlers.&lt;computed&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/follow-redirects/index.js:49:24)&#10;    at ClientRequest.emit (node:events:519:28)&#10;    at emitErrorEvent (node:_http_client:108:11)&#10;    at Socket.socketErrorListener (node:_http_client:511:5)&#10;    at Socket.emit (node:events:519:28)&#10;    at emitErrorNT (node:internal/streams/destroy:169:8)&#10;    at emitErrorCloseNT (node:internal/streams/destroy:128:3)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:114:26) {&#10;  code: 'ECONNREFUSED',&#10;  errors: [&#10;    Error: connect ECONNREFUSED ::1:5000&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '::1',&#10;      port: 5000&#10;    },&#10;    Error: connect ECONNREFUSED 127.0.0.1:5000&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '127.0.0.1',&#10;      port: 5000&#10;    }&#10;  ],&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': undefined,&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    baseURL: 'http://localhost:5000',&#10;    method: 'get',&#10;    url: '/health',&#10;    data: undefined&#10;  },&#10;  request: &lt;ref *1&gt; Writable {&#10;    _events: {&#10;      close: undefined,&#10;      error: [Function: handleRequestError],&#10;      prefinish: undefined,&#10;      finish: undefined,&#10;      drain: undefined,&#10;      response: [Function: handleResponse],&#10;      socket: [Function: handleRequestSocket]&#10;    },&#10;    _writableState: WritableState {&#10;      highWaterMark: 16384,&#10;      length: 0,&#10;      corked: 0,&#10;      onwrite: [Function: bound onwrite],&#10;      writelen: 0,&#10;      bufferedIndex: 0,&#10;      pendingcb: 0,&#10;      [Symbol(kState)]: 17580812,&#10;      [Symbol(kBufferedValue)]: null&#10;    },&#10;    _maxListeners: undefined,&#10;    _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/health',&#10;      method: 'GET',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '5000',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/health'&#10;    },&#10;    _ended: true,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 0,&#10;    _requestBodyBuffers: [],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: true,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: false,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: 0,&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'GET /health HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:5000\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'GET',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/health',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:5000/health',&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: AggregateError [ECONNREFUSED]: &#10;      at internalConnectMultiple (node:net:1118:18)&#10;      at afterConnectMultiple (node:net:1685:7) {&#10;    code: 'ECONNREFUSED',&#10;    [errors]: [ [Error], [Error] ]&#10;  }&#10;}&#10;Server is running on http://localhost:3000&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Failed to initialize Ollama service: 'OllamaService' object has no attribute '_ensure_clean_database_state'&#10;&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [103777]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Failed to initialize Ollama service: 'OllamaService' object has no attribute '_ensure_clean_database_state'&#10;&#10;Server initialization failed: AxiosError [AggregateError]&#10;    at AxiosError.from (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:876:14)&#10;    at RedirectableRequest.handleRequestError (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3156:25)&#10;    at RedirectableRequest.emit (node:events:519:28)&#10;    at eventHandlers.&lt;computed&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/follow-redirects/index.js:49:24)&#10;    at ClientRequest.emit (node:events:519:28)&#10;    at emitErrorEvent (node:_http_client:108:11)&#10;    at Socket.socketErrorListener (node:_http_client:511:5)&#10;    at Socket.emit (node:events:519:28)&#10;    at emitErrorNT (node:internal/streams/destroy:169:8)&#10;    at emitErrorCloseNT (node:internal/streams/destroy:128:3)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async Timeout.initializeServer [as _onTimeout] (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:114:26) {&#10;  code: 'ECONNREFUSED',&#10;  errors: [&#10;    Error: connect ECONNREFUSED ::1:5000&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '::1',&#10;      port: 5000&#10;    },&#10;    Error: connect ECONNREFUSED 127.0.0.1:5000&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '127.0.0.1',&#10;      port: 5000&#10;    }&#10;  ],&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': undefined,&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    baseURL: 'http://localhost:5000',&#10;    method: 'get',&#10;    url: '/health',&#10;    data: undefined&#10;  },&#10;  request: &lt;ref *1&gt; Writable {&#10;    _events: {&#10;      close: undefined,&#10;      error: [Function: handleRequestError],&#10;      prefinish: undefined,&#10;      finish: undefined,&#10;      drain: undefined,&#10;      response: [Function: handleResponse],&#10;      socket: [Function: handleRequestSocket]&#10;    },&#10;    _writableState: WritableState {&#10;      highWaterMark: 16384,&#10;      length: 0,&#10;      corked: 0,&#10;      onwrite: [Function: bound onwrite],&#10;      writelen: 0,&#10;      bufferedIndex: 0,&#10;      pendingcb: 0,&#10;      [Symbol(kState)]: 17580812,&#10;      [Symbol(kBufferedValue)]: null&#10;    },&#10;    _maxListeners: undefined,&#10;    _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/health',&#10;      method: 'GET',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '5000',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/health'&#10;    },&#10;    _ended: true,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 0,&#10;    _requestBodyBuffers: [],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: true,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: false,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: 0,&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'GET /health HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:5000\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'GET',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/health',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:5000/health',&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: AggregateError [ECONNREFUSED]: &#10;      at internalConnectMultiple (node:net:1118:18)&#10;      at afterConnectMultiple (node:net:1685:7) {&#10;    code: 'ECONNREFUSED',&#10;    [errors]: [ [Error], [Error] ]&#10;  }&#10;}&#10;^CShutting down server...&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Failed to initialize Ollama service: 'OllamaService' object has no attribute '_ensure_clean_database_state'&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;Ollama service error: ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/starlette/routing.py&quot;, line 743, in lifespan&#10;    await receive()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/uvicorn/lifespan/on.py&quot;, line 137, in receive&#10;    return await self.receive_queue.get()&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/usr/lib64/python3.12/asyncio/queues.py&quot;, line 158, in get&#10;    await getter&#10;asyncio.exceptions.CancelledError&#10;&#10;&#10;Ollama service: Startup error: Failed to initialize after 3 attempts: 'OllamaService' object has no attribute '_ensure_clean_database_state'&#10;&#10;Ollama service exited with code 0&#10;&#10;Process finished with exit code 0&#10;" />
        <option value="please look at the files and find out why this error is happening" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [104650]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service error: INFO:__main__:Initializing Ollama service...&#10;INFO:__main__:Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace&#10;&#10;Ollama service error: ERROR:__main__:Database validation failed: External error: RuntimeError: Task was aborted&#10;ERROR:__main__:Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Server initialization failed: AxiosError [AggregateError]&#10;    at AxiosError.from (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:876:14)&#10;    at RedirectableRequest.handleRequestError (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3156:25)&#10;    at RedirectableRequest.emit (node:events:519:28)&#10;    at eventHandlers.&lt;computed&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/follow-redirects/index.js:49:24)&#10;    at ClientRequest.emit (node:events:519:28)&#10;    at emitErrorEvent (node:_http_client:108:11)&#10;    at Socket.socketErrorListener (node:_http_client:511:5)&#10;    at Socket.emit (node:events:519:28)&#10;    at emitErrorNT (node:internal/streams/destroy:169:8)&#10;    at emitErrorCloseNT (node:internal/streams/destroy:128:3)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:114:26) {&#10;  code: 'ECONNREFUSED',&#10;  errors: [&#10;    Error: connect ECONNREFUSED ::1:5000&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '::1',&#10;      port: 5000&#10;    },&#10;    Error: connect ECONNREFUSED 127.0.0.1:5000&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '127.0.0.1',&#10;      port: 5000&#10;    }&#10;  ],&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': undefined,&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    baseURL: 'http://localhost:5000',&#10;    method: 'get',&#10;    url: '/health',&#10;    data: undefined&#10;  },&#10;  request: &lt;ref *1&gt; Writable {&#10;    _events: {&#10;      close: undefined,&#10;      error: [Function: handleRequestError],&#10;      prefinish: undefined,&#10;      finish: undefined,&#10;      drain: undefined,&#10;      response: [Function: handleResponse],&#10;      socket: [Function: handleRequestSocket]&#10;    },&#10;    _writableState: WritableState {&#10;      highWaterMark: 16384,&#10;      length: 0,&#10;      corked: 0,&#10;      onwrite: [Function: bound onwrite],&#10;      writelen: 0,&#10;      bufferedIndex: 0,&#10;      pendingcb: 0,&#10;      [Symbol(kState)]: 17580812,&#10;      [Symbol(kBufferedValue)]: null&#10;    },&#10;    _maxListeners: undefined,&#10;    _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/health',&#10;      method: 'GET',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '5000',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/health'&#10;    },&#10;    _ended: true,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 0,&#10;    _requestBodyBuffers: [],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: true,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: false,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: 0,&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'GET /health HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:5000\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'GET',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/health',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:5000/health',&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: AggregateError [ECONNREFUSED]: &#10;      at internalConnectMultiple (node:net:1118:18)&#10;      at afterConnectMultiple (node:net:1685:7) {&#10;    code: 'ECONNREFUSED',&#10;    [errors]: [ [Error], [Error] ]&#10;  }&#10;}&#10;Server is running on http://localhost:3000&#10;Ollama service error: INFO:__main__:Initializing Ollama service...&#10;&#10;Ollama service error: INFO:__main__:Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;&#10;Ollama service error: ERROR:__main__:Database validation failed: External error: RuntimeError: Task was aborted&#10;ERROR:__main__:Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Starting Ollama service...&#10;^CShutting down server...&#10;Ollama service error: Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 677, in _resolve_forward_ref&#10;&#10;Ollama service exited with code null&#10;Ollama service crashed, attempting restart...&#10;&#10;Process finished with exit code 0&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Starting Ollama service...&#10;Error starting Ollama service: Error: EISDIR: illegal operation on a directory, unlink '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data/space_training.lance'&#10;    at Object.unlinkSync (node:fs:1884:11)&#10;    at /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:77:28&#10;    at Array.forEach (&lt;anonymous&gt;)&#10;    at startOllamaService (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:75:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:144:9) {&#10;  errno: -21,&#10;  code: 'EISDIR',&#10;  syscall: 'unlink',&#10;  path: '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data/space_training.lance'&#10;}&#10;Retrying in 10 seconds (1/3)...&#10;Error starting Ollama service: Error: EISDIR: illegal operation on a directory, unlink '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data/space_training.lance'&#10;    at Object.unlinkSync (node:fs:1884:11)&#10;    at /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:77:28&#10;    at Array.forEach (&lt;anonymous&gt;)&#10;    at startOllamaService (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:75:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:144:9) {&#10;  errno: -21,&#10;  code: 'EISDIR',&#10;  syscall: 'unlink',&#10;  path: '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data/space_training.lance'&#10;}&#10;Retrying in 10 seconds (2/3)...&#10;^CShutting down server...&#10;&#10;Process finished with exit code 0&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Starting Ollama service...&#10;Error starting Ollama service: Error: EISDIR: illegal operation on a directory, unlink '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data/space_training.lance'&#10;    at Object.unlinkSync (node:fs:1884:11)&#10;    at /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:77:28&#10;    at Array.forEach (&lt;anonymous&gt;)&#10;    at startOllamaService (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:75:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:144:9) {&#10;  errno: -21,&#10;  code: 'EISDIR',&#10;  syscall: 'unlink',&#10;  path: '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data/space_training.lance'&#10;}&#10;Retrying in 10 seconds (1/3)...&#10;Error starting Ollama service: Error: EISDIR: illegal operation on a directory, unlink '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data/space_training.lance'&#10;    at Object.unlinkSync (node:fs:1884:11)&#10;    at /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:77:28&#10;    at Array.forEach (&lt;anonymous&gt;)&#10;    at startOllamaService (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:75:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:144:9) {&#10;  errno: -21,&#10;  code: 'EISDIR',&#10;  syscall: 'unlink',&#10;  path: '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data/space_training.lance'&#10;}&#10;Retrying in 10 seconds (2/3)...&#10;^CShutting down server...&#10;&#10;Process finished with exit code 0&#10;&#10;" />
        <option value="look at the server.js file" />
        <option value="the python service should launch seperately, as a micro service, and the server seperately as ell" />
        <option value="ill manually start the python " />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Cannot connect to Ollama service: Request failed with status code 500&#10;Server is running on http://localhost:3000&#10;Cannot connect to Ollama service: Request failed with status code 500&#10;Cannot connect to Ollama service: Request failed with status code 500&#10;" />
        <option value="Cannot connect to Ollama service: Request failed with status code 500&#10;Server is running on http://localhost:3000&#10;Cannot connect to Ollama service: Request failed with status code 500&#10;" />
        <option value="why is there a fucking start_ollama_service file? dont make random fucking files ill start it from the terminal" />
        <option value=" raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:16:47] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:16:52,117] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:16:52] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:16:57,124] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:16:57] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:02,129] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:02] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:07,135] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:07] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:12,142] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:12] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:17,149] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:17] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:22,154] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:22] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:27,159] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:27] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:32,167] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:32] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:37,170] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:37] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:42,175] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:42] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:47,182] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:47] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:52,188] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:52] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:57,193] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:57] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;" />
        <option value="wouldnt fastap be better here?" />
        <option value="we dont need fast api and flask" />
        <option value="no! i want it to be a seperate service damn" />
        <option value="no mother fucker stop wasting my credits, the ollama should be a microservice with its own endpoints that connect to the server.js" />
        <option value="Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Ollama service initialization complete.&#10;INFO:     127.0.0.1:57968 - &quot;POST /initialize HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:50908 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;" />
        <option value="how long does llama take to complete" />
        <option value="i man to initialize the llama" />
        <option value="does this code actually initialize llmama" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Cannot connect to Ollama service: Request failed with status code 500&#10;Server is running on http://localhost:3000&#10;Cannot connect to Ollama service: Request failed with status code 500&#10;Cannot connect to Ollama service: Request failed with status code 500&#10;&#10;&#10;v) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [108119]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:49956 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:49956 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:48508 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:55470 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:55488 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:49694 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [108119]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [109153]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:36142 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:36154 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:35818 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:48348 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;&#10;" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [108119]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:49956 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:49956 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:48508 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:55470 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:55488 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:49694 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [108119]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [109153]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:36142 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:36154 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:35818 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:48348 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [109153]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [109371]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:38056 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:38056 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:38068 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:38068 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:41850 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:41850 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:41856 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:41856 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:35590 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:35590 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:42296 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:42296 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:42304 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:42304 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;^CINFO:     127.0.0.1:50232 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;" />
        <option value="it should start initializing regardless of server.js" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [108119]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:49956 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:49956 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:48508 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:55470 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:55488 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:49694 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [108119]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [109153]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:36142 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:36154 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:35818 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:48348 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [109153]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [109371]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:38056 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:38056 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:38068 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:38068 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:41850 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:41850 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:41856 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:41856 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:35590 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:35590 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:42296 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:42296 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:42304 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:42304 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;^CINFO:     127.0.0.1:50232 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [109371]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [109778]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:45628 - &quot;GET / HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:45628 - &quot;GET /favicon.ico HTTP/1.1&quot; 404 Not Found&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [109778]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [110003]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;^[[Ai^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [110003]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [110102]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Request error during chat: 500 Server Error: Internal Server Error for url: http://localhost:11434/api/generate&#10;Initialization failed: Model test failed: Failed to get response from Ollama: 500 Server Error: Internal Server Error for url: http://localhost:11434/api/generate&#10;Initial startup initialization failed: Model test failed: Failed to get response from Ollama: 500 Server Error: Internal Server Error for url: http://localhost:11434/api/generate&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [109371]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:38056 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:38056 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:38068 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:38068 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:41850 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:41850 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:41856 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:41856 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:35590 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:35590 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:42296 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:42296 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:42304 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:42304 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;^CINFO:     127.0.0.1:50232 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [109371]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [109778]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:45628 - &quot;GET / HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:45628 - &quot;GET /favicon.ico HTTP/1.1&quot; 404 Not Found&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ cd webapps&#10;bash: cd: webapps: No such file or directory&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ cd webapp&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp$ cd services&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [110258]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Initialization failed: 'OllamaService' object has no attribute 'check_ollama_running'&#10;Initial startup initialization failed: 'OllamaService' object has no attribute 'check_ollama_running'&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [110361]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Initialization failed: 'OllamaService' object has no attribute 'pull_model'&#10;Initial startup initialization failed: 'OllamaService' object has no attribute 'pull_model'&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;" />
        <option value="Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: 500 Server Error: Internal Server Error for url: http://localhost:11434/api/generate&#10;Initial startup initialization failed: Model test failed: 500 Server Error: Internal Server Error for url: http://localhost:11434/api/generate&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit" />
        <option value="can you fix up my index and css some" />
        <option value="take another look at the formating" />
        <option value="ry harder to make the formatting better" />
        <option value="ok ideally instead of just chatting, i want to use the lancedb database in the data folder, inside of which is a table with questions for the llm to ask the user and judge their answer" />
        <option value="ok i want the llm to ask questions from the lancedb in the project" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:221: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [124645]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;" />
        <option value="the table is called space_training" />
        <option value="emew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:221: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [124645]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;^Z&#10;[1]+  Stopped                 python ollama.py&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:221: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [124696]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:221: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [124715]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ ^C&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:196: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [124767]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1311, in mkdir&#10;    os.mkdir(self, mode)&#10;FileNotFoundError: [Errno 2] No such file or directory: '/webapp/data'&#10;&#10;During handling of the above exception, another exception occurred:&#10;&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 13, in &lt;module&gt;&#10;    db = lancedb.connect('/webapp/data')&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/__init__.py&quot;, line 115, in connect&#10;    return LanceDBConnection(uri, read_consistency_interval=read_consistency_interval)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/db.py&quot;, line 339, in __init__&#10;    Path(uri).mkdir(parents=True, exist_ok=True)&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1315, in mkdir&#10;    self.parent.mkdir(parents=True, exist_ok=True)&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1311, in mkdir&#10;    os.mkdir(self, mode)&#10;PermissionError: [Errno 13] Permission denied: '/webapp'&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1311, in mkdir&#10;    os.mkdir(self, mode)&#10;FileNotFoundError: [Errno 2] No such file or directory: '/webapp/data'&#10;&#10;During handling of the above exception, another exception occurred:&#10;&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 13, in &lt;module&gt;&#10;    db = lancedb.connect('/webapp/data')&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/__init__.py&quot;, line 115, in connect&#10;    return LanceDBConnection(uri, read_consistency_interval=read_consistency_interval)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/db.py&quot;, line 339, in __init__&#10;    Path(uri).mkdir(parents=True, exist_ok=True)&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1315, in mkdir&#10;    self.parent.mkdir(parents=True, exist_ok=True)&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1311, in mkdir&#10;    os.mkdir(self, mode)&#10;PermissionError: [Errno 13] Permission denied: '/webapp'&#10;(venv) barthmalemew@ladmin:~/WebstormP" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1311, in mkdir&#10;    os.mkdir(self, mode)&#10;FileNotFoundError: [Errno 2] No such file or directory: '/webapp/data'&#10;&#10;During handling of the above exception, another exception occurred:&#10;&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 13, in &lt;module&gt;&#10;    db = lancedb.connect('/webapp/data')&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/__init__.py&quot;, line 115, in connect&#10;    return LanceDBConnection(uri, read_consistency_interval=read_consistency_interval)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/db.py&quot;, line 339, in __init__&#10;    Path(uri).mkdir(parents=True, exist_ok=True)&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1315, in mkdir&#10;    self.parent.mkdir(parents=True, exist_ok=True)&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1311, in mkdir&#10;    os.mkdir(self, mode)&#10;PermissionError: [Errno 13] Permission denied: '/webapp'&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="whats wrong with this" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1311, in mkdir&#10;    os.mkdir(self, mode)&#10;FileNotFoundError: [Errno 2] No such file or directory: '/webapp/data'&#10;&#10;During handling of the above exception, another exception occurred:&#10;&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 13, in &lt;module&gt;&#10;    db = lancedb.connect('/webapp/data')&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/__init__.py&quot;, line 115, in connect&#10;    return LanceDBConnection(uri, read_consistency_interval=read_consistency_interval)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/db.py&quot;, line 339, in __init__&#10;    Path(uri).mkdir(parents=True, exist_ok=True)&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1315, in mkdir&#10;    self.parent.mkdir(parents=True, exist_ok=True)&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1311, in mkdir&#10;    os.mkdir(self, mode)&#10;PermissionError: [Errno 13] Permission denied: '/webapp'&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ ^C&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:305: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [125930]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:305: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [125953]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="look at the file" />
        <option value="fix the files" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:331: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;Starting server on port 5002&#10;INFO:     Started server process [126406]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Model test error: Model test failed with status 500: {&quot;error&quot;:&quot;model requires more system memory (8.4 GiB) than is available (7.6 GiB)&quot;}&#10;Initialization failed: Model test failed: Model test failed with status 500: {&quot;error&quot;:&quot;model requires more system memory (8.4 GiB) than is available (7.6 GiB)&quot;}&#10;Initialization failed: Model test failed: Model test failed with status 500: {&quot;error&quot;:&quot;model requires more system memory (8.4 GiB) than is available (7.6 GiB)&quot;}&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5002 (Press CTRL+C to quit)&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [126406]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ ^C&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:196: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [127015]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:196: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [127018]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="nope dont chnage the model" />
        <option value="ok what if i want to make a database of question for the model to ask the user" />
        <option value="lets avoid lance db, ill give you free rain to choose the best technology for the job" />
        <option value="would mongo db be viable alternaative" />
        <option value="not worth it at the moment" />
        <option value="Mechanical Problem: You are in the space ship doing maintenence and you notice your oxygen tanks are leaking, what do you do in this situation?&#10;[5:07 PM]&#10;Navigational problem: What would the steps be when preparing to make a landing on mars?                                                                              Resource Management: How would you go about when preparing food in zero gravity?&#10; here are the sample questions" />
        <option value="ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.&#10;llama-index-core 0.11.21 requires pydantic&lt;3.0.0,&gt;=2.7.0, but you have pydantic 2.5.2 which is incompatible.&#10;Successfully installed dnspython-2.7.0 motor-3.3.2 pydantic-2.5.2 pydantic-core-2.14.5 pymongo-4.6.1&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="we are doing sql lite" />
        <option value="remove everything related to mongo db" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:291: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 382, in &lt;module&gt;&#10;    import uvicorn&#10;ModuleNotFoundError: No module named 'uvicorn'&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="ok i need to make a simple sql database that holds questions, these questions will be presented tot he user where they will be asked to answer them while the llama model proctors them" />
        <option value="Mechanical Problem: You are in the space ship doing maintenence and you notice your oxygen tanks are leaking, what do you do in this situation?&#10;[5:07 PM]&#10;Navigational problem: What would the steps be when preparing to make a landing on mars?                                                                              Resource Management: How would you go about when preparing food in zero gravity? here are some of the example questions" />
        <option value="why is ollama.py giving errors" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 438&#10;    return question if question else {&quot;Here is the continued modified `webapp/services/ollama.py` file:&#10;                                      ^&#10;SyntaxError: unterminated string literal (detected at line 438)&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:354: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="the ollama.py is giving a lot of errors" />
        <option value="theres still errors all over th eproject" />
        <option value="dude ollama_service is gone can you not see that" />
        <option value="settings and database mamanger also dont exist, also is config.py even used for anything or just junk" />
        <option value="the databse should just run through the server.js" />
        <option value="obiouslt config.py should exist, like i said the database is used by the jerver.js file to go to the front end" />
        <option value="no dumbass there shouldnt be a config.py" />
        <option value="all the database stuff should go through server.js and nothing the fuck else" />
        <option value="youre wasting credits on purpose, the database, with questions should be accessed by script.js and server.js dont make up new file or hallucentate ones" />
        <option value="&lt;!DOCTYPE html&gt;&#10;&lt;html lang=&quot;en&quot;&gt;&#10;&lt;head&gt;&#10;    &lt;!-- &#10;    NASA Space Duck Chat Interface&#10;    Main chat interface for space training simulation&#10;    Updated with modern NASA styling&#10;    --&gt;&#10;    &lt;meta charset=&quot;UTF-8&quot;&gt;&#10;    &lt;title&gt;NASA Space Duck&lt;/title&gt;&#10;    &lt;link rel=&quot;stylesheet&quot; href=&quot;styles.css&quot;&gt;&#10;&lt;/head&gt;&#10;&lt;body&gt;&#10;    &lt;h1&gt;NASA Space Duck&lt;/h1&gt;&#10;    &lt;div class=&quot;block&quot; id=&quot;rotate2D&quot;&gt;&lt;/div&gt;&#10;    &lt;div class=&quot;nasa-logo&quot;&gt;&lt;/div&gt;&#10;    &lt;div class=&quot;chat-container&quot;&gt;&#10;        &lt;div class=&quot;chat-messages&quot; id=&quot;chatMessages&quot;&gt;&#10;            &lt;div class=&quot;message bot-message&quot;&gt;&#10;                Welcome to NASA's Virtual Training Portal. I'm your AI instructor, ready to guide you through space science, &#10;                astronaut training, and space exploration. How can I assist you today?&#10;            &lt;/div&gt;&#10;        &lt;/div&gt;&#10;        &lt;div class=&quot;chat-input-container&quot;&gt;&#10;            &lt;textarea &#10;                id=&quot;chatInput&quot; &#10;                placeholder=&quot;Ask a question about space...&quot;&#10;                rows=&quot;3&quot;&#10;                cols=&quot;50&quot;&#10;                aria-label=&quot;Chat input&quot;&#10;            &gt;&lt;/textarea&gt;&#10;            &lt;button id=&quot;sendButton&quot;&gt;Send&lt;/button&gt;&#10;        &lt;/div&gt;&#10;    &lt;/div&gt;&#10;    &lt;script src=&quot;scripts.js&quot;&gt;&lt;/script&gt;&#10;&lt;/body&gt;&#10;&lt;/html&gt;&#10;&#10;the index.html chatbox should have this functionality" />
        <option value="i like the previous lay out though!!!! i just wanted the chatbox to work like this not erase all of the other functions" />
        <option value="sigh the chat box is not function at all as it should, its randomly giving questions from the database when there should be a drop down box to select questions by cataory this also means the llamama communication is broken" />
        <option value="now neither the questions or llama chatbox work" />
        <option value="s/lib/router/index.js:284:15&#10;    at param (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:365:14)&#10;    at param (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:376:14)&#10;    at Function.process_params (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:421:3) {&#10;  code: 'SQLITE_ERROR'&#10;}&#10;Detailed chat error: {&#10;  detail: &quot;Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'\n&quot; +&#10;    'For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500'&#10;}&#10;Detailed chat error: {&#10;  detail: &quot;Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'\n&quot; +&#10;    'For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500'&#10;}&#10;Detailed chat error: {&#10;  detail: &quot;Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'\n&quot; +&#10;    'For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500'&#10;}&#10;Detailed chat error: {&#10;  detail: &quot;Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'\n&quot; +&#10;    'For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500'&#10;}&#10;Detailed chat error: {&#10;  detail: &quot;Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'\n&quot; +&#10;    'For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500'&#10;}&#10;Detailed chat error: {&#10;  detail: &quot;Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'\n&quot; +&#10;    'For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500'&#10;}&#10;Detailed chat error: {&#10;  detail: &quot;Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'\n&quot; +&#10;    'For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500'&#10;}&#10;Error fetching question: SqliteError: near &quot;WHERE&quot;: syntax error&#10;    at Database.prepare (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/better-sqlite3/lib/methods/wrappers.js:5:21)&#10;    at /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:150:18&#10;    at Layer.handle [as handle_request] (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/layer.js:95:5)&#10;    at next (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/route.js:149:13)&#10;    at Route.dispatch (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/route.js:119:3)&#10;    at Layer.handle [as handle_request] (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/layer.js:95:5)&#10;    at /home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:284:15&#10;    at param (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:365:14)&#10;    at param (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:376:14)&#10;    at Function.process_params (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:421:3) {&#10;  code: 'SQLITE_ERROR'&#10;}&#10;Error fetching question: SqliteError: near &quot;WHERE&quot;: syntax error&#10;    at Database.prepare (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/better-sqlite3/lib/methods/wrappers.js:5:21)&#10;    at /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:150:18&#10;    at Layer.handle [as handle_request] (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/layer.js:95:5)&#10;    at next (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/route.js:149:13)&#10;    at Route.dispatch (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/route.js:119:3)&#10;    at Layer.handle [as handle_request] (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/layer.js:95:5)&#10;    at /home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:284:15&#10;    at param (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:365:14)&#10;    at param (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:376:14)&#10;    at Function.process_params (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:421:3) {&#10;  code: 'SQLITE_ERROR'&#10;}&#10;&#10;" />
        <option value="you broke it" />
        <option value="fucking idiot im saying your changes to ollama.py broke the model. " />
        <option value="make sure ollama.py actually shuts down the process when it stops" />
        <option value="        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [154714]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [154714]&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:197: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [154890]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server disconnected without sending a response.&#10;Initialization failed: Model test failed: Server disconnected without sending a response.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;" />
        <option value="INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:55306 - &quot;GET /status HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:55320 - &quot;GET /status HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:36548 - &quot;GET /status HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:36560 - &quot;GET /status HTTP/1.1&quot; 404 Not Found&#10;&#10;" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 195, in &lt;module&gt;&#10;    app = FastAPI(lifespan=lifespan)&#10;                           ^^^^^^^^&#10;NameError: name 'lifespan' is not defined&#10;" />
        <option value="you keep adding random shit to a program that worked before when its the server.js changes that broke it" />
        <option value="but the ollama.py is still broken" />
        <option value="just add proper process killing to ollama.py" />
        <option value="sigh nope try again" />
      </list>
    </option>
    <option name="selectedModel" value="codebuddy:CLAUDE_3.5_SONNET_V7" />
    <option name="ttsEnabled" value="false" />
  </component>
</project>