<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CodebuddyPersistentProjectState">
    <option name="additionalOptionsVisible" value="true" />
    <option name="autoFileSelection" value="true" />
    <option name="promptHistory">
      <list>
        <option value="ok i want to set up the current project to be a node web app using the files i already created" />
        <option value="yes" />
        <option value="ry again with the files ive given you" />
        <option value="ok in the llm directory, i want to install ollama" />
        <option value="what does ollama.js do" />
        <option value="wats the best way to integrate ollama into this current project" />
        <option value="youre allowed to creat files if needed" />
        <option value="llama2 is installed" />
        <option value="how do i modify the llama to act as a nasa instructor" />
        <option value="do i need to run the ollama.js as well?" />
        <option value="how do i start it then?" />
        <option value="ive made changes to the port used" />
        <option value="Error: listen tcp 127.0.0.1:11434: bind: address already in use&#10;" />
        <option value="whats wrong with my code" />
        <option value="please actually look at the codebase, something is wrong" />
        <option value="please document y code thoruohly to explain everythings use" />
        <option value="i didt ask you to touch the style sheet, i just want mroe comments" />
        <option value="anything wrong with my code?" />
        <option value="Chat error: Error: Failed to get response from Ollama: Request failed with status code 500&#10;    at OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:104:19)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:70:26&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed: &#10;pulling manifest &#10;pulling 8934d96d3f08... 100% ▕████████████████▏ 3.8 GB                         &#10;pulling 8c17c2ebb0ea... 100% ▕████████████████▏ 7.0 KB                         &#10;pulling 7c23fb36d801... 100% ▕████████████████▏ 4.8 KB                         &#10;pulling 2e0493f67d0c... 100% ▕████████████████▏   59 B                         &#10;pulling fa304d675061... 100% ▕████████████████▏   91 B                         &#10;pulling 42ba7f8a01dd... 100% ▕████████████████▏  557 B                         &#10;verifying sha256 digest &#10;writing manifest &#10;success &#10;&#10;Ollama service initialization complete&#10;Server initialization complete, ready to handle requests&#10;Server is running on http://localhost:3000&#10;[68hgjd] Starting chat request&#10;[68hgjd] Ollama chat error: {&#10;  message: 'Request failed with status code 500',&#10;  status: 500,&#10;  data: {&#10;    error: 'model requires more system memory (8.4 GiB) than is available (8.3 GiB)'&#10;  }&#10;}&#10;Chat error: Error: Ollama server error. The model might be overloaded or experiencing issues.&#10;    at OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:133:23)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:92:26&#10;[bngu8o] Starting chat request&#10;[bngu8o] Ollama chat error: { message: '', status: undefined, data: undefined }&#10;Chat error: Error: Could not connect to Ollama. Please ensure Ollama is running.&#10;    at OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:131:23)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:92:26&#10;&#10;&#10;" />
        <option value="what if i want to add lanceDB to this project" />
        <option value="/usr/bin/npm install&#10;npm error code E404&#10;npm error 404 Not Found - GET https://registry.npmjs.org/lance-db - Not found&#10;npm error 404&#10;npm error 404  'lance-db@^0.5.0' is not in this registry.&#10;npm error 404&#10;npm error 404 Note that you can also install from a&#10;npm error 404 tarball, folder, http url, or git url.&#10;npm error A complete log of this run can be found in: /home/barthmalemew/.npm/_logs/2024-11-02T20_05_43_695Z-debug-0.log&#10;&#10;Process finished with exit code 1&#10;" />
        <option value="but i want lance db" />
        <option value="/usr/bin/npm install&#10;npm error code ETARGET&#10;npm error notarget No matching version found for vectorizer@^1.2.0.&#10;npm error notarget In most cases you or one of your dependencies are requesting&#10;npm error notarget a package version that doesn't exist.&#10;npm error A complete log of this run can be found in: /home/barthmalemew/.npm/_logs/2024-11-02T20_08_30_028Z-debug-0.log&#10;&#10;Process finished with exit code 1&#10;" />
        <option value="cool, so ideally i want to store question in the db for the LLM to access, these will serve as the questions the LLM has with the user" />
        <option value="is anything redundat in my code" />
        <option value="I want the LLM itself to interpret if the answer is correct, like a teacher, and point our possible mistakes the user made" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed: &#10;pulling manifest &#10;pulling 8934d96d3f08... 100% ▕████████████████▏ 3.8 GB                         &#10;pulling 8c17c2ebb0ea... 100% ▕████████████████▏ 7.0 KB                         &#10;pulling 7c23fb36d801... 100% ▕████████████████▏ 4.8 KB                         &#10;pulling 2e0493f67d0c... 100% ▕████████████████▏   59 B                         &#10;pulling fa304d675061... 100% ▕████████████████▏   91 B                         &#10;pulling 42ba7f8a01dd... 100% ▕████████████████▏  557 B                         &#10;verifying sha256 digest &#10;writing manifest &#10;success &#10;&#10;Ollama service initialization complete&#10;Database initialization failed: Error: At least one record or a schema needs to be provided&#10;    at makeArrowTable (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/@lancedb/lancedb/dist/arrow.js:298:15)&#10;    at Connection.createTable (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/@lancedb/lancedb/dist/connection.js:116:48)&#10;    at DatabaseService.initialize (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/database.js:30:44)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:42:9)&#10;Server initialization failed: AppError: Database initialization failed&#10;    at DatabaseService.initialize (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/database.js:54:19)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:42:9) {&#10;  statusCode: 500,&#10;  status: 'error'&#10;}&#10;&#10;Process finished with exit code 1&#10;&#10;" />
        <option value="yes we need to give the database data" />
        <option value="how do i put data in the data.db" />
        <option value="cant i just manually enter data into the file?" />
        <option value="doesnt data bricks have tool to do this easier?" />
        <option value="please remove all code and files that arent needed for the project, like the insertsion script" />
        <option value="please remove files that arent usful" />
        <option value="are those all really needed?" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Server initialization failed: TypeError: ollamaService.initialize is not a function&#10;    at initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:17:29)&#10;    at Object.&lt;anonymous&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:50:1)&#10;    at Module._compile (node:internal/modules/cjs/loader:1469:14)&#10;    at Module._extensions..js (node:internal/modules/cjs/loader:1548:10)&#10;    at Module.load (node:internal/modules/cjs/loader:1288:32)&#10;    at Module._load (node:internal/modules/cjs/loader:1104:12)&#10;    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:174:12)&#10;    at node:internal/main/run_main_module:28:49&#10;&#10;Process finished with exit code 1&#10;&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Initializing Ollama service...&#10;Ollama initialization failed: Model llama2 not found in Ollama server&#10;Server initialization failed: Error: Failed to initialize Ollama service: Model llama2 not found in Ollama server&#10;    at OllamaService.initialize (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:32:19)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:26:9)&#10;Failed to initialize server: Error: Failed to initialize Ollama service: Model llama2 not found in Ollama server&#10;    at OllamaService.initialize (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:32:19)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:26:9)&#10;&#10;Process finished with exit code 1&#10;&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Initializing Ollama service...&#10;Model llama2 not found. Attempting to pull...&#10;Pulling model llama2...&#10;Successfully pulled model llama2&#10;Ollama service initialized with model: llama2&#10;Server initialization complete, ready to handle requests&#10;Server is running on http://localhost:3000&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:59:30)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:51:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '612',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '612',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [Timeout],&#10;        _idleStart: 6892,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 137,&#10;        [Symbol(triggerId)]: 135&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 612\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: false,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 612,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 20:48:44 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 0,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '612',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 612\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (7.8 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:59:30)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:51:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '612',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '612',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [TimersList],&#10;        _idleStart: 24960,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 175,&#10;        [Symbol(triggerId)]: 173&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 612\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: false,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 612,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 20:49:02 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 0,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '612',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 612\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (8.0 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;&#10;" />
        <option value="woah wtf why are pulling orca mini???" />
        <option value="      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '612',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '612',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [TimersList],&#10;        _idleStart: 49525,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 199,&#10;        [Symbol(triggerId)]: 197&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 612\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: false,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 612,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      _timeout: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 20:54:05 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 60000,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '612',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 612\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (7.1 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;Chat error: Error: Failed to get response from Ollama: Request failed with status code 500&#10;    at OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:106:19)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:60:26&#10;&#10;" />
        <option value="no utility files, they were needed before" />
        <option value="can you make sure llama2 process ends when the server stops?" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;This may take several minutes...&#10;Model llama2 pull completed: &#10;pulling manifest &#10;pulling 8934d96d3f08... 100% ▕████████████████▏ 3.8 GB                         &#10;pulling 8c17c2ebb0ea... 100% ▕████████████████▏ 7.0 KB                         &#10;pulling 7c23fb36d801... 100% ▕████████████████▏ 4.8 KB                         &#10;pulling 2e0493f67d0c... 100% ▕████████████████▏   59 B                         &#10;pulling fa304d675061... 100% ▕████████████████▏   91 B                         &#10;pulling 42ba7f8a01dd... 100% ▕████████████████▏  557 B                         &#10;verifying sha256 digest &#10;writing manifest &#10;success &#10;&#10;Successfully initialized with model: llama2&#10;Ollama service initialization complete&#10;Server initialization complete, ready to handle requests&#10;Server is running on http://localhost:3000&#10;Ollama chat error: AxiosError: socket hang up&#10;    at AxiosError.from (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:876:14)&#10;    at RedirectableRequest.handleRequestError (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3156:25)&#10;    at RedirectableRequest.emit (node:events:531:35)&#10;    at eventHandlers.&lt;computed&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/follow-redirects/index.js:49:24)&#10;    at ClientRequest.emit (node:events:519:28)&#10;    at emitErrorEvent (node:_http_client:108:11)&#10;    at Socket.socketOnEnd (node:_http_client:535:5)&#10;    at Socket.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:87:30)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:61:26 {&#10;  code: 'ECONNRESET',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 60000,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '612',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;  },&#10;  request: &lt;ref *1&gt; Writable {&#10;    _events: {&#10;      close: undefined,&#10;      error: [Function: handleRequestError],&#10;      prefinish: undefined,&#10;      finish: undefined,&#10;      drain: undefined,&#10;      response: [Function: handleResponse],&#10;      socket: [Array],&#10;      timeout: undefined,&#10;      abort: undefined&#10;    },&#10;    _writableState: WritableState {&#10;      highWaterMark: 16384,&#10;      length: 0,&#10;      corked: 0,&#10;      onwrite: [Function: bound onwrite],&#10;      writelen: 0,&#10;      bufferedIndex: 0,&#10;      pendingcb: 0,&#10;      [Symbol(kState)]: 17580812,&#10;      [Symbol(kBufferedValue)]: null&#10;    },&#10;    _maxListeners: undefined,&#10;    _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/api/generate',&#10;      method: 'POST',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '11434',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/api/generate'&#10;    },&#10;    _ended: true,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 612,&#10;    _requestBodyBuffers: [ [Object] ],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '612',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 612\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: true,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:11434/api/generate',&#10;    _timeout: null,&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: Error: socket hang up&#10;      at Socket.socketOnEnd (node:_http_client:535:25)&#10;      at Socket.emit (node:events:531:35)&#10;      at endReadableNT (node:internal/streams/readable:1696:12)&#10;      at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {&#10;    code: 'ECONNRESET'&#10;  }&#10;}&#10;Chat error: Error: Failed to get response from Ollama: socket hang up&#10;    at OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:107:19)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:61:26&#10;&#10;" />
        <option value="no added files" />
        <option value="bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;node:internal/modules/cjs/loader:1228&#10;  throw err;&#10;  ^&#10;&#10;Error: Cannot find module './utils/connectionManager'&#10;Require stack:&#10;- /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;    at Module._resolveFilename (node:internal/modules/cjs/loader:1225:15)&#10;    at Module._load (node:internal/modules/cjs/loader:1051:27)&#10;    at Module.require (node:internal/modules/cjs/loader:1311:19)&#10;    at require (node:internal/modules/helpers:179:18)&#10;    at Object.&lt;anonymous&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:9:27)&#10;    at Module._compile (node:internal/modules/cjs/loader:1469:14)&#10;    at Module._extensions..js (node:internal/modules/cjs/loader:1548:10)&#10;    at Module.load (node:internal/modules/cjs/loader:1288:32)&#10;    at Module._load (node:internal/modules/cjs/loader:1104:12)&#10;    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:174:12) {&#10;  code: 'MODULE_NOT_FOUND',&#10;  requireStack: [ '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js' ]&#10;}&#10;&#10;Node.js v20.17.0&#10;&#10;Process finished with exit code 1&#10;&#10;" />
        <option value="no i just said no adding files" />
        <option value="no i said no new files, just use script.js ollma and server" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;This may take several minutes...&#10;Model llama2 pull completed: &#10;pulling manifest &#10;pulling 8934d96d3f08... 100% ▕████████████████▏ 3.8 GB                         &#10;pulling 8c17c2ebb0ea... 100% ▕████████████████▏ 7.0 KB                         &#10;pulling 7c23fb36d801... 100% ▕████████████████▏ 4.8 KB                         &#10;pulling 2e0493f67d0c... 100% ▕████████████████▏   59 B                         &#10;pulling fa304d675061... 100% ▕████████████████▏   91 B                         &#10;pulling 42ba7f8a01dd... 100% ▕████████████████▏  557 B                         &#10;verifying sha256 digest &#10;writing manifest &#10;success &#10;&#10;Successfully initialized with model: llama2&#10;Ollama service initialization complete&#10;Server initialization complete, ready to handle requests&#10;Server is running on http://localhost:3000&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '999',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [Timeout],&#10;        _idleStart: 7695,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 186,&#10;        [Symbol(triggerId)]: 184&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 999\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: false,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 999,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 21:11:57 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 0,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      signal: [AbortSignal],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (7.0 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;Retry attempt 1 after error: Failed to get response from Ollama: Request failed with status code 500&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '999',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [Timeout],&#10;        _idleStart: 8727,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 206,&#10;        [Symbol(triggerId)]: 204&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 999\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: true,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 999,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 21:11:58 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 0,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      signal: [AbortSignal],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: true,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (7.0 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;Retry attempt 2 after error: Failed to get response from Ollama: Request failed with status code 500&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '999',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [TimersList],&#10;        _idleStart: 10749,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 227,&#10;        [Symbol(triggerId)]: 225&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 999\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: true,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 999,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 21:12:00 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 0,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      signal: [AbortSignal],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: true,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (7.1 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;Chat error: Error: Failed to get response from Ollama: Request failed with status code 500&#10;    at OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:135:19)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '999',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [TimersList],&#10;        _idleStart: 19579,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 265,&#10;        [Symbol(triggerId)]: 263&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 999\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: false,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 999,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 21:12:09 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 0,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      signal: [AbortSignal],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (7.1 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;Retry attempt 1 after error: Failed to get response from Ollama: Request failed with status code 500&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '999',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [TimersList],&#10;        _idleStart: 20606,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 285,&#10;        [Symbol(triggerId)]: 283&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 999\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: true,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 999,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 21:12:10 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 0,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      signal: [AbortSignal],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: true,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (7.2 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;Retry attempt 2 after error: Failed to get response from Ollama: Request failed with status code 500&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '999',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [TimersList],&#10;        _idleStart: 22632,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 305,&#10;        [Symbol(triggerId)]: 303&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 999\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: true,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 999,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 21:12:12 GMT',&#10;      'content-length': '83'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 0,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      signal: [AbortSignal],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: true,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: {&#10;      error: 'model requires more system memory (8.4 GiB) than is available (7.2 GiB)'&#10;    }&#10;  },&#10;  status: 500&#10;}&#10;Chat error: Error: Failed to get response from Ollama: Request failed with status code 500&#10;    at OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:135:19)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26&#10;&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;This may take several minutes...&#10;Model llama2 pull completed: &#10;pulling manifest &#10;pulling 8934d96d3f08... 100% ▕████████████████▏ 3.8 GB                         &#10;pulling 8c17c2ebb0ea... 100% ▕████████████████▏ 7.0 KB                         &#10;pulling 7c23fb36d801... 100% ▕████████████████▏ 4.8 KB                         &#10;pulling 2e0493f67d0c... 100% ▕████████████████▏   59 B                         &#10;pulling fa304d675061... 100% ▕████████████████▏   91 B                         &#10;pulling 42ba7f8a01dd... 100% ▕████████████████▏  557 B                         &#10;verifying sha256 digest &#10;writing manifest &#10;success &#10;&#10;Successfully initialized with model: llama2&#10;Ollama service initialization complete&#10;Server initialization complete, ready to handle requests&#10;Server is running on http://localhost:3000&#10;Ollama chat error: AxiosError: socket hang up&#10;    at AxiosError.from (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:876:14)&#10;    at RedirectableRequest.handleRequestError (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3156:25)&#10;    at RedirectableRequest.emit (node:events:519:28)&#10;    at eventHandlers.&lt;computed&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/follow-redirects/index.js:49:24)&#10;    at ClientRequest.emit (node:events:519:28)&#10;    at emitErrorEvent (node:_http_client:108:11)&#10;    at Socket.socketOnEnd (node:_http_client:535:5)&#10;    at Socket.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ECONNRESET',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: Hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; Writable {&#10;    _events: {&#10;      close: undefined,&#10;      error: [Function: handleRequestError],&#10;      prefinish: undefined,&#10;      finish: undefined,&#10;      drain: undefined,&#10;      response: [Function: handleResponse],&#10;      socket: [Function: handleRequestSocket]&#10;    },&#10;    _writableState: WritableState {&#10;      highWaterMark: 16384,&#10;      length: 0,&#10;      corked: 0,&#10;      onwrite: [Function: bound onwrite],&#10;      writelen: 0,&#10;      bufferedIndex: 0,&#10;      pendingcb: 0,&#10;      [Symbol(kState)]: 17580812,&#10;      [Symbol(kBufferedValue)]: null&#10;    },&#10;    _maxListeners: undefined,&#10;    _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/api/generate',&#10;      method: 'POST',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '11434',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/api/generate'&#10;    },&#10;    _ended: true,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 999,&#10;    _requestBodyBuffers: [ [Object] ],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:11434/api/generate',&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: Error: socket hang up&#10;      at Socket.socketOnEnd (node:_http_client:535:25)&#10;      at Socket.emit (node:events:531:35)&#10;      at endReadableNT (node:internal/streams/readable:1696:12)&#10;      at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {&#10;    code: 'ECONNRESET'&#10;  }&#10;}&#10;Retry attempt 1 after error: Failed to get response from Ollama: socket hang up&#10;Ollama chat error: AxiosError [AggregateError]&#10;    at AxiosError.from (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:876:14)&#10;    at RedirectableRequest.handleRequestError (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3156:25)&#10;    at RedirectableRequest.emit (node:events:519:28)&#10;    at eventHandlers.&lt;computed&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/follow-redirects/index.js:49:24)&#10;    at ClientRequest.emit (node:events:519:28)&#10;    at emitErrorEvent (node:_http_client:108:11)&#10;    at Socket.socketErrorListener (node:_http_client:511:5)&#10;    at Socket.emit (node:events:519:28)&#10;    at emitErrorNT (node:internal/streams/destroy:169:8)&#10;    at emitErrorCloseNT (node:internal/streams/destroy:128:3)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ECONNREFUSED',&#10;  errors: [&#10;    Error: connect ECONNREFUSED ::1:11434&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '::1',&#10;      port: 11434&#10;    },&#10;    Error: connect ECONNREFUSED 127.0.0.1:11434&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '127.0.0.1',&#10;      port: 11434&#10;    }&#10;  ],&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: Hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; Writable {&#10;    _events: {&#10;      close: undefined,&#10;      error: [Function: handleRequestError],&#10;      prefinish: undefined,&#10;      finish: undefined,&#10;      drain: undefined,&#10;      response: [Function: handleResponse],&#10;      socket: [Function: handleRequestSocket]&#10;    },&#10;    _writableState: WritableState {&#10;      highWaterMark: 16384,&#10;      length: 0,&#10;      corked: 0,&#10;      onwrite: [Function: bound onwrite],&#10;      writelen: 0,&#10;      bufferedIndex: 0,&#10;      pendingcb: 0,&#10;      [Symbol(kState)]: 17580812,&#10;      [Symbol(kBufferedValue)]: null&#10;    },&#10;    _maxListeners: undefined,&#10;    _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/api/generate',&#10;      method: 'POST',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '11434',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/api/generate'&#10;    },&#10;    _ended: false,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 999,&#10;    _requestBodyBuffers: [ [Object] ],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: false,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:11434/api/generate',&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: AggregateError [ECONNREFUSED]: &#10;      at internalConnectMultiple (node:net:1118:18)&#10;      at afterConnectMultiple (node:net:1685:7) {&#10;    code: 'ECONNREFUSED',&#10;    [errors]: [ [Error], [Error] ]&#10;  }&#10;}&#10;Retry attempt 2 after error: Could not connect to Ollama. Please ensure Ollama is running.&#10;Ollama chat error: AxiosError [AggregateError]&#10;    at AxiosError.from (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:876:14)&#10;    at RedirectableRequest.handleRequestError (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3156:25)&#10;    at RedirectableRequest.emit (node:events:519:28)&#10;    at eventHandlers.&lt;computed&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/follow-redirects/index.js:49:24)&#10;    at ClientRequest.emit (node:events:519:28)&#10;    at emitErrorEvent (node:_http_client:108:11)&#10;    at Socket.socketErrorListener (node:_http_client:511:5)&#10;    at Socket.emit (node:events:519:28)&#10;    at emitErrorNT (node:internal/streams/destroy:169:8)&#10;    at emitErrorCloseNT (node:internal/streams/destroy:128:3)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:109:30)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26 {&#10;  code: 'ECONNREFUSED',&#10;  errors: [&#10;    Error: connect ECONNREFUSED ::1:11434&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '::1',&#10;      port: 11434&#10;    },&#10;    Error: connect ECONNREFUSED 127.0.0.1:11434&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '127.0.0.1',&#10;      port: 11434&#10;    }&#10;  ],&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '999',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    signal: AbortSignal { aborted: false },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: `{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are NASA's Space Duck, an AI assistant specializing in space science and exploration. \\n        Your core responsibilities:\\n        - Provide accurate, NASA-sourced information about space, astronomy, and space exploration\\n        - Explain complex space concepts in simple, engaging terms\\n        - Share interesting facts about NASA missions, both historical and current\\n        - Encourage scientific thinking and space enthusiasm\\n        \\n        Response guidelines:\\n        - Keep answers concise (2-3 sentences for general responses)\\n        - Use precise scientific terminology with brief explanations\\n        - Include relevant NASA mission references when applicable\\n        - Maintain professional tone while being approachable\\n        - Focus on verified scientific facts\\n        \\n        If unsure about any information, acknowledge uncertainty and suggest referring to NASA's official resources.\\n\\nUser: Hello&quot;,&quot;stream&quot;:false,&quot;context&quot;:[]}`&#10;  },&#10;  request: &lt;ref *1&gt; Writable {&#10;    _events: {&#10;      close: undefined,&#10;      error: [Function: handleRequestError],&#10;      prefinish: undefined,&#10;      finish: undefined,&#10;      drain: undefined,&#10;      response: [Function: handleResponse],&#10;      socket: [Function: handleRequestSocket]&#10;    },&#10;    _writableState: WritableState {&#10;      highWaterMark: 16384,&#10;      length: 0,&#10;      corked: 0,&#10;      onwrite: [Function: bound onwrite],&#10;      writelen: 0,&#10;      bufferedIndex: 0,&#10;      pendingcb: 0,&#10;      [Symbol(kState)]: 17580812,&#10;      [Symbol(kBufferedValue)]: null&#10;    },&#10;    _maxListeners: undefined,&#10;    _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/api/generate',&#10;      method: 'POST',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '11434',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/api/generate'&#10;    },&#10;    _ended: false,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 999,&#10;    _requestBodyBuffers: [ [Object] ],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '999',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: false,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 999\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:11434/api/generate',&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: AggregateError [ECONNREFUSED]: &#10;      at internalConnectMultiple (node:net:1118:18)&#10;      at afterConnectMultiple (node:net:1685:7) {&#10;    code: 'ECONNREFUSED',&#10;    [errors]: [ [Error], [Error] ]&#10;  }&#10;}&#10;Chat error: Error: Could not connect to Ollama. Please ensure Ollama is running.&#10;    at OllamaService._attemptChat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:132:23)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:91:34)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:74:26&#10;&#10;" />
        <option value="no for the last time no adding files" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed: &#10;pulling manifest &#10;pulling 8934d96d3f08... 100% ▕████████████████▏ 3.8 GB                         &#10;pulling 8c17c2ebb0ea... 100% ▕████████████████▏ 7.0 KB                         &#10;pulling 7c23fb36d801... 100% ▕████████████████▏ 4.8 KB                         &#10;pulling 2e0493f67d0c... 100% ▕████████████████▏   59 B                         &#10;pulling fa304d675061... 100% ▕████████████████▏   91 B                         &#10;pulling 42ba7f8a01dd... 100% ▕████████████████▏  557 B                         &#10;verifying sha256 digest &#10;writing manifest &#10;success &#10;&#10;Ollama service initialization complete&#10;Server initialization complete, ready to handle requests&#10;Server is running on http://localhost:3000&#10;Ollama chat error: AxiosError: Request failed with status code 500&#10;    at settle (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:2019:12)&#10;    at IncomingMessage.handleStreamEnd (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3135:11)&#10;    at IncomingMessage.emit (node:events:531:35)&#10;    at endReadableNT (node:internal/streams/readable:1696:12)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:84:30)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:60:26 {&#10;  code: 'ERR_BAD_RESPONSE',&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 60000,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': 'application/json',&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Content-Length': '612',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    method: 'post',&#10;    url: 'http://localhost:11434/api/generate',&#10;    data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;  },&#10;  request: &lt;ref *1&gt; ClientRequest {&#10;    _events: [Object: null prototype] {&#10;      abort: [Function (anonymous)],&#10;      aborted: [Function (anonymous)],&#10;      connect: [Function (anonymous)],&#10;      error: [Function (anonymous)],&#10;      socket: [Function (anonymous)],&#10;      timeout: [Function (anonymous)],&#10;      finish: [Function: requestOnFinish]&#10;    },&#10;    _eventsCount: 7,&#10;    _maxListeners: undefined,&#10;    outputData: [],&#10;    outputSize: 0,&#10;    writable: true,&#10;    destroyed: true,&#10;    _last: false,&#10;    chunkedEncoding: false,&#10;    shouldKeepAlive: true,&#10;    maxRequestsOnConnectionReached: false,&#10;    _defaultKeepAlive: true,&#10;    useChunkedEncodingByDefault: true,&#10;    sendDate: false,&#10;    _removedConnection: false,&#10;    _removedContLen: false,&#10;    _removedTE: false,&#10;    strictContentLength: false,&#10;    _contentLength: '612',&#10;    _hasBody: true,&#10;    _trailer: '',&#10;    finished: true,&#10;    _headerSent: true,&#10;    _closed: true,&#10;    socket: Socket {&#10;      connecting: false,&#10;      _hadError: false,&#10;      _parent: null,&#10;      _host: 'localhost',&#10;      _closeAfterHandlingError: false,&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _writableState: [WritableState],&#10;      allowHalfOpen: false,&#10;      _maxListeners: undefined,&#10;      _eventsCount: 6,&#10;      _sockname: null,&#10;      _pendingData: null,&#10;      _pendingEncoding: '',&#10;      server: null,&#10;      _server: null,&#10;      timeout: 5000,&#10;      parser: null,&#10;      _httpMessage: null,&#10;      autoSelectFamilyAttemptedAddresses: [Array],&#10;      [Symbol(async_id_symbol)]: -1,&#10;      [Symbol(kHandle)]: [TCP],&#10;      [Symbol(lastWriteQueueSize)]: 0,&#10;      [Symbol(timeout)]: Timeout {&#10;        _idleTimeout: 5000,&#10;        _idlePrev: [TimersList],&#10;        _idleNext: [TimersList],&#10;        _idleStart: 19701,&#10;        _onTimeout: [Function: bound ],&#10;        _timerArgs: undefined,&#10;        _repeat: null,&#10;        _destroyed: false,&#10;        [Symbol(refed)]: false,&#10;        [Symbol(kHasPrimitive)]: false,&#10;        [Symbol(asyncId)]: 208,&#10;        [Symbol(triggerId)]: 206&#10;      },&#10;      [Symbol(kBuffer)]: null,&#10;      [Symbol(kBufferCb)]: null,&#10;      [Symbol(kBufferGen)]: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kSetNoDelay)]: true,&#10;      [Symbol(kSetKeepAlive)]: true,&#10;      [Symbol(kSetKeepAliveInitialDelay)]: 1,&#10;      [Symbol(kBytesRead)]: 0,&#10;      [Symbol(kBytesWritten)]: 0&#10;    },&#10;    _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;      'Accept: application/json, text/plain, */*\r\n' +&#10;      'Content-Type: application/json\r\n' +&#10;      'User-Agent: axios/1.7.7\r\n' +&#10;      'Content-Length: 612\r\n' +&#10;      'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;      'Host: localhost:11434\r\n' +&#10;      'Connection: keep-alive\r\n' +&#10;      '\r\n',&#10;    _keepAliveTimeout: 0,&#10;    _onPendingData: [Function: nop],&#10;    agent: Agent {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 2,&#10;      _maxListeners: undefined,&#10;      defaultPort: 80,&#10;      protocol: 'http:',&#10;      options: [Object: null prototype],&#10;      requests: [Object: null prototype] {},&#10;      sockets: [Object: null prototype] {},&#10;      freeSockets: [Object: null prototype],&#10;      keepAliveMsecs: 1000,&#10;      keepAlive: true,&#10;      maxSockets: Infinity,&#10;      maxFreeSockets: 256,&#10;      scheduling: 'lifo',&#10;      maxTotalSockets: Infinity,&#10;      totalSocketCount: 1,&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    socketPath: undefined,&#10;    method: 'POST',&#10;    maxHeaderSize: undefined,&#10;    insecureHTTPParser: undefined,&#10;    joinDuplicateHeaders: undefined,&#10;    path: '/api/generate',&#10;    _ended: true,&#10;    res: IncomingMessage {&#10;      _events: [Object],&#10;      _readableState: [ReadableState],&#10;      _maxListeners: undefined,&#10;      socket: null,&#10;      httpVersionMajor: 1,&#10;      httpVersionMinor: 1,&#10;      httpVersion: '1.1',&#10;      complete: true,&#10;      rawHeaders: [Array],&#10;      rawTrailers: [],&#10;      joinDuplicateHeaders: undefined,&#10;      aborted: false,&#10;      upgrade: false,&#10;      url: '',&#10;      method: null,&#10;      statusCode: 500,&#10;      statusMessage: 'Internal Server Error',&#10;      client: [Socket],&#10;      _consuming: false,&#10;      _dumped: false,&#10;      req: [Circular *1],&#10;      _eventsCount: 4,&#10;      responseUrl: 'http://localhost:11434/api/generate',&#10;      redirects: [],&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kHeaders)]: [Object],&#10;      [Symbol(kHeadersCount)]: 6,&#10;      [Symbol(kTrailers)]: null,&#10;      [Symbol(kTrailersCount)]: 0&#10;    },&#10;    aborted: false,&#10;    timeoutCb: null,&#10;    upgradeOrConnect: false,&#10;    parser: null,&#10;    maxHeadersCount: null,&#10;    reusedSocket: false,&#10;    host: 'localhost',&#10;    protocol: 'http:',&#10;    _redirectable: Writable {&#10;      _events: [Object],&#10;      _writableState: [WritableState],&#10;      _maxListeners: undefined,&#10;      _options: [Object],&#10;      _ended: true,&#10;      _ending: true,&#10;      _redirectCount: 0,&#10;      _redirects: [],&#10;      _requestBodyLength: 612,&#10;      _requestBodyBuffers: [],&#10;      _eventsCount: 3,&#10;      _onNativeResponse: [Function (anonymous)],&#10;      _currentRequest: [Circular *1],&#10;      _currentUrl: 'http://localhost:11434/api/generate',&#10;      _timeout: null,&#10;      [Symbol(shapeMode)]: true,&#10;      [Symbol(kCapture)]: false&#10;    },&#10;    [Symbol(shapeMode)]: false,&#10;    [Symbol(kCapture)]: false,&#10;    [Symbol(kBytesWritten)]: 0,&#10;    [Symbol(kNeedDrain)]: false,&#10;    [Symbol(corked)]: 0,&#10;    [Symbol(kOutHeaders)]: [Object: null prototype] {&#10;      accept: [Array],&#10;      'content-type': [Array],&#10;      'user-agent': [Array],&#10;      'content-length': [Array],&#10;      'accept-encoding': [Array],&#10;      host: [Array]&#10;    },&#10;    [Symbol(errored)]: null,&#10;    [Symbol(kHighWaterMark)]: 16384,&#10;    [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;    [Symbol(kUniqueHeaders)]: null&#10;  },&#10;  response: {&#10;    status: 500,&#10;    statusText: 'Internal Server Error',&#10;    headers: Object [AxiosHeaders] {&#10;      'content-type': 'application/json; charset=utf-8',&#10;      date: 'Sat, 02 Nov 2024 21:29:29 GMT',&#10;      'content-length': '63'&#10;    },&#10;    config: {&#10;      transitional: [Object],&#10;      adapter: [Array],&#10;      transformRequest: [Array],&#10;      transformResponse: [Array],&#10;      timeout: 60000,&#10;      xsrfCookieName: 'XSRF-TOKEN',&#10;      xsrfHeaderName: 'X-XSRF-TOKEN',&#10;      maxContentLength: -1,&#10;      maxBodyLength: -1,&#10;      env: [Object],&#10;      validateStatus: [Function: validateStatus],&#10;      headers: [Object [AxiosHeaders]],&#10;      method: 'post',&#10;      url: 'http://localhost:11434/api/generate',&#10;      data: '{&quot;model&quot;:&quot;llama2&quot;,&quot;prompt&quot;:&quot;You are a NASA instructor providing clear, concise information about space exploration and science. \\n        Your responses should be:\\n        - Brief and to the point (2-3 sentences for general responses)\\n        - Professional and factual\\n        - Free of roleplay elements or emotive actions\\n        - Focused on accurate scientific information\\n        - Written in a clear, straightforward style\\n        \\n        If the user asks a specific technical question, you may provide more detailed information, but keep general responses concise.\\n\\nUser: hello&quot;,&quot;stream&quot;:false}'&#10;    },&#10;    request: &lt;ref *1&gt; ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: true,&#10;      _last: false,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: true,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: '612',&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: true,&#10;      socket: [Socket],&#10;      _header: 'POST /api/generate HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'Content-Type: application/json\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Content-Length: 612\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:11434\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'POST',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/api/generate',&#10;      _ended: true,&#10;      res: [IncomingMessage],&#10;      aborted: false,&#10;      timeoutCb: null,&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Writable],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    data: { error: 'llama runner process has terminated: signal: killed' }&#10;  },&#10;  status: 500&#10;}&#10;Chat error: Error: Failed to get response from Ollama: Request failed with status code 500&#10;    at OllamaService.chat (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:104:19)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:60:26&#10;&#10;fix this without adding any files" />
        <option value="is it better to have the server.js launch llama2 or have it be a sepearte starting service?" />
        <option value="ok i now want to use the data in the lancedb files to to ask the user questions using the LLM" />
        <option value="/usr/bin/npm install&#10;npm error code ETARGET&#10;npm error notarget No matching version found for lancedb@^0.5.3.&#10;npm error notarget In most cases you or one of your dependencies are requesting&#10;npm error notarget a package version that doesn't exist.&#10;npm error A complete log of this run can be found in: /home/barthmalemew/.npm/_logs/2024-11-02T22_09_05_917Z-debug-0.log&#10;&#10;Process finished with exit code 1&#10;" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ npm install landcedb&#10;npm error code ETARGET&#10;npm error notarget No matching version found for lancedb@^0.3.3.&#10;npm error notarget In most cases you or one of your dependencies are requesting&#10;npm error notarget a package version that doesn't exist.&#10;npm error A complete log of this run can be found in: /home/barthmalemew/.npm/_logs/2024-11-02T22_12_42_681Z-debug-0.log&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ npm install landcedb&#10;" />
        <option value="how does it work now" />
        <option value="it should prompt the user with a quesition from the database on its sown" />
        <option value="pip install lancedb==0.3.3" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;node:internal/modules/cjs/loader:495&#10;      throw err;&#10;      ^&#10;&#10;Error: Cannot find module '/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/lancedb/index.js'. Please verify that the package.json has a valid &quot;main&quot; entry&#10;    at tryPackage (node:internal/modules/cjs/loader:487:19)&#10;    at Module._findPath (node:internal/modules/cjs/loader:771:18)&#10;    at Module._resolveFilename (node:internal/modules/cjs/loader:1211:27)&#10;    at Module._load (node:internal/modules/cjs/loader:1051:27)&#10;    at Module.require (node:internal/modules/cjs/loader:1311:19)&#10;    at require (node:internal/modules/helpers:179:18)&#10;    at Object.&lt;anonymous&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:12:17)&#10;    at Module._compile (node:internal/modules/cjs/loader:1469:14)&#10;    at Module._extensions..js (node:internal/modules/cjs/loader:1548:10)&#10;    at Module.load (node:internal/modules/cjs/loader:1288:32) {&#10;  code: 'MODULE_NOT_FOUND',&#10;  path: '/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/lancedb/package.json',&#10;  requestPath: 'lancedb'&#10;}&#10;&#10;Node.js v20.17.0&#10;&#10;Process finished with exit code 1&#10;&#10;" />
        <option value="node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;node:internal/modules/cjs/loader:495&#10;      throw err;&#10;      ^&#10;&#10;Error: Cannot find module '/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/lancedb/index.js'. Please verify that the package.json has a valid &quot;main&quot; entry&#10;    at tryPackage (node:internal/modules/cjs/loader:487:19)&#10;    at Module._findPath (node:internal/modules/cjs/loader:771:18)&#10;    at Module._resolveFilename (node:internal/modules/cjs/loader:1211:27)&#10;    at Module._load (node:internal/modules/cjs/loader:1051:27)&#10;    at Module.require (node:internal/modules/cjs/loader:1311:19)&#10;    at require (node:internal/modules/helpers:179:18)&#10;    at Object.&lt;anonymous&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.js:12:17)&#10;    at Module._compile (node:internal/modules/cjs/loader:1469:14)&#10;    at Module._extensions..js (node:internal/modules/cjs/loader:1548:10)&#10;    at Module.load (node:internal/modules/cjs/loader:1288:32) {&#10;  code: 'MODULE_NOT_FOUND',&#10;  path: '/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/lancedb/package.json',&#10;  requestPath: 'lancedb'&#10;}&#10;&#10;Node.js v20.17.0&#10;&#10;Process finished with exit code 1&#10;&#10;" />
        <option value="have a look over the code and suggest changes" />
        <option value="no please dont add files" />
        <option value="ok ignoreing style sheets look over my code. i want the server.js to start the llama2 model automatically by running he ollama.py" />
        <option value="rthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:76&#10;app.get('/api/random-question', checkServerReady, async (req, res) =&gt; {&#10;^&#10;&#10;ReferenceError: app is not defined&#10;    at Object.&lt;anonymous&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:76:1)&#10;    at Module._compile (node:internal/modules/cjs/loader:1469:14)&#10;    at Module._extensions..js (node:internal/modules/cjs/loader:1548:10)&#10;    at Module.load (node:internal/modules/cjs/loader:1288:32)&#10;    at Module._load (node:internal/modules/cjs/loader:1104:12)&#10;    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:174:12)&#10;    at node:internal/main/run_main_module:28:49&#10;&#10;Node.js v20.17.0&#10;&#10;Process finished with exit code 1&#10;&#10;" />
        <option value="please look at thep python script to make sure its ocrrect" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ pip install fastapu&#10;ERROR: Could not find a version that satisfies the requirement fastapu (from versions: none)&#10;ERROR: No matching distribution found for fastapu&#10;^CTraceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/bin/pip&quot;, line 8, in &lt;module&gt;&#10;    sys.exit(main())&#10;             ^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/cli/main.py&quot;, line 79, in main&#10;    return command.main(cmd_args)&#10;           ^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/cli/base_command.py&quot;, line 101, in main&#10;    return self._main(args)&#10;           ^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/cli/base_command.py&quot;, line 236, in _main&#10;    self.handle_pip_version_check(options)&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/cli/req_command.py&quot;, line 191, in handle_pip_version_check&#10;    pip_self_version_check(session, options)&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/self_outdated_check.py&quot;, line 230, in pip_self_version_check&#10;    upgrade_prompt = _self_version_check_logic(&#10;                     ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/self_outdated_check.py&quot;, line 193, in _self_version_check_logic&#10;    remote_version_str = get_remote_version()&#10;                         ^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/self_outdated_check.py&quot;, line 177, in _get_current_remote_pip_version&#10;    best_candidate = finder.find_best_candidate(&quot;pip&quot;).best_candidate&#10;                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/index/package_finder.py&quot;, line 890, in find_best_candidate&#10;    candidates = self.find_all_candidates(project_name)&#10;                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/index/package_finder.py&quot;, line 831, in find_all_candidates&#10;    page_candidates = list(page_candidates_it)&#10;                      ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/index/sources.py&quot;, line 134, in page_candidates&#10;    yield from self._candidates_from_page(self._link)&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/index/package_finder.py&quot;, line 798, in process_project_url&#10;    package_links = self.evaluate_links(&#10;                    ^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/index/package_finder.py&quot;, line 778, in evaluate_links&#10;    candidate = self.get_install_candidate(link_evaluator, link)&#10;                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/index/package_finder.py&quot;, line 759, in get_install_candidate&#10;    result, detail = link_evaluator.evaluate_link(link)&#10;                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/index/package_finder.py&quot;, line 238, in evaluate_link&#10;    supports_python = _check_link_requires_python(&#10;                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/index/package_finder.py&quot;, line 66, in _check_link_requires_python&#10;    is_compatible = check_requires_python(&#10;                    ^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_internal/utils/packaging.py&quot;, line 31, in check_requires_python&#10;    requires_python_specifier = specifiers.SpecifierSet(requires_python)&#10;                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_vendor/packaging/specifiers.py&quot;, line 634, in __init__&#10;    parsed.add(Specifier(specifier))&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_vendor/packaging/specifiers.py&quot;, line 125, in __hash__&#10;    return hash(self._canonical_spec)&#10;                ^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_vendor/packaging/specifiers.py&quot;, line 122, in _canonical_spec&#10;    return self._spec[0], canonicalize_version(self._spec[1])&#10;                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/pip/_vendor/packaging/utils.py&quot;, line 60, in canonicalize_version&#10;    parts.append(re.sub(r&quot;(\.0)+$&quot;, &quot;&quot;, &quot;.&quot;.join(str(x) for x in parsed.release)))&#10;                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;KeyboardInterrupt&#10;&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ pip install fastapi&#10;Collecting fastapi&#10;  Obtaining dependency information for fastapi from https://files.pythonhosted.org/packages/99/f6/af0d1f58f86002be0cf1e2665cdd6f7a4a71cdc8a7a9438cdc9e3b5375fe/fastapi-0.115.4-py3-none-any.whl.metadata&#10;  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)&#10;Collecting starlette&lt;0.42.0,&gt;=0.40.0 (from fastapi)&#10;  Obtaining dependency information for starlette&lt;0.42.0,&gt;=0.40.0 from https://files.pythonhosted.org/packages/54/43/f185bfd0ca1d213beb4293bed51d92254df23d8ceaf6c0e17146d508a776/starlette-0.41.2-py3-none-any.whl.metadata&#10;  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)&#10;Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,&lt;3.0.0,&gt;=1.7.4 (from fastapi)&#10;  Obtaining dependency information for pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,&lt;3.0.0,&gt;=1.7.4 from https://files.pythonhosted.org/packages/df/e4/ba44652d562cbf0bf320e0f3810206149c8a4e99cdbf66da82e97ab53a15/pydantic-2.9.2-py3-none-any.whl.metadata&#10;  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)&#10;Collecting typing-extensions&gt;=4.8.0 (from fastapi)&#10;  Obtaining dependency information for typing-extensions&gt;=4.8.0 from https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl.metadata&#10;  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)&#10;Collecting annotated-types&gt;=0.6.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,&lt;3.0.0,&gt;=1.7.4-&gt;fastapi)&#10;  Obtaining dependency information for annotated-types&gt;=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata&#10;  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)&#10;Collecting pydantic-core==2.23.4 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,&lt;3.0.0,&gt;=1.7.4-&gt;fastapi)&#10;  Obtaining dependency information for pydantic-core==2.23.4 from https://files.pythonhosted.org/packages/06/c8/7d4b708f8d05a5cbfda3243aad468052c6e99de7d0937c9146c24d9f12e9/pydantic_core-2.23.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata&#10;  Using cached pydantic_core-2.23.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)&#10;Collecting anyio&lt;5,&gt;=3.4.0 (from starlette&lt;0.42.0,&gt;=0.40.0-&gt;fastapi)&#10;  Obtaining dependency information for anyio&lt;5,&gt;=3.4.0 from https://files.pythonhosted.org/packages/e4/f5/f2b75d2fc6f1a260f340f0e7c6a060f4dd2961cc16884ed851b0d18da06a/anyio-4.6.2.post1-py3-none-any.whl.metadata&#10;  Downloading anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)&#10;Collecting idna&gt;=2.8 (from anyio&lt;5,&gt;=3.4.0-&gt;starlette&lt;0.42.0,&gt;=0.40.0-&gt;fastapi)&#10;  Obtaining dependency information for idna&gt;=2.8 from https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl.metadata&#10;  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)&#10;Collecting sniffio&gt;=1.1 (from anyio&lt;5,&gt;=3.4.0-&gt;starlette&lt;0.42.0,&gt;=0.40.0-&gt;fastapi)&#10;  Obtaining dependency information for sniffio&gt;=1.1 from https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl.metadata&#10;  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)&#10;Downloading fastapi-0.115.4-py3-none-any.whl (94 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.7/94.7 kB 2.8 MB/s eta 0:00:00&#10;Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)&#10;Using cached pydantic_core-2.23.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)&#10;Downloading starlette-0.41.2-py3-none-any.whl (73 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.3/73.3 kB 3.7 MB/s eta 0:00:00&#10;Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)&#10;Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)&#10;Downloading anyio-4.6.2.post1-py3-none-any.whl (90 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.4/90.4 kB 3.2 MB/s eta 0:00:00&#10;Downloading idna-3.10-py3-none-any.whl (70 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.4/70.4 kB 5.7 MB/s eta 0:00:00&#10;Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)&#10;Installing collected packages: typing-extensions, sniffio, idna, annotated-types, pydantic-core, anyio, starlette, pydantic, fastapi&#10;Successfully installed annotated-types-0.7.0 anyio-4.6.2.post1 fastapi-0.115.4 idna-3.10 pydantic-2.9.2 pydantic-core-2.23.4 sniffio-1.3.1 starlette-0.41.2 typing-extensions-4.12.2&#10;&#10;[notice] A new release of pip is available: 23.2.1 -&gt; 24.3.1&#10;[notice] To update, run: pip install --upgrade pip&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ pip install lancedb&#10;Collecting lancedb&#10;  Obtaining dependency information for lancedb from https://files.pythonhosted.org/packages/e3/a8/67a8dc6fd7a57bde6b9f75b10368805db46b109adc7d634d7ce45bc655f3/lancedb-0.15.0-cp38-abi3-manylinux_2_28_x86_64.whl.metadata&#10;  Using cached lancedb-0.15.0-cp38-abi3-manylinux_2_28_x86_64.whl.metadata (4.8 kB)&#10;Collecting deprecation (from lancedb)&#10;  Obtaining dependency information for deprecation from https://files.pythonhosted.org/packages/02/c3/253a89ee03fc9b9682f1541728eb66db7db22148cd94f89ab22528cd1e1b/deprecation-2.1.0-py2.py3-none-any.whl.metadata&#10;  Using cached deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)&#10;Collecting pylance==0.19.1 (from lancedb)&#10;  Obtaining dependency information for pylance==0.19.1 from https://files.pythonhosted.org/packages/3b/af/3bf6d0c9dc52e2ae048c575249527f3c2cc8a4df85c94905900c719b42e0/pylance-0.19.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata&#10;  Using cached pylance-0.19.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (7.4 kB)&#10;Collecting requests&gt;=2.31.0 (from lancedb)&#10;  Obtaining dependency information for requests&gt;=2.31.0 from https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl.metadata&#10;  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)&#10;Collecting tqdm&gt;=4.27.0 (from lancedb)&#10;  Obtaining dependency information for tqdm&gt;=4.27.0 from https://files.pythonhosted.org/packages/41/73/02342de9c2d20922115f787e101527b831c0cffd2105c946c4a4826bcfd4/tqdm-4.66.6-py3-none-any.whl.metadata&#10;  Downloading tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)&#10;     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 1.2 MB/s eta 0:00:00&#10;Requirement already satisfied: pydantic&gt;=1.10 in ./venv/lib/python3.12/site-packages (from lancedb) (2.9.2)&#10;Collecting attrs&gt;=21.3.0 (from lancedb)&#10;  Obtaining dependency information for attrs&gt;=21.3.0 from https://files.pythonhosted.org/packages/6a/21/5b6702a7f963e95456c0de2d495f67bf5fd62840ac655dc451586d23d39a/attrs-24.2.0-py3-none-any.whl.metadata&#10;  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)&#10;Collecting packaging (from lancedb)&#10;  Obtaining dependency information for packaging from https://files.pythonhosted.org/packages/08/aa/cc0199a5f0ad350994d660967a8efb233fe0416e4639146c089643407ce6/packaging-24.1-py3-none-any.whl.metadata&#10;  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)&#10;Collecting cachetools (from lancedb)&#10;  Obtaining dependency information for cachetools from https://files.pythonhosted.org/packages/a4/07/14f8ad37f2d12a5ce41206c21820d8cb6561b728e51fad4530dff0552a67/cachetools-5.5.0-py3-none-any.whl.metadata&#10;  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)&#10;Collecting overrides&gt;=0.7 (from lancedb)&#10;  Obtaining dependency information for overrides&gt;=0.7 from https://files.pythonhosted.org/packages/2c/ab/fc8290c6a4c722e5514d80f62b2dc4c4df1a68a41d1364e625c35990fcf3/overrides-7.7.0-py3-none-any.whl.metadata&#10;  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)&#10;Collecting pyarrow&gt;=12 (from pylance==0.19.1-&gt;lancedb)&#10;  Obtaining dependency information for pyarrow&gt;=12 from https://files.pythonhosted.org/packages/8d/1f/9bb3b3a644892d631dbbe99053cdb5295092d2696b4bcd3d21f29624c689/pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata&#10;  Using cached pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)&#10;Collecting numpy&lt;2,&gt;=1.22 (from pylance==0.19.1-&gt;lancedb)&#10;  Obtaining dependency information for numpy&lt;2,&gt;=1.22 from https://files.pythonhosted.org/packages/0f/50/de23fde84e45f5c4fda2488c759b69990fd4512387a8632860f3ac9cd225/numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata&#10;  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)&#10;Requirement already satisfied: annotated-types&gt;=0.6.0 in ./venv/lib/python3.12/site-packages (from pydantic&gt;=1.10-&gt;lancedb) (0.7.0)&#10;Requirement already satisfied: pydantic-core==2.23.4 in ./venv/lib64/python3.12/site-packages (from pydantic&gt;=1.10-&gt;lancedb) (2.23.4)&#10;Requirement already satisfied: typing-extensions&gt;=4.6.1 in ./venv/lib/python3.12/site-packages (from pydantic&gt;=1.10-&gt;lancedb) (4.12.2)&#10;Collecting charset-normalizer&lt;4,&gt;=2 (from requests&gt;=2.31.0-&gt;lancedb)&#10;  Obtaining dependency information for charset-normalizer&lt;4,&gt;=2 from https://files.pythonhosted.org/packages/16/92/92a76dc2ff3a12e69ba94e7e05168d37d0345fa08c87e1fe24d0c2a42223/charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata&#10;  Downloading charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)&#10;Requirement already satisfied: idna&lt;4,&gt;=2.5 in ./venv/lib/python3.12/site-packages (from requests&gt;=2.31.0-&gt;lancedb) (3.10)&#10;Collecting urllib3&lt;3,&gt;=1.21.1 (from requests&gt;=2.31.0-&gt;lancedb)&#10;  Obtaining dependency information for urllib3&lt;3,&gt;=1.21.1 from https://files.pythonhosted.org/packages/ce/d9/5f4c13cecde62396b0d3fe530a50ccea91e7dfc1ccf0e09c228841bb5ba8/urllib3-2.2.3-py3-none-any.whl.metadata&#10;  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)&#10;Collecting certifi&gt;=2017.4.17 (from requests&gt;=2.31.0-&gt;lancedb)&#10;  Obtaining dependency information for certifi&gt;=2017.4.17 from https://files.pythonhosted.org/packages/12/90/3c9ff0512038035f59d279fddeb79f5f1eccd8859f06d6163c58798b9487/certifi-2024.8.30-py3-none-any.whl.metadata&#10;  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)&#10;Using cached lancedb-0.15.0-cp38-abi3-manylinux_2_28_x86_64.whl (27.1 MB)&#10;Using cached pylance-0.19.1-cp39-abi3-manylinux_2_28_x86_64.whl (30.4 MB)&#10;Using cached attrs-24.2.0-py3-none-any.whl (63 kB)&#10;Using cached overrides-7.7.0-py3-none-any.whl (17 kB)&#10;Using cached requests-2.32.3-py3-none-any.whl (64 kB)&#10;Downloading tqdm-4.66.6-py3-none-any.whl (78 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 10.7 MB/s eta 0:00:00&#10;Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)&#10;Using cached deprecation-2.1.0-py2.py3-none-any.whl (11 kB)&#10;Using cached packaging-24.1-py3-none-any.whl (53 kB)&#10;Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.3/167.3 kB 6.2 MB/s eta 0:00:00&#10;Downloading charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.8/143.8 kB 2.2 MB/s eta 0:00:00&#10;Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)&#10;Using cached pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.0 MB)&#10;Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.3/126.3 kB 2.8 MB/s eta 0:00:00&#10;Installing collected packages: urllib3, tqdm, pyarrow, packaging, overrides, numpy, charset-normalizer, certifi, cachetools, attrs, requests, pylance, deprecation, lancedb&#10;Successfully installed attrs-24.2.0 cachetools-5.5.0 certifi-2024.8.30 charset-normalizer-3.4.0 deprecation-2.1.0 lancedb-0.15.0 numpy-1.26.4 overrides-7.7.0 packaging-24.1 pyarrow-18.0.0 pylance-0.19.1 requests-2.32.3 tqdm-4.66.6 urllib3-2.2.3&#10;&#10;[notice] A new release of pip is available: 23.2.1 -&gt; 24.3.1&#10;[notice] To update, run: pip install --upgrade pip&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ pip install uvicorn&#10;Collecting uvicorn&#10;  Obtaining dependency information for uvicorn from https://files.pythonhosted.org/packages/eb/14/78bd0e95dd2444b6caacbca2b730671d4295ccb628ef58b81bee903629df/uvicorn-0.32.0-py3-none-any.whl.metadata&#10;  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)&#10;Collecting click&gt;=7.0 (from uvicorn)&#10;  Obtaining dependency information for click&gt;=7.0 from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata&#10;  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)&#10;Collecting h11&gt;=0.8 (from uvicorn)&#10;  Obtaining dependency information for h11&gt;=0.8 from https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata&#10;  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)&#10;Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)&#10;   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.7/63.7 kB 2.6 MB/s eta 0:00:00&#10;Using cached click-8.1.7-py3-none-any.whl (97 kB)&#10;Using cached h11-0.14.0-py3-none-any.whl (58 kB)&#10;Installing collected packages: h11, click, uvicorn&#10;Successfully installed click-8.1.7 h11-0.14.0 uvicorn-0.32.0&#10;&#10;[notice] A new release of pip is available: 23.2.1 -&gt; 24.3.1&#10;[notice] To update, run: pip install --upgrade pip&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ ls&#10;node_modules  package.json  package-lock.json  venv  webapp&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ ls&#10;node_modules  package.json  package-lock.json  venv  webapp&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ cd webapp&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp$ ls&#10;data  public  server.js  services&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp$ cd services&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ ls&#10;ollama.py&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:148: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [90534]&#10;INFO:     Waiting for application startup.&#10;Initializing Ollama service...&#10;Failed to initialize Ollama service: Table space_training does not exist.Please first call db.create_table(space_training, data)&#10;ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 693, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 569, in __aenter__&#10;    await self._router.startup()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 670, in startup&#10;    await handler()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 150, in startup_event&#10;    await service.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 51, in initialize&#10;    self.table = await self.db.open_table('space_training')&#10;                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/db.py&quot;, line 450, in open_table&#10;    return LanceTable.open(self, name, index_cache_size=index_cache_size)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/table.py&quot;, line 1085, in open&#10;    raise FileNotFoundError(&#10;FileNotFoundError: Table space_training does not exist.Please first call db.create_table(space_training, data)&#10;&#10;ERROR:    Application startup failed. Exiting.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 16, in &lt;module&gt;&#10;    import aiohttp&#10;ModuleNotFoundError: No module named 'aiohttp'&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="make sure the file paths are correct accross the project" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:168: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:172: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;shutdown&quot;)&#10;INFO:     Started server process [92125]&#10;INFO:     Waiting for application startup.&#10;Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Failed to initialize Ollama service: object LanceTable can't be used in 'await' expression&#10;ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 693, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 569, in __aenter__&#10;    await self._router.startup()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 670, in startup&#10;    await handler()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 170, in startup_event&#10;    await service.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 59, in initialize&#10;    self.table = await self.db.open_table('space_training')&#10;                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;TypeError: object LanceTable can't be used in 'await' expression&#10;&#10;ERROR:    Application startup failed. Exiting.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:180: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:184: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;shutdown&quot;)&#10;INFO:     Started server process [92200]&#10;INFO:     Waiting for application startup.&#10;Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Successfully connected to Ollama service&#10;Failed to initialize Ollama service: object LanceTable can't be used in 'await' expression&#10;ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 693, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 569, in __aenter__&#10;    await self._router.startup()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/starlette/routing.py&quot;, line 670, in startup&#10;    await handler()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 182, in startup_event&#10;    await service.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 60, in initialize&#10;    self.table = await self.db.open_table('space_training')&#10;                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;TypeError: object LanceTable can't be used in 'await' expression&#10;&#10;ERROR:    Application startup failed. Exiting.&#10;" />
        <option value="e /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;node:internal/modules/cjs/loader:1228&#10;  throw err;&#10;  ^&#10;&#10;Error: Cannot find module 'express-rate-limit'&#10;Require stack:&#10;- /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;    at Module._resolveFilename (node:internal/modules/cjs/loader:1225:15)&#10;    at Module._load (node:internal/modules/cjs/loader:1051:27)&#10;    at Module.require (node:internal/modules/cjs/loader:1311:19)&#10;    at require (node:internal/modules/helpers:179:18)&#10;    at Object.&lt;anonymous&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:22:19)&#10;    at Module._compile (node:internal/modules/cjs/loader:1469:14)&#10;    at Module._extensions..js (node:internal/modules/cjs/loader:1548:10)&#10;    at Module.load (node:internal/modules/cjs/loader:1288:32)&#10;    at Module._load (node:internal/modules/cjs/loader:1104:12)&#10;    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:174:12) {&#10;  code: 'MODULE_NOT_FOUND',&#10;  requireStack: [ '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js' ]&#10;}&#10;&#10;Node.js v20.17.0&#10;&#10;Process finished with exit code 1&#10;&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Starting Ollama service...&#10;Ollama service error: Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 6, in &lt;module&gt;&#10;&#10;Ollama service error:     from fastapi import FastAPI, HTTPException&#10;ModuleNotFoundError: No module named 'fastapi'&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;Server initialization complete, ready to handle requests&#10;Server is running on http://localhost:3000&#10;Starting Ollama service...&#10;Ollama service error: Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 6, in &lt;module&gt;&#10;&#10;Ollama service error:     from fastapi import FastAPI, HTTPException&#10;ModuleNotFoundError: No module named 'fastapi'&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;Starting Ollama service...&#10;Ollama service error: Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 6, in &lt;module&gt;&#10;&#10;Ollama service error:     from fastapi import FastAPI, HTTPException&#10;ModuleNotFoundError: No module named 'fastapi'&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;&#10;" />
        <option value="look at the html and css, make very few changes i just want to know whats wrong with it" />
        <option value="can you look at my index and style sheet to figure out wats wrong" />
        <option value="ok can you look at my style sheet and html please?" />
        <option value="  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/starlette/routing.py&quot;, line 734, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/usr/lib64/python3.12/contextlib.py&quot;, line 217, in __aexit__&#10;    await anext(self.gen)&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 184, in lifespan&#10;    service.db.close()&#10;    ^^^^^^^^^^^^^^^^&#10;AttributeError: 'LanceDBConnection' object has no attribute 'close'&#10;&#10;&#10;Ollama service error: ERROR:    Application shutdown failed. Exiting.&#10;&#10;Ollama service: Ollama service initialization complete&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;Server initialization complete, ready to handle requests&#10;Server is running on http://localhost:3000&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [99480]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Successfully connected to Ollama service&#10;&#10;Ollama service: Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;&#10;Ollama service error: ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/starlette/routing.py&quot;, line 734, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/usr/lib64/python3.12/contextlib.py&quot;, line 217, in __aexit__&#10;    await anext(self.gen)&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 184, in lifespan&#10;    service.db.close()&#10;    ^^^^^^^^^^^^^^^^&#10;AttributeError: 'LanceDBConnection' object has no attribute 'close'&#10;&#10;&#10;Ollama service error: ERROR:    Application shutdown failed. Exiting.&#10;&#10;Ollama service: Ollama service initialization complete&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [99507]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Successfully connected to Ollama service&#10;&#10;Ollama service: Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;&#10;Ollama service error: INFO:     Waiting for application shutdown.&#10;&#10;Ollama service error: ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/starlette/routing.py&quot;, line 734, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/usr/lib64/python3.12/contextlib.py&quot;, line 217, in __aexit__&#10;    await anext(self.gen)&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 184, in lifespan&#10;    service.db.close()&#10;    ^^^^^^^^^^^^^^^^&#10;AttributeError: 'LanceDBConnection' object has no attribute 'close'&#10;&#10;&#10;Ollama service error: ERROR:    Application shutdown failed. Exiting.&#10;&#10;Ollama service: Ollama service initialization complete&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [99537]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Successfully connected to Ollama service&#10;&#10;Ollama service: Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;&#10;Ollama service error: ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/starlette/routing.py&quot;, line 734, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/usr/lib64/python3.12/contextlib.py&quot;, line 217, in __aexit__&#10;    await anext(self.gen)&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 184, in lifespan&#10;    service.db.close()&#10;    ^^^^^^^^^^^^^^^^&#10;AttributeError: 'LanceDBConnection' object has no attribute 'close'&#10;&#10;&#10;Ollama service error: ERROR:    Application shutdown failed. Exiting.&#10;&#10;Ollama service: Ollama service initialization complete&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [99568]&#10;&#10;Ollama service error: INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Successfully connected to Ollama service&#10;&#10;Ollama service: Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;&#10;Ollama service error: ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/starlette/routing.py&quot;, line 734, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/usr/lib64/python3.12/contextlib.py&quot;, line 217, in __aexit__&#10;    await anext(self.gen)&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 184, in lifespan&#10;    service.db.close()&#10;    ^^^^^^^^^^^^^^^^&#10;AttributeError: 'LanceDBConnection' object has no attribute 'close'&#10;&#10;&#10;Ollama service error: ERROR:    Application shutdown failed. Exiting.&#10;&#10;Ollama service: Ollama service initialization complete&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [102035]&#10;&#10;Ollama service error: INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Successfully connected to Ollama service&#10;&#10;Ollama service: Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;Ollama service: Ollama service initialization complete&#10;Successfully connected to Ollama service&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255&#10;Ollama service error: :9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace&#10;&#10;Ollama service: INFO:     127.0.0.1:34560 - &quot;GET /health HTTP/1.1&quot; 200 OK&#10;&#10;Server is running on http://localhost:3000&#10;" />
        <option value="id prefer if you fix the error without mkaing mroe files" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [102362]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Successfully connected to Ollama service&#10;&#10;Ollama service: Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;Ollama service: Ollama service initialization complete&#10;Successfully connected to Ollama service&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace&#10;&#10;Ollama service: INFO:     127.0.0.1:57602 - &quot;GET /health HTTP/1.1&quot; 200 OK&#10;&#10;Server is running on http://localhost:3000&#10;" />
        <option value="no new files please" />
        <option value="no.. new files" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [102362]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Successfully connected to Ollama service&#10;&#10;Ollama service: Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;Ollama service: Ollama service initialization complete&#10;Successfully connected to Ollama service&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace&#10;&#10;Ollama service: INFO:     127.0.0.1:57602 - &quot;GET /health HTTP/1.1&quot; 200 OK&#10;&#10;Server is running on http://localhost:3000&#10;" />
        <option value="please stop trying to make mroe files youre waisting my credits" />
        <option value=" _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/health',&#10;      method: 'GET',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '5000',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/health'&#10;    },&#10;    _ended: true,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 0,&#10;    _requestBodyBuffers: [],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: true,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: false,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: 0,&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'GET /health HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:5000\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'GET',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/health',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:5000/health',&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: AggregateError [ECONNREFUSED]: &#10;      at internalConnectMultiple (node:net:1118:18)&#10;      at afterConnectMultiple (node:net:1685:7) {&#10;    code: 'ECONNREFUSED',&#10;    [errors]: [ [Error], [Error] ]&#10;  }&#10;}&#10;^CShutting down server...&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Database validation failed: External error: RuntimeError: Task was aborted&#10;Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;Ollama service error: ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/starlette/routing.py&quot;, line 743, in lifespan&#10;    await receive()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/uvicorn/lifespan/on.py&quot;, line 137, in receive&#10;    return await self.receive_queue.get()&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/usr/lib64/python3.12/asyncio/queues.py&quot;, line 158, in get&#10;    await getter&#10;asyncio.exceptions.CancelledError&#10;&#10;&#10;Ollama service: Startup error: Failed to initialize after 3 attempts: External error: RuntimeError: Task was aborted&#10;&#10;Ollama service exited with code 0&#10;&#10;Process finished with exit code 0&#10;&#10;" />
        <option value="iled to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Ollama service error: INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Database validation failed: External error: RuntimeError: Task was aborted&#10;Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;&#10;Ollama service error: INFO:     Waiting for application shutdown.&#10;&#10;Ollama service: Startup error: Failed to initialize after 3 attempts: External error: RuntimeError: Task was aborted&#10;Shutting down Ollama service...&#10;&#10;Ollama service error: INFO:     Application shutdown complete.&#10;&#10;Ollama service exited with code 1&#10;Ollama service crashed, attempting restart...&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Database validation failed: External error: RuntimeError: Task was aborted&#10;Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Database validation failed: External error: RuntimeError: Task was aborted&#10;Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Starting Ollama service...&#10;Port 5000 is in use, killing existing process...&#10;Ollama service exited with code null&#10;Ollama service crashed, attempting restart...&#10;Ollama service error: INFO:     Started server process [103209]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Database validation failed: External error: RuntimeError: Task was aborted&#10;Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [103260]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Database validation failed: External error: RuntimeError: Task was aborted&#10;Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Server initialization failed: AxiosError [AggregateError]&#10;    at AxiosError.from (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:876:14)&#10;    at RedirectableRequest.handleRequestError (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3156:25)&#10;    at RedirectableRequest.emit (node:events:519:28)&#10;    at eventHandlers.&lt;computed&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/follow-redirects/index.js:49:24)&#10;    at ClientRequest.emit (node:events:519:28)&#10;    at emitErrorEvent (node:_http_client:108:11)&#10;    at Socket.socketErrorListener (node:_http_client:511:5)&#10;    at Socket.emit (node:events:519:28)&#10;    at emitErrorNT (node:internal/streams/destroy:169:8)&#10;    at emitErrorCloseNT (node:internal/streams/destroy:128:3)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async Timeout.initializeServer [as _onTimeout] (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:114:26) {&#10;  code: 'ECONNREFUSED',&#10;  errors: [&#10;    Error: connect ECONNREFUSED ::1:5000&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '::1',&#10;      port: 5000&#10;    },&#10;    Error: connect ECONNREFUSED 127.0.0.1:5000&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '127.0.0.1',&#10;      port: 5000&#10;    }&#10;  ],&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': undefined,&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    baseURL: 'http://localhost:5000',&#10;    method: 'get',&#10;    url: '/health',&#10;    data: undefined&#10;  },&#10;  request: &lt;ref *1&gt; Writable {&#10;    _events: {&#10;      close: undefined,&#10;      error: [Function: handleRequestError],&#10;      prefinish: undefined,&#10;      finish: undefined,&#10;      drain: undefined,&#10;      response: [Function: handleResponse],&#10;      socket: [Function: handleRequestSocket]&#10;    },&#10;    _writableState: WritableState {&#10;      highWaterMark: 16384,&#10;      length: 0,&#10;      corked: 0,&#10;      onwrite: [Function: bound onwrite],&#10;      writelen: 0,&#10;      bufferedIndex: 0,&#10;      pendingcb: 0,&#10;      [Symbol(kState)]: 17580812,&#10;      [Symbol(kBufferedValue)]: null&#10;    },&#10;    _maxListeners: undefined,&#10;    _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/health',&#10;      method: 'GET',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '5000',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/health'&#10;    },&#10;    _ended: true,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 0,&#10;    _requestBodyBuffers: [],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: true,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: false,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: 0,&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'GET /health HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:5000\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'GET',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/health',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:5000/health',&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: AggregateError [ECONNREFUSED]: &#10;      at internalConnectMultiple (node:net:1118:18)&#10;      at afterConnectMultiple (node:net:1685:7) {&#10;    code: 'ECONNREFUSED',&#10;    [errors]: [ [Error], [Error] ]&#10;  }&#10;}&#10;^CShutting down server...&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Database validation failed: External error: RuntimeError: Task was aborted&#10;Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;&#10;Ollama service error: INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;Ollama service error: ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/starlette/routing.py&quot;, line 743, in lifespan&#10;    await receive()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/uvicorn/lifespan/on.py&quot;, line 137, in receive&#10;    return await self.receive_queue.get()&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/usr/lib64/python3.12/asyncio/queues.py&quot;, line 158, in get&#10;    await getter&#10;asyncio.exceptions.CancelledError&#10;&#10;&#10;Ollama service: Startup error: Failed to initialize after 3 attempts: External error: RuntimeError: Task was aborted&#10;&#10;Ollama service exited with code 0&#10;&#10;Process finished with exit code 0&#10;&#10;&#10;&#10;&#10;&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [103754]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Failed to initialize Ollama service: 'OllamaService' object has no attribute '_ensure_clean_database_state'&#10;&#10;Server initialization failed: AxiosError [AggregateError]&#10;    at AxiosError.from (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:876:14)&#10;    at RedirectableRequest.handleRequestError (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3156:25)&#10;    at RedirectableRequest.emit (node:events:519:28)&#10;    at eventHandlers.&lt;computed&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/follow-redirects/index.js:49:24)&#10;    at ClientRequest.emit (node:events:519:28)&#10;    at emitErrorEvent (node:_http_client:108:11)&#10;    at Socket.socketErrorListener (node:_http_client:511:5)&#10;    at Socket.emit (node:events:519:28)&#10;    at emitErrorNT (node:internal/streams/destroy:169:8)&#10;    at emitErrorCloseNT (node:internal/streams/destroy:128:3)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:114:26) {&#10;  code: 'ECONNREFUSED',&#10;  errors: [&#10;    Error: connect ECONNREFUSED ::1:5000&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '::1',&#10;      port: 5000&#10;    },&#10;    Error: connect ECONNREFUSED 127.0.0.1:5000&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '127.0.0.1',&#10;      port: 5000&#10;    }&#10;  ],&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': undefined,&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    baseURL: 'http://localhost:5000',&#10;    method: 'get',&#10;    url: '/health',&#10;    data: undefined&#10;  },&#10;  request: &lt;ref *1&gt; Writable {&#10;    _events: {&#10;      close: undefined,&#10;      error: [Function: handleRequestError],&#10;      prefinish: undefined,&#10;      finish: undefined,&#10;      drain: undefined,&#10;      response: [Function: handleResponse],&#10;      socket: [Function: handleRequestSocket]&#10;    },&#10;    _writableState: WritableState {&#10;      highWaterMark: 16384,&#10;      length: 0,&#10;      corked: 0,&#10;      onwrite: [Function: bound onwrite],&#10;      writelen: 0,&#10;      bufferedIndex: 0,&#10;      pendingcb: 0,&#10;      [Symbol(kState)]: 17580812,&#10;      [Symbol(kBufferedValue)]: null&#10;    },&#10;    _maxListeners: undefined,&#10;    _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/health',&#10;      method: 'GET',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '5000',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/health'&#10;    },&#10;    _ended: true,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 0,&#10;    _requestBodyBuffers: [],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: true,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: false,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: 0,&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'GET /health HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:5000\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'GET',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/health',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:5000/health',&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: AggregateError [ECONNREFUSED]: &#10;      at internalConnectMultiple (node:net:1118:18)&#10;      at afterConnectMultiple (node:net:1685:7) {&#10;    code: 'ECONNREFUSED',&#10;    [errors]: [ [Error], [Error] ]&#10;  }&#10;}&#10;Server is running on http://localhost:3000&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Failed to initialize Ollama service: 'OllamaService' object has no attribute '_ensure_clean_database_state'&#10;&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [103777]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Failed to initialize Ollama service: 'OllamaService' object has no attribute '_ensure_clean_database_state'&#10;&#10;Server initialization failed: AxiosError [AggregateError]&#10;    at AxiosError.from (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:876:14)&#10;    at RedirectableRequest.handleRequestError (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3156:25)&#10;    at RedirectableRequest.emit (node:events:519:28)&#10;    at eventHandlers.&lt;computed&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/follow-redirects/index.js:49:24)&#10;    at ClientRequest.emit (node:events:519:28)&#10;    at emitErrorEvent (node:_http_client:108:11)&#10;    at Socket.socketErrorListener (node:_http_client:511:5)&#10;    at Socket.emit (node:events:519:28)&#10;    at emitErrorNT (node:internal/streams/destroy:169:8)&#10;    at emitErrorCloseNT (node:internal/streams/destroy:128:3)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async Timeout.initializeServer [as _onTimeout] (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:114:26) {&#10;  code: 'ECONNREFUSED',&#10;  errors: [&#10;    Error: connect ECONNREFUSED ::1:5000&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '::1',&#10;      port: 5000&#10;    },&#10;    Error: connect ECONNREFUSED 127.0.0.1:5000&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '127.0.0.1',&#10;      port: 5000&#10;    }&#10;  ],&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': undefined,&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    baseURL: 'http://localhost:5000',&#10;    method: 'get',&#10;    url: '/health',&#10;    data: undefined&#10;  },&#10;  request: &lt;ref *1&gt; Writable {&#10;    _events: {&#10;      close: undefined,&#10;      error: [Function: handleRequestError],&#10;      prefinish: undefined,&#10;      finish: undefined,&#10;      drain: undefined,&#10;      response: [Function: handleResponse],&#10;      socket: [Function: handleRequestSocket]&#10;    },&#10;    _writableState: WritableState {&#10;      highWaterMark: 16384,&#10;      length: 0,&#10;      corked: 0,&#10;      onwrite: [Function: bound onwrite],&#10;      writelen: 0,&#10;      bufferedIndex: 0,&#10;      pendingcb: 0,&#10;      [Symbol(kState)]: 17580812,&#10;      [Symbol(kBufferedValue)]: null&#10;    },&#10;    _maxListeners: undefined,&#10;    _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/health',&#10;      method: 'GET',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '5000',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/health'&#10;    },&#10;    _ended: true,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 0,&#10;    _requestBodyBuffers: [],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: true,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: false,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: 0,&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'GET /health HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:5000\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'GET',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/health',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:5000/health',&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: AggregateError [ECONNREFUSED]: &#10;      at internalConnectMultiple (node:net:1118:18)&#10;      at afterConnectMultiple (node:net:1685:7) {&#10;    code: 'ECONNREFUSED',&#10;    [errors]: [ [Error], [Error] ]&#10;  }&#10;}&#10;^CShutting down server...&#10;Ollama service: Initializing Ollama service...&#10;Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;Failed to initialize Ollama service: 'OllamaService' object has no attribute '_ensure_clean_database_state'&#10;&#10;Ollama service error: INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;Ollama service error: ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/starlette/routing.py&quot;, line 743, in lifespan&#10;    await receive()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/uvicorn/lifespan/on.py&quot;, line 137, in receive&#10;    return await self.receive_queue.get()&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/usr/lib64/python3.12/asyncio/queues.py&quot;, line 158, in get&#10;    await getter&#10;asyncio.exceptions.CancelledError&#10;&#10;&#10;Ollama service: Startup error: Failed to initialize after 3 attempts: 'OllamaService' object has no attribute '_ensure_clean_database_state'&#10;&#10;Ollama service exited with code 0&#10;&#10;Process finished with exit code 0&#10;" />
        <option value="please look at the files and find out why this error is happening" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Starting Ollama service...&#10;Ollama service error: INFO:     Started server process [104650]&#10;INFO:     Waiting for application startup.&#10;&#10;Ollama service error: INFO:__main__:Initializing Ollama service...&#10;INFO:__main__:Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace&#10;&#10;Ollama service error: ERROR:__main__:Database validation failed: External error: RuntimeError: Task was aborted&#10;ERROR:__main__:Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Server initialization failed: AxiosError [AggregateError]&#10;    at AxiosError.from (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:876:14)&#10;    at RedirectableRequest.handleRequestError (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:3156:25)&#10;    at RedirectableRequest.emit (node:events:519:28)&#10;    at eventHandlers.&lt;computed&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/follow-redirects/index.js:49:24)&#10;    at ClientRequest.emit (node:events:519:28)&#10;    at emitErrorEvent (node:_http_client:108:11)&#10;    at Socket.socketErrorListener (node:_http_client:511:5)&#10;    at Socket.emit (node:events:519:28)&#10;    at emitErrorNT (node:internal/streams/destroy:169:8)&#10;    at emitErrorCloseNT (node:internal/streams/destroy:128:3)&#10;    at Axios.request (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/axios/dist/node/axios.cjs:4287:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:114:26) {&#10;  code: 'ECONNREFUSED',&#10;  errors: [&#10;    Error: connect ECONNREFUSED ::1:5000&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '::1',&#10;      port: 5000&#10;    },&#10;    Error: connect ECONNREFUSED 127.0.0.1:5000&#10;        at createConnectionError (node:net:1648:14)&#10;        at afterConnectMultiple (node:net:1678:16) {&#10;      errno: -111,&#10;      code: 'ECONNREFUSED',&#10;      syscall: 'connect',&#10;      address: '127.0.0.1',&#10;      port: 5000&#10;    }&#10;  ],&#10;  config: {&#10;    transitional: {&#10;      silentJSONParsing: true,&#10;      forcedJSONParsing: true,&#10;      clarifyTimeoutError: false&#10;    },&#10;    adapter: [ 'xhr', 'http', 'fetch' ],&#10;    transformRequest: [ [Function: transformRequest] ],&#10;    transformResponse: [ [Function: transformResponse] ],&#10;    timeout: 0,&#10;    xsrfCookieName: 'XSRF-TOKEN',&#10;    xsrfHeaderName: 'X-XSRF-TOKEN',&#10;    maxContentLength: -1,&#10;    maxBodyLength: -1,&#10;    env: { FormData: [Function], Blob: [class Blob] },&#10;    validateStatus: [Function: validateStatus],&#10;    headers: Object [AxiosHeaders] {&#10;      Accept: 'application/json, text/plain, */*',&#10;      'Content-Type': undefined,&#10;      'User-Agent': 'axios/1.7.7',&#10;      'Accept-Encoding': 'gzip, compress, deflate, br'&#10;    },&#10;    baseURL: 'http://localhost:5000',&#10;    method: 'get',&#10;    url: '/health',&#10;    data: undefined&#10;  },&#10;  request: &lt;ref *1&gt; Writable {&#10;    _events: {&#10;      close: undefined,&#10;      error: [Function: handleRequestError],&#10;      prefinish: undefined,&#10;      finish: undefined,&#10;      drain: undefined,&#10;      response: [Function: handleResponse],&#10;      socket: [Function: handleRequestSocket]&#10;    },&#10;    _writableState: WritableState {&#10;      highWaterMark: 16384,&#10;      length: 0,&#10;      corked: 0,&#10;      onwrite: [Function: bound onwrite],&#10;      writelen: 0,&#10;      bufferedIndex: 0,&#10;      pendingcb: 0,&#10;      [Symbol(kState)]: 17580812,&#10;      [Symbol(kBufferedValue)]: null&#10;    },&#10;    _maxListeners: undefined,&#10;    _options: {&#10;      maxRedirects: 21,&#10;      maxBodyLength: Infinity,&#10;      protocol: 'http:',&#10;      path: '/health',&#10;      method: 'GET',&#10;      headers: [Object: null prototype],&#10;      agents: [Object],&#10;      auth: undefined,&#10;      family: undefined,&#10;      beforeRedirect: [Function: dispatchBeforeRedirect],&#10;      beforeRedirects: [Object],&#10;      hostname: 'localhost',&#10;      port: '5000',&#10;      agent: undefined,&#10;      nativeProtocols: [Object],&#10;      pathname: '/health'&#10;    },&#10;    _ended: true,&#10;    _ending: true,&#10;    _redirectCount: 0,&#10;    _redirects: [],&#10;    _requestBodyLength: 0,&#10;    _requestBodyBuffers: [],&#10;    _eventsCount: 3,&#10;    _onNativeResponse: [Function (anonymous)],&#10;    _currentRequest: ClientRequest {&#10;      _events: [Object: null prototype],&#10;      _eventsCount: 7,&#10;      _maxListeners: undefined,&#10;      outputData: [],&#10;      outputSize: 0,&#10;      writable: true,&#10;      destroyed: false,&#10;      _last: true,&#10;      chunkedEncoding: false,&#10;      shouldKeepAlive: true,&#10;      maxRequestsOnConnectionReached: false,&#10;      _defaultKeepAlive: true,&#10;      useChunkedEncodingByDefault: false,&#10;      sendDate: false,&#10;      _removedConnection: false,&#10;      _removedContLen: false,&#10;      _removedTE: false,&#10;      strictContentLength: false,&#10;      _contentLength: 0,&#10;      _hasBody: true,&#10;      _trailer: '',&#10;      finished: true,&#10;      _headerSent: true,&#10;      _closed: false,&#10;      socket: [Socket],&#10;      _header: 'GET /health HTTP/1.1\r\n' +&#10;        'Accept: application/json, text/plain, */*\r\n' +&#10;        'User-Agent: axios/1.7.7\r\n' +&#10;        'Accept-Encoding: gzip, compress, deflate, br\r\n' +&#10;        'Host: localhost:5000\r\n' +&#10;        'Connection: keep-alive\r\n' +&#10;        '\r\n',&#10;      _keepAliveTimeout: 0,&#10;      _onPendingData: [Function: nop],&#10;      agent: [Agent],&#10;      socketPath: undefined,&#10;      method: 'GET',&#10;      maxHeaderSize: undefined,&#10;      insecureHTTPParser: undefined,&#10;      joinDuplicateHeaders: undefined,&#10;      path: '/health',&#10;      _ended: false,&#10;      res: null,&#10;      aborted: false,&#10;      timeoutCb: [Function: emitRequestTimeout],&#10;      upgradeOrConnect: false,&#10;      parser: null,&#10;      maxHeadersCount: null,&#10;      reusedSocket: false,&#10;      host: 'localhost',&#10;      protocol: 'http:',&#10;      _redirectable: [Circular *1],&#10;      [Symbol(shapeMode)]: false,&#10;      [Symbol(kCapture)]: false,&#10;      [Symbol(kBytesWritten)]: 0,&#10;      [Symbol(kNeedDrain)]: false,&#10;      [Symbol(corked)]: 0,&#10;      [Symbol(kOutHeaders)]: [Object: null prototype],&#10;      [Symbol(errored)]: null,&#10;      [Symbol(kHighWaterMark)]: 16384,&#10;      [Symbol(kRejectNonStandardBodyWrites)]: false,&#10;      [Symbol(kUniqueHeaders)]: null&#10;    },&#10;    _currentUrl: 'http://localhost:5000/health',&#10;    [Symbol(shapeMode)]: true,&#10;    [Symbol(kCapture)]: false&#10;  },&#10;  cause: AggregateError [ECONNREFUSED]: &#10;      at internalConnectMultiple (node:net:1118:18)&#10;      at afterConnectMultiple (node:net:1685:7) {&#10;    code: 'ECONNREFUSED',&#10;    [errors]: [ [Error], [Error] ]&#10;  }&#10;}&#10;Server is running on http://localhost:3000&#10;Ollama service error: INFO:__main__:Initializing Ollama service...&#10;&#10;Ollama service error: INFO:__main__:Using data directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data&#10;&#10;Ollama service error: thread 'lance_background_thread' panicked at /root/.cargo/registry/src/index.crates.io-6f17d22bba15001f/bytes-1.5.0/src/bytes.rs:255:9:&#10;range start must not be greater than end: 12884855907 &lt;= 4096&#10;&#10;Ollama service error: ERROR:__main__:Database validation failed: External error: RuntimeError: Task was aborted&#10;ERROR:__main__:Failed to initialize Ollama service: External error: RuntimeError: Task was aborted&#10;&#10;Starting Ollama service...&#10;^CShutting down server...&#10;Ollama service error: Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/venv/lib64/python3.12/site-packages/pydantic/_internal/_generate_schema.py&quot;, line 677, in _resolve_forward_ref&#10;&#10;Ollama service exited with code null&#10;Ollama service crashed, attempting restart...&#10;&#10;Process finished with exit code 0&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Starting Ollama service...&#10;Error starting Ollama service: Error: EISDIR: illegal operation on a directory, unlink '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data/space_training.lance'&#10;    at Object.unlinkSync (node:fs:1884:11)&#10;    at /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:77:28&#10;    at Array.forEach (&lt;anonymous&gt;)&#10;    at startOllamaService (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:75:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:144:9) {&#10;  errno: -21,&#10;  code: 'EISDIR',&#10;  syscall: 'unlink',&#10;  path: '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data/space_training.lance'&#10;}&#10;Retrying in 10 seconds (1/3)...&#10;Error starting Ollama service: Error: EISDIR: illegal operation on a directory, unlink '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data/space_training.lance'&#10;    at Object.unlinkSync (node:fs:1884:11)&#10;    at /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:77:28&#10;    at Array.forEach (&lt;anonymous&gt;)&#10;    at startOllamaService (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:75:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:144:9) {&#10;  errno: -21,&#10;  code: 'EISDIR',&#10;  syscall: 'unlink',&#10;  path: '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data/space_training.lance'&#10;}&#10;Retrying in 10 seconds (2/3)...&#10;^CShutting down server...&#10;&#10;Process finished with exit code 0&#10;" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Starting Ollama service...&#10;Error starting Ollama service: Error: EISDIR: illegal operation on a directory, unlink '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data/space_training.lance'&#10;    at Object.unlinkSync (node:fs:1884:11)&#10;    at /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:77:28&#10;    at Array.forEach (&lt;anonymous&gt;)&#10;    at startOllamaService (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:75:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:144:9) {&#10;  errno: -21,&#10;  code: 'EISDIR',&#10;  syscall: 'unlink',&#10;  path: '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data/space_training.lance'&#10;}&#10;Retrying in 10 seconds (1/3)...&#10;Error starting Ollama service: Error: EISDIR: illegal operation on a directory, unlink '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data/space_training.lance'&#10;    at Object.unlinkSync (node:fs:1884:11)&#10;    at /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:77:28&#10;    at Array.forEach (&lt;anonymous&gt;)&#10;    at startOllamaService (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:75:41)&#10;    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)&#10;    at async initializeServer (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:144:9) {&#10;  errno: -21,&#10;  code: 'EISDIR',&#10;  syscall: 'unlink',&#10;  path: '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/data/space_training.lance'&#10;}&#10;Retrying in 10 seconds (2/3)...&#10;^CShutting down server...&#10;&#10;Process finished with exit code 0&#10;&#10;" />
        <option value="look at the server.js file" />
        <option value="the python service should launch seperately, as a micro service, and the server seperately as ell" />
        <option value="ill manually start the python " />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Cannot connect to Ollama service: Request failed with status code 500&#10;Server is running on http://localhost:3000&#10;Cannot connect to Ollama service: Request failed with status code 500&#10;Cannot connect to Ollama service: Request failed with status code 500&#10;" />
        <option value="Cannot connect to Ollama service: Request failed with status code 500&#10;Server is running on http://localhost:3000&#10;Cannot connect to Ollama service: Request failed with status code 500&#10;" />
        <option value="why is there a fucking start_ollama_service file? dont make random fucking files ill start it from the terminal" />
        <option value=" raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:16:47] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:16:52,117] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:16:52] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:16:57,124] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:16:57] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:02,129] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:02] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:07,135] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:07] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:12,142] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:12] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:17,149] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:17] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:22,154] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:22] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:27,159] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:27] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:32,167] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:32] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:37,170] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:37] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:42,175] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:42] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:47,182] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:47] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:52,188] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:52] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;[2024-11-02 20:17:57,193] ERROR in app: Exception on /initialize [POST]&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 1473, in wsgi_app&#10;    response = self.full_dispatch_request()&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 882, in full_dispatch_request&#10;    rv = self.handle_user_exception(e)&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 880, in full_dispatch_request&#10;    rv = self.dispatch_request()&#10;         ^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 865, in dispatch_request&#10;    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 939, in ensure_sync&#10;    return self.async_to_sync(func)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib/python3.12/site-packages/flask/app.py&quot;, line 960, in async_to_sync&#10;    raise RuntimeError(&#10;RuntimeError: Install Flask with the 'async' extra in order to use async views.&#10;127.0.0.1 - - [02/Nov/2024 20:17:57] &quot;POST /initialize HTTP/1.1&quot; 500 -&#10;" />
        <option value="wouldnt fastap be better here?" />
        <option value="we dont need fast api and flask" />
        <option value="no! i want it to be a seperate service damn" />
        <option value="no mother fucker stop wasting my credits, the ollama should be a microservice with its own endpoints that connect to the server.js" />
        <option value="Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Ollama service initialization complete.&#10;INFO:     127.0.0.1:57968 - &quot;POST /initialize HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:50908 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;" />
        <option value="how long does llama take to complete" />
        <option value="i man to initialize the llama" />
        <option value="does this code actually initialize llmama" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Cannot connect to Ollama service: Request failed with status code 500&#10;Server is running on http://localhost:3000&#10;Cannot connect to Ollama service: Request failed with status code 500&#10;Cannot connect to Ollama service: Request failed with status code 500&#10;&#10;&#10;v) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [108119]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:49956 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:49956 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:48508 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:55470 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:55488 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:49694 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [108119]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [109153]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:36142 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:36154 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:35818 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:48348 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;&#10;" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [108119]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:49956 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:49956 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:48508 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:55470 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:55488 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:49694 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [108119]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [109153]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:36142 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:36154 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:35818 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:48348 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [109153]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [109371]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:38056 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:38056 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:38068 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:38068 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:41850 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:41850 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:41856 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:41856 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:35590 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:35590 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:42296 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:42296 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:42304 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:42304 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;^CINFO:     127.0.0.1:50232 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;" />
        <option value="it should start initializing regardless of server.js" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [108119]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:49956 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:49956 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:48508 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:55470 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;Error in chat endpoint: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:55488 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:49694 - &quot;GET /random-question HTTP/1.1&quot; 404 Not Found&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [108119]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [109153]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:36142 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:36154 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:35818 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;INFO:     127.0.0.1:48348 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [109153]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [109371]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:38056 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:38056 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:38068 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:38068 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:41850 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:41850 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:41856 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:41856 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:35590 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:35590 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:42296 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:42296 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:42304 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:42304 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;^CINFO:     127.0.0.1:50232 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [109371]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [109778]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:45628 - &quot;GET / HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:45628 - &quot;GET /favicon.ico HTTP/1.1&quot; 404 Not Found&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [109778]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [110003]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;^[[Ai^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [110003]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [110102]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Request error during chat: 500 Server Error: Internal Server Error for url: http://localhost:11434/api/generate&#10;Initialization failed: Model test failed: Failed to get response from Ollama: 500 Server Error: Internal Server Error for url: http://localhost:11434/api/generate&#10;Initial startup initialization failed: Model test failed: Failed to get response from Ollama: 500 Server Error: Internal Server Error for url: http://localhost:11434/api/generate&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [109371]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:38056 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:38056 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:38068 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:38068 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:41850 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:41850 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:41856 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:41856 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:35590 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:35590 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:42296 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:42296 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;INFO:     127.0.0.1:42304 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Initializing Ollama service...&#10;Default model llama2 not found, downloading...&#10;Starting to pull model llama2. This may take several minutes...&#10;Model llama2 pull completed.&#10;Initialization failed: Model test failed: Ollama service is not initialized. Please wait for initialization to complete.&#10;INFO:     127.0.0.1:42304 - &quot;POST /initialize HTTP/1.1&quot; 500 Internal Server Error&#10;^CINFO:     127.0.0.1:50232 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [109371]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [109778]&#10;INFO:     Waiting for application startup.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:45628 - &quot;GET / HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:45628 - &quot;GET /favicon.ico HTTP/1.1&quot; 404 Not Found&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ cd webapps&#10;bash: cd: webapps: No such file or directory&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck$ cd webapp&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp$ cd services&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [110258]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Initialization failed: 'OllamaService' object has no attribute 'check_ollama_running'&#10;Initial startup initialization failed: 'OllamaService' object has no attribute 'check_ollama_running'&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [110361]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Initialization failed: 'OllamaService' object has no attribute 'pull_model'&#10;Initial startup initialization failed: 'OllamaService' object has no attribute 'pull_model'&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;" />
        <option value="Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: 500 Server Error: Internal Server Error for url: http://localhost:11434/api/generate&#10;Initial startup initialization failed: Model test failed: 500 Server Error: Internal Server Error for url: http://localhost:11434/api/generate&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit" />
        <option value="can you fix up my index and css some" />
        <option value="take another look at the formating" />
        <option value="ry harder to make the formatting better" />
        <option value="ok ideally instead of just chatting, i want to use the lancedb database in the data folder, inside of which is a table with questions for the llm to ask the user and judge their answer" />
        <option value="ok i want the llm to ask questions from the lancedb in the project" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:221: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [124645]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;" />
        <option value="the table is called space_training" />
        <option value="emew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:221: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [124645]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;^Z&#10;[1]+  Stopped                 python ollama.py&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:221: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [124696]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:221: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [124715]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ ^C&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:196: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [124767]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1311, in mkdir&#10;    os.mkdir(self, mode)&#10;FileNotFoundError: [Errno 2] No such file or directory: '/webapp/data'&#10;&#10;During handling of the above exception, another exception occurred:&#10;&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 13, in &lt;module&gt;&#10;    db = lancedb.connect('/webapp/data')&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/__init__.py&quot;, line 115, in connect&#10;    return LanceDBConnection(uri, read_consistency_interval=read_consistency_interval)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/db.py&quot;, line 339, in __init__&#10;    Path(uri).mkdir(parents=True, exist_ok=True)&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1315, in mkdir&#10;    self.parent.mkdir(parents=True, exist_ok=True)&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1311, in mkdir&#10;    os.mkdir(self, mode)&#10;PermissionError: [Errno 13] Permission denied: '/webapp'&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1311, in mkdir&#10;    os.mkdir(self, mode)&#10;FileNotFoundError: [Errno 2] No such file or directory: '/webapp/data'&#10;&#10;During handling of the above exception, another exception occurred:&#10;&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 13, in &lt;module&gt;&#10;    db = lancedb.connect('/webapp/data')&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/__init__.py&quot;, line 115, in connect&#10;    return LanceDBConnection(uri, read_consistency_interval=read_consistency_interval)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/db.py&quot;, line 339, in __init__&#10;    Path(uri).mkdir(parents=True, exist_ok=True)&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1315, in mkdir&#10;    self.parent.mkdir(parents=True, exist_ok=True)&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1311, in mkdir&#10;    os.mkdir(self, mode)&#10;PermissionError: [Errno 13] Permission denied: '/webapp'&#10;(venv) barthmalemew@ladmin:~/WebstormP" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1311, in mkdir&#10;    os.mkdir(self, mode)&#10;FileNotFoundError: [Errno 2] No such file or directory: '/webapp/data'&#10;&#10;During handling of the above exception, another exception occurred:&#10;&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 13, in &lt;module&gt;&#10;    db = lancedb.connect('/webapp/data')&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/__init__.py&quot;, line 115, in connect&#10;    return LanceDBConnection(uri, read_consistency_interval=read_consistency_interval)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/db.py&quot;, line 339, in __init__&#10;    Path(uri).mkdir(parents=True, exist_ok=True)&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1315, in mkdir&#10;    self.parent.mkdir(parents=True, exist_ok=True)&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1311, in mkdir&#10;    os.mkdir(self, mode)&#10;PermissionError: [Errno 13] Permission denied: '/webapp'&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="whats wrong with this" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1311, in mkdir&#10;    os.mkdir(self, mode)&#10;FileNotFoundError: [Errno 2] No such file or directory: '/webapp/data'&#10;&#10;During handling of the above exception, another exception occurred:&#10;&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 13, in &lt;module&gt;&#10;    db = lancedb.connect('/webapp/data')&#10;         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/__init__.py&quot;, line 115, in connect&#10;    return LanceDBConnection(uri, read_consistency_interval=read_consistency_interval)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/venv/lib64/python3.12/site-packages/lancedb/db.py&quot;, line 339, in __init__&#10;    Path(uri).mkdir(parents=True, exist_ok=True)&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1315, in mkdir&#10;    self.parent.mkdir(parents=True, exist_ok=True)&#10;  File &quot;/usr/lib64/python3.12/pathlib.py&quot;, line 1311, in mkdir&#10;    os.mkdir(self, mode)&#10;PermissionError: [Errno 13] Permission denied: '/webapp'&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ ^C&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:305: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [125930]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:305: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [125953]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="look at the file" />
        <option value="fix the files" />
        <option value="(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:331: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;Starting server on port 5002&#10;INFO:     Started server process [126406]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Model test error: Model test failed with status 500: {&quot;error&quot;:&quot;model requires more system memory (8.4 GiB) than is available (7.6 GiB)&quot;}&#10;Initialization failed: Model test failed: Model test failed with status 500: {&quot;error&quot;:&quot;model requires more system memory (8.4 GiB) than is available (7.6 GiB)&quot;}&#10;Initialization failed: Model test failed: Model test failed with status 500: {&quot;error&quot;:&quot;model requires more system memory (8.4 GiB) than is available (7.6 GiB)&quot;}&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5002 (Press CTRL+C to quit)&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [126406]&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ ^C&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:196: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [127015]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:196: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [127018]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="nope dont chnage the model" />
        <option value="ok what if i want to make a database of question for the model to ask the user" />
        <option value="lets avoid lance db, ill give you free rain to choose the best technology for the job" />
        <option value="would mongo db be viable alternaative" />
        <option value="not worth it at the moment" />
        <option value="Mechanical Problem: You are in the space ship doing maintenence and you notice your oxygen tanks are leaking, what do you do in this situation?&#10;[5:07 PM]&#10;Navigational problem: What would the steps be when preparing to make a landing on mars?                                                                              Resource Management: How would you go about when preparing food in zero gravity?&#10; here are the sample questions" />
        <option value="ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.&#10;llama-index-core 0.11.21 requires pydantic&lt;3.0.0,&gt;=2.7.0, but you have pydantic 2.5.2 which is incompatible.&#10;Successfully installed dnspython-2.7.0 motor-3.3.2 pydantic-2.5.2 pydantic-core-2.14.5 pymongo-4.6.1&#10;(venv) barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="we are doing sql lite" />
        <option value="remove everything related to mongo db" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:291: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 382, in &lt;module&gt;&#10;    import uvicorn&#10;ModuleNotFoundError: No module named 'uvicorn'&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="ok i need to make a simple sql database that holds questions, these questions will be presented tot he user where they will be asked to answer them while the llama model proctors them" />
        <option value="Mechanical Problem: You are in the space ship doing maintenence and you notice your oxygen tanks are leaking, what do you do in this situation?&#10;[5:07 PM]&#10;Navigational problem: What would the steps be when preparing to make a landing on mars?                                                                              Resource Management: How would you go about when preparing food in zero gravity? here are some of the example questions" />
        <option value="why is ollama.py giving errors" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 438&#10;    return question if question else {&quot;Here is the continued modified `webapp/services/ollama.py` file:&#10;                                      ^&#10;SyntaxError: unterminated string literal (detected at line 438)&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:354: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="the ollama.py is giving a lot of errors" />
        <option value="theres still errors all over th eproject" />
        <option value="dude ollama_service is gone can you not see that" />
        <option value="settings and database mamanger also dont exist, also is config.py even used for anything or just junk" />
        <option value="the databse should just run through the server.js" />
        <option value="obiouslt config.py should exist, like i said the database is used by the jerver.js file to go to the front end" />
        <option value="no dumbass there shouldnt be a config.py" />
        <option value="all the database stuff should go through server.js and nothing the fuck else" />
        <option value="youre wasting credits on purpose, the database, with questions should be accessed by script.js and server.js dont make up new file or hallucentate ones" />
        <option value="&lt;!DOCTYPE html&gt;&#10;&lt;html lang=&quot;en&quot;&gt;&#10;&lt;head&gt;&#10;    &lt;!-- &#10;    NASA Space Duck Chat Interface&#10;    Main chat interface for space training simulation&#10;    Updated with modern NASA styling&#10;    --&gt;&#10;    &lt;meta charset=&quot;UTF-8&quot;&gt;&#10;    &lt;title&gt;NASA Space Duck&lt;/title&gt;&#10;    &lt;link rel=&quot;stylesheet&quot; href=&quot;styles.css&quot;&gt;&#10;&lt;/head&gt;&#10;&lt;body&gt;&#10;    &lt;h1&gt;NASA Space Duck&lt;/h1&gt;&#10;    &lt;div class=&quot;block&quot; id=&quot;rotate2D&quot;&gt;&lt;/div&gt;&#10;    &lt;div class=&quot;nasa-logo&quot;&gt;&lt;/div&gt;&#10;    &lt;div class=&quot;chat-container&quot;&gt;&#10;        &lt;div class=&quot;chat-messages&quot; id=&quot;chatMessages&quot;&gt;&#10;            &lt;div class=&quot;message bot-message&quot;&gt;&#10;                Welcome to NASA's Virtual Training Portal. I'm your AI instructor, ready to guide you through space science, &#10;                astronaut training, and space exploration. How can I assist you today?&#10;            &lt;/div&gt;&#10;        &lt;/div&gt;&#10;        &lt;div class=&quot;chat-input-container&quot;&gt;&#10;            &lt;textarea &#10;                id=&quot;chatInput&quot; &#10;                placeholder=&quot;Ask a question about space...&quot;&#10;                rows=&quot;3&quot;&#10;                cols=&quot;50&quot;&#10;                aria-label=&quot;Chat input&quot;&#10;            &gt;&lt;/textarea&gt;&#10;            &lt;button id=&quot;sendButton&quot;&gt;Send&lt;/button&gt;&#10;        &lt;/div&gt;&#10;    &lt;/div&gt;&#10;    &lt;script src=&quot;scripts.js&quot;&gt;&lt;/script&gt;&#10;&lt;/body&gt;&#10;&lt;/html&gt;&#10;&#10;the index.html chatbox should have this functionality" />
        <option value="i like the previous lay out though!!!! i just wanted the chatbox to work like this not erase all of the other functions" />
        <option value="sigh the chat box is not function at all as it should, its randomly giving questions from the database when there should be a drop down box to select questions by cataory this also means the llamama communication is broken" />
        <option value="now neither the questions or llama chatbox work" />
        <option value="s/lib/router/index.js:284:15&#10;    at param (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:365:14)&#10;    at param (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:376:14)&#10;    at Function.process_params (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:421:3) {&#10;  code: 'SQLITE_ERROR'&#10;}&#10;Detailed chat error: {&#10;  detail: &quot;Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'\n&quot; +&#10;    'For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500'&#10;}&#10;Detailed chat error: {&#10;  detail: &quot;Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'\n&quot; +&#10;    'For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500'&#10;}&#10;Detailed chat error: {&#10;  detail: &quot;Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'\n&quot; +&#10;    'For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500'&#10;}&#10;Detailed chat error: {&#10;  detail: &quot;Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'\n&quot; +&#10;    'For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500'&#10;}&#10;Detailed chat error: {&#10;  detail: &quot;Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'\n&quot; +&#10;    'For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500'&#10;}&#10;Detailed chat error: {&#10;  detail: &quot;Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'\n&quot; +&#10;    'For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500'&#10;}&#10;Detailed chat error: {&#10;  detail: &quot;Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'\n&quot; +&#10;    'For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500'&#10;}&#10;Error fetching question: SqliteError: near &quot;WHERE&quot;: syntax error&#10;    at Database.prepare (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/better-sqlite3/lib/methods/wrappers.js:5:21)&#10;    at /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:150:18&#10;    at Layer.handle [as handle_request] (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/layer.js:95:5)&#10;    at next (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/route.js:149:13)&#10;    at Route.dispatch (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/route.js:119:3)&#10;    at Layer.handle [as handle_request] (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/layer.js:95:5)&#10;    at /home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:284:15&#10;    at param (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:365:14)&#10;    at param (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:376:14)&#10;    at Function.process_params (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:421:3) {&#10;  code: 'SQLITE_ERROR'&#10;}&#10;Error fetching question: SqliteError: near &quot;WHERE&quot;: syntax error&#10;    at Database.prepare (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/better-sqlite3/lib/methods/wrappers.js:5:21)&#10;    at /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:150:18&#10;    at Layer.handle [as handle_request] (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/layer.js:95:5)&#10;    at next (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/route.js:149:13)&#10;    at Route.dispatch (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/route.js:119:3)&#10;    at Layer.handle [as handle_request] (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/layer.js:95:5)&#10;    at /home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:284:15&#10;    at param (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:365:14)&#10;    at param (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:376:14)&#10;    at Function.process_params (/home/barthmalemew/WebstormProjects/SpaceDuck/node_modules/express/lib/router/index.js:421:3) {&#10;  code: 'SQLITE_ERROR'&#10;}&#10;&#10;" />
        <option value="you broke it" />
        <option value="fucking idiot im saying your changes to ollama.py broke the model. " />
        <option value="make sure ollama.py actually shuts down the process when it stops" />
        <option value="        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [154714]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;Initialization failed: Model test failed: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'&#10;For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [154714]&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:197: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [154890]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: Server disconnected without sending a response.&#10;Initialization failed: Model test failed: Server disconnected without sending a response.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;&#10;" />
        <option value="INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:55306 - &quot;GET /status HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:55320 - &quot;GET /status HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:36548 - &quot;GET /status HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:36560 - &quot;GET /status HTTP/1.1&quot; 404 Not Found&#10;&#10;" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 195, in &lt;module&gt;&#10;    app = FastAPI(lifespan=lifespan)&#10;                           ^^^^^^^^&#10;NameError: name 'lifespan' is not defined&#10;" />
        <option value="you keep adding random shit to a program that worked before when its the server.js changes that broke it" />
        <option value="but the ollama.py is still broken" />
        <option value="just add proper process killing to ollama.py" />
        <option value="sigh nope try again" />
        <option value="make sure ollama.py closes completely when it stops" />
        <option value="sigh those changes did not work &#10;&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:234: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:242: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;shutdown&quot;)&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 264, in &lt;module&gt;&#10;    async def chat(request: ChatRequest):&#10;                            ^^^^^^^^^^^&#10;NameError: name 'ChatRequest' is not defined&#10;barthmalemew@ladmin:~/W" />
        <option value="i want the port of ollama.py to terminate properly when the microservice is closed" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 234, in &lt;module&gt;&#10;    async def chat(request: ChatRequest):&#10;                            ^^^^^^^^^^^&#10;NameError: name 'ChatRequest' is not defined&#10;" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:196: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [3311]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: &#10;Initialization failed: Model test failed: &#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [3311]&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:196: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [3604]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Model test completed successfully&#10;Ollama service initialization complete.&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:51920 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:36850 - &quot;POST /chat HTTP/1.1&quot; 200 OK&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [3604]&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:234: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:242: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;shutdown&quot;)&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 264, in &lt;module&gt;&#10;    async def chat(request: ChatRequest):&#10;                            ^^^^^^^^^^^&#10;NameError: name 'ChatRequest' is not defined&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:234: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:242: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;shutdown&quot;)&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 264, in &lt;module&gt;&#10;    async def chat(request: ChatRequest):&#10;                            ^^^^^^^^^^^&#10;NameError: name 'ChatRequest' is not defined&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:196: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [4341]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: &#10;Initialization failed: Model test failed: &#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;^CINFO:     Shutting down&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;INFO:     Finished server process [4341]&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 234, in &lt;module&gt;&#10;    async def chat(request: ChatRequest):&#10;                            ^^^^^^^^^^^&#10;NameError: name 'ChatRequest' is not defined&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;INFO:     Started server process [4599]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Initializing Ollama service...&#10;Ollama service is running and responding&#10;Default model llama2 not found, downloading...&#10;Successfully pulled model: llama2&#10;Initialization failed: Model test failed: &#10;Cleaning up resources...&#10;ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpx/_transports/default.py&quot;, line 69, in map_httpcore_exceptions&#10;    yield&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpx/_transports/default.py&quot;, line 373, in handle_async_request&#10;    resp = await self._pool.handle_async_request(req)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpcore/_async/connection_pool.py&quot;, line 216, in handle_async_request&#10;    raise exc from None&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpcore/_async/connection_pool.py&quot;, line 196, in handle_async_request&#10;    response = await connection.handle_async_request(&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpcore/_async/connection.py&quot;, line 101, in handle_async_request&#10;    return await self._connection.handle_async_request(request)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpcore/_async/http11.py&quot;, line 143, in handle_async_request&#10;    raise exc&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpcore/_async/http11.py&quot;, line 113, in handle_async_request&#10;    ) = await self._receive_response_headers(**kwargs)&#10;        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpcore/_async/http11.py&quot;, line 186, in _receive_response_headers&#10;    event = await self._receive_event(timeout=timeout)&#10;            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpcore/_async/http11.py&quot;, line 224, in _receive_event&#10;    data = await self._network_stream.read(&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpcore/_backends/anyio.py&quot;, line 32, in read&#10;    with map_exceptions(exc_map):&#10;  File &quot;/usr/lib64/python3.12/contextlib.py&quot;, line 158, in __exit__&#10;    self.gen.throw(value)&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpcore/_exceptions.py&quot;, line 14, in map_exceptions&#10;    raise to_exc(exc) from exc&#10;httpcore.ReadTimeout&#10;&#10;The above exception was the direct cause of the following exception:&#10;&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 120, in _test_model&#10;    response = await client.post(&#10;               ^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpx/_client.py&quot;, line 1892, in post&#10;    return await self.request(&#10;           ^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpx/_client.py&quot;, line 1574, in request&#10;    return await self.send(request, auth=auth, follow_redirects=follow_redirects)&#10;           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpx/_client.py&quot;, line 1661, in send&#10;    response = await self._send_handling_auth(&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpx/_client.py&quot;, line 1689, in _send_handling_auth&#10;    response = await self._send_handling_redirects(&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpx/_client.py&quot;, line 1726, in _send_handling_redirects&#10;    response = await self._send_single_request(request)&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpx/_client.py&quot;, line 1763, in _send_single_request&#10;    response = await transport.handle_async_request(request)&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpx/_transports/default.py&quot;, line 372, in handle_async_request&#10;    with map_httpcore_exceptions():&#10;  File &quot;/usr/lib64/python3.12/contextlib.py&quot;, line 158, in __exit__&#10;    self.gen.throw(value)&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/httpx/_transports/default.py&quot;, line 86, in map_httpcore_exceptions&#10;    raise mapped_exc(message) from exc&#10;httpx.ReadTimeout&#10;&#10;During handling of the above exception, another exception occurred:&#10;&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 693, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/usr/lib64/python3.12/contextlib.py&quot;, line 210, in __aenter__&#10;    return await anext(self.gen)&#10;           ^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 220, in lifespan&#10;    await ollama_service.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 105, in initialize&#10;    await self._test_model()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 138, in _test_model&#10;    raise RuntimeError(f&quot;Model test failed: {str(e)}&quot;)&#10;RuntimeError: Model test failed: &#10;&#10;ERROR:    Application startup failed. Exiting.&#10;" />
        <option value="nope, the program worked perfectly fine originally, i jsut want it to fucking close its port when it ends you should have to add this much shit" />
        <option value="great now i want the questions in the database to be posted to the training content box of the html once i select a catgory " />
        <option value="r/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Uncaught Exception: [Error: SQLITE_CANTOPEN: unable to open database file] {&#10;  errno: 14,&#10;  code: 'SQLITE_CANTOPEN'&#10;}&#10;&#10;Process finished with exit code 1&#10;&#10;" />
        <option value="wrong location the question should appear Training conent box" />
        <option value="it should go in the presentation box" />
        <option value="ok i want the front end to remain untouched, i want server.js to do a better job of getting stuff from the database for it" />
        <option value="you are are theres an existing databse file already in the proect containing he questiosn for the server.js to fetch? you dont have to create a whole databse.js file" />
        <option value="again there shouldnt be a database.js" />
        <option value="my html and css are still differnent on different screen sizes" />
        <option value="the htmls and css appear different at different scree resolution" />
        <option value="i want to add llama index to this project, converting it to a chatbox using the technology" />
        <option value="maybe try again" />
        <option value="try again to turn the existing app into a llamaindex chatbot" />
        <option value="id rather keep llama as the underlying llm" />
        <option value="ok but i want to make a llamaindex chatbot with the existing chatbot app, keeping with llama as the llm used" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ pip install -r requirements.txt&#10;Defaulting to user installation because normal site-packages is not writeable&#10;Collecting fastapi==0.104.1 (from -r requirements.txt (line 1))&#10;  Obtaining dependency information for fastapi==0.104.1 from https://files.pythonhosted.org/packages/f3/4f/0ce34195b63240b6693086496c9bab4ef23999112184399a3e88854c7674/fastapi-0.104.1-py3-none-any.whl.metadata&#10;  Using cached fastapi-0.104.1-py3-none-any.whl.metadata (24 kB)&#10;Collecting uvicorn==0.24.0 (from -r requirements.txt (line 2))&#10;  Obtaining dependency information for uvicorn==0.24.0 from https://files.pythonhosted.org/packages/ed/0c/a9b90a856bbdd75bf71a1dd191af1e9c9ac8a272ed337f7200950c3d3dd4/uvicorn-0.24.0-py3-none-any.whl.metadata&#10;  Using cached uvicorn-0.24.0-py3-none-any.whl.metadata (6.4 kB)&#10;Collecting python-dotenv==1.0.0 (from -r requirements.txt (line 3))&#10;  Obtaining dependency information for python-dotenv==1.0.0 from https://files.pythonhosted.org/packages/44/2f/62ea1c8b593f4e093cc1a7768f0d46112107e790c3e478532329e434f00b/python_dotenv-1.0.0-py3-none-any.whl.metadata&#10;  Using cached python_dotenv-1.0.0-py3-none-any.whl.metadata (21 kB)&#10;Collecting httpx==0.25.1 (from -r requirements.txt (line 4))&#10;  Obtaining dependency information for httpx==0.25.1 from https://files.pythonhosted.org/packages/82/61/a5fca4a1e88e40969bbd0cf0d981f3aa76d5057db160b94f49603fc18740/httpx-0.25.1-py3-none-any.whl.metadata&#10;  Using cached httpx-0.25.1-py3-none-any.whl.metadata (7.1 kB)&#10;ERROR: Ignored the following versions that require a different python version: 0.10.0 Requires-Python &gt;=3.8.1,&lt;3.12; 0.10.1 Requires-Python &gt;=3.8.1,&lt;3.12; 0.10.11 Requires-Python &gt;=3.8.1,&lt;3.12; 0.10.12 Requires-Python &gt;=3.8.1,&lt;3.12; 0.10.3 Requires-Python &gt;=3.8.1,&lt;3.12; 0.10.4 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.43 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.43.post1 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.44 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.45 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.45.post1 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.46 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.47 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.48 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.49 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.50 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.51 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.51.post1 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.52 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.53 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.53.post3 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.54 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.55 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.56 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.57 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.58 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.59 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.61 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.62 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.63.post1 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.63.post2 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.64 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.64.post1 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.65 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.66 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.67 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.68 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.69 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.69.post1 Requires-Python &gt;=3.8.1,&lt;3.12; 0.8.69.post2 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.0 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.0.post1 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.0a1 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.0a2 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.0a3 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.1 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.10 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.10a1 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.10a2 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.11 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.11.post1 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.2 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.3 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.3.post1 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.4 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.5 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.6 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.6.post1 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.6.post2 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.7 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.8 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.8.post1 Requires-Python &gt;=3.8.1,&lt;3.12; 0.9.9 Requires-Python &gt;=3.8.1,&lt;3.12&#10;ERROR: Could not find a version that satisfies the requirement llama-index==0.9.3 (from versions: 0.4.4, 0.4.4.post1, 0.4.4.post2, 0.4.5, 0.4.6, 0.4.7, 0.4.8, 0.4.9, 0.4.10, 0.4.11, 0.4.12, 0.4.13, 0.4.14, 0.4.15, 0.4.16, 0.4.17, 0.4.18, 0.4.19, 0.4.20, 0.4.21, 0.4.22, 0.4.22.post1, 0.4.23, 0.4.24, 0.4.25, 0.4.26, 0.4.27, 0.4.28, 0.4.29, 0.4.30, 0.4.31, 0.4.32, 0.4.33, 0.4.34, 0.4.35, 0.4.35.post1, 0.4.36, 0.4.37, 0.4.38, 0.4.39, 0.4.40, 0.5.0, 0.5.1, 0.5.2, 0.5.3, 0.5.4, 0.5.5, 0.5.6, 0.5.7, 0.5.8, 0.5.9, 0.5.10, 0.5.11, 0.5.12, 0.5.13, 0.5.13.post1, 0.5.15, 0.5.16, 0.5.17, 0.5.17.post1, 0.5.18, 0.5.19, 0.5.20, 0.5.21, 0.5.22, 0.5.23, 0.5.23.post1, 0.5.25, 0.5.26, 0.5.27, 0.6.0a1, 0.6.0a2, 0.6.0a3, 0.6.0a4, 0.6.0a5, 0.6.0a6, 0.6.0a7, 0.6.0, 0.6.1, 0.6.2, 0.6.4, 0.6.5, 0.6.6, 0.6.7, 0.6.8, 0.6.9, 0.6.10, 0.6.10.post1, 0.6.11, 0.6.12, 0.6.13, 0.6.14, 0.6.15, 0.6.16, 0.6.16.post1, 0.6.17, 0.6.18, 0.6.19, 0.6.20, 0.6.21.post1, 0.6.22, 0.6.23, 0.6.24, 0.6.25, 0.6.25.post1, 0.6.26, 0.6.27, 0.6.28, 0.6.29, 0.6.30, 0.6.31, 0.6.32, 0.6.33, 0.6.34, 0.6.34.post1, 0.6.35, 0.6.36, 0.6.37, 0.6.38, 0.6.38.post1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7, 0.7.8, 0.7.9, 0.7.10, 0.7.10.post1, 0.7.11, 0.7.11.post1, 0.7.12, 0.7.13, 0.7.14, 0.7.15, 0.7.16, 0.7.17, 0.7.18, 0.7.19, 0.7.20, 0.7.21, 0.7.22, 0.7.23, 0.7.24.post1, 0.8.0, 0.8.1, 0.8.1.post1, 0.8.2, 0.8.2.post1, 0.8.3, 0.8.4, 0.8.5, 0.8.5.post1, 0.8.5.post2, 0.8.6, 0.8.7, 0.8.8, 0.8.9, 0.8.10, 0.8.10.post1, 0.8.11, 0.8.11.post1, 0.8.11.post2, 0.8.11.post3, 0.8.12, 0.8.13, 0.8.14, 0.8.15, 0.8.16, 0.8.17, 0.8.18, 0.8.19, 0.8.20, 0.8.21, 0.8.22, 0.8.23, 0.8.23.post1, 0.8.24, 0.8.24.post1, 0.8.25, 0.8.26, 0.8.26.post1, 0.8.27, 0.8.28a1, 0.8.28, 0.8.29, 0.8.29.post1, 0.8.30, 0.8.31, 0.8.32, 0.8.33, 0.8.34, 0.8.35, 0.8.36, 0.8.37, 0.8.38, 0.8.39, 0.8.39.post2, 0.8.40, 0.8.41, 0.8.42, 0.9.12a1, 0.9.12a2, 0.9.12a3, 0.9.12a4, 0.9.12a5, 0.9.12a6, 0.9.12, 0.9.13, 0.9.14, 0.9.14.post1, 0.9.14.post2, 0.9.14.post3, 0.9.15, 0.9.15.post1, 0.9.15.post2, 0.9.16.dev1, 0.9.16.dev2, 0.9.16, 0.9.16.post1, 0.9.17.dev1, 0.9.17, 0.9.18, 0.9.19, 0.9.20, 0.9.21, 0.9.22, 0.9.23, 0.9.24, 0.9.25a1, 0.9.25a2, 0.9.25, 0.9.25.post1, 0.9.26, 0.9.27, 0.9.28, 0.9.28.post1, 0.9.28.post2, 0.9.29, 0.9.30, 0.9.31, 0.9.32, 0.9.33a2, 0.9.33a3, 0.9.33a4, 0.9.33a5, 0.9.33a6, 0.9.33, 0.9.34, 0.9.35, 0.9.36, 0.9.37, 0.9.37.post1, 0.9.38, 0.9.39, 0.9.40, 0.9.41, 0.9.42, 0.9.42.post1, 0.9.42.post2, 0.9.43, 0.9.44, 0.9.45, 0.9.45.post1, 0.9.46, 0.9.47, 0.9.48, 0.10.5a1, 0.10.5, 0.10.6, 0.10.7, 0.10.8, 0.10.9, 0.10.10, 0.10.13, 0.10.13.post1, 0.10.14, 0.10.15, 0.10.16, 0.10.17, 0.10.18, 0.10.19, 0.10.20, 0.10.22, 0.10.23, 0.10.24, 0.10.25, 0.10.26, 0.10.27, 0.10.28, 0.10.29, 0.10.30, 0.10.31, 0.10.32, 0.10.33, 0.10.34, 0.10.35, 0.10.36, 0.10.37, 0.10.38, 0.10.39, 0.10.40, 0.10.41, 0.10.42, 0.10.43, 0.10.44, 0.10.45, 0.10.45.post1, 0.10.46, 0.10.47, 0.10.48, 0.10.48.post1, 0.10.49, 0.10.50, 0.10.51, 0.10.52, 0.10.53, 0.10.54, 0.10.54.post1, 0.10.55, 0.10.56, 0.10.57, 0.10.58, 0.10.59, 0.10.61, 0.10.62, 0.10.63, 0.10.64, 0.10.65, 0.10.67.post1, 0.10.68, 0.11.0, 0.11.1, 0.11.2, 0.11.3, 0.11.4, 0.11.5, 0.11.6, 0.11.7, 0.11.8, 0.11.9, 0.11.10, 0.11.11, 0.11.12, 0.11.13, 0.11.14, 0.11.15, 0.11.16, 0.11.17, 0.11.18, 0.11.19, 0.11.20, 0.11.21)&#10;ERROR: No matching distribution found for llama-index==0.9.3&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="Cannot find reference 'HuggingFaceEmbeddings' in 'imported module llama_index.embeddings'" />
        <option value="Cannot find reference 'VectorStoreIndex' in 'imported module llama_index'" />
        <option value="Cannot find reference 'base' in 'imported module llama_index.llms'" />
        <option value="Cannot find reference 'HuggingFaceEmbeddings' in '__init__.py'" />
        <option value="Cannot find reference 'base' in 'imported module llama_index.llms'" />
        <option value="Cannot find reference 'base_query_engine' in '__init__.py'" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 136, in &lt;module&gt;&#10;    ollama_service = OllamaService()&#10;                     ^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 28, in __init__&#10;    self.llama_index = LlamaIndexService()&#10;                       ^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/llama_index_service.py&quot;, line 36, in __init__&#10;    self.llm = OllamaLLM()&#10;               ^^^^^^^^^^^&#10;TypeError: Can't instantiate abstract class OllamaLLM without an implementation for abstract methods '_aquery', '_get_prompt_modules', '_query'&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 4, in &lt;module&gt;&#10;    from llama_index_service import LlamaIndexService&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/llama_index_service.py&quot;, line 3, in &lt;module&gt;&#10;    from llama_index.llms.base import LLM&#10;ModuleNotFoundError: No module named 'llama_index.llms.base'&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 4, in &lt;module&gt;&#10;    from llama_index_service import LlamaIndexService&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/llama_index_service.py&quot;, line 3, in &lt;module&gt;&#10;    from llama_index.core.base.llms import LLM&#10;ImportError: cannot import name 'LLM' from 'llama_index.core.base.llms' (/home/barthmalemew/.local/lib/python3.12/site-packages/llama_index/core/base/llms/__init__.py)&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 136, in &lt;module&gt;&#10;    ollama_service = OllamaService()&#10;                     ^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 28, in __init__&#10;    self.llama_index = LlamaIndexService()&#10;                       ^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/llama_index_service.py&quot;, line 41, in __init__&#10;    self.llm = OllamaLLM()&#10;               ^^^^^^^^^^^&#10;TypeError: Can't instantiate abstract class OllamaLLM without an implementation for abstract methods 'achat', 'acomplete', 'astream_chat', 'astream_complete', 'chat', 'stream_chat'&#10;" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 136, in &lt;module&gt;&#10;    ollama_service = OllamaService()&#10;                     ^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 28, in __init__&#10;    self.llama_index = LlamaIndexService()&#10;                       ^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/llama_index_service.py&quot;, line 85, in __init__&#10;    self.llm = OllamaLLM()&#10;               ^^^^^^^^^^^&#10;TypeError: Can't instantiate abstract class OllamaLLM without an implementation for abstract method 'complete'&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:284: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [17369]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Downloading model llama2...&#10;Error initializing LlamaIndex service: Can't instantiate abstract class OllamaLLM without an implementation for abstract methods 'astream_chat', 'astream_complete', 'stream_chat', 'stream_complete'&#10;Initialization failed: Can't instantiate abstract class OllamaLLM without an implementation for abstract methods 'astream_chat', 'astream_complete', 'stream_chat', 'stream_complete'&#10;ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 693, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 569, in __aenter__&#10;    await self._router.startup()&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 670, in startup&#10;    await handler()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 287, in startup_event&#10;    await ollama_service.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 206, in initialize&#10;    await self.llama_index.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 103, in initialize&#10;    self.llm = OllamaLLM(model=self.model)&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;TypeError: Can't instantiate abstract class OllamaLLM without an implementation for abstract methods 'astream_chat', 'astream_complete', 'stream_chat', 'stream_complete'&#10;&#10;ERROR:    Application startup failed. Exiting.&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="Unresolved reference 'Generator'" />
        <option value="rthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:353: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [17744]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Downloading model llama2...&#10;Error initializing LlamaIndex service: &quot;OllamaLLM&quot; object has no field &quot;base_url&quot;&#10;Initialization failed: &quot;OllamaLLM&quot; object has no field &quot;base_url&quot;&#10;ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 693, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 569, in __aenter__&#10;    await self._router.startup()&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 670, in startup&#10;    await handler()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 356, in startup_event&#10;    await ollama_service.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 275, in initialize&#10;    await self.llama_index.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 172, in initialize&#10;    self.llm = OllamaLLM(model=self.model)&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 23, in __init__&#10;    self.base_url = base_url&#10;    ^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/pydantic/main.py&quot;, line 884, in __setattr__&#10;    raise ValueError(f'&quot;{self.__class__.__name__}&quot; object has no field &quot;{name}&quot;')&#10;ValueError: &quot;OllamaLLM&quot; object has no field &quot;base_url&quot;&#10;&#10;ERROR:    Application startup failed. Exiting.&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="emew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:356: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [17863]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Downloading model llama2...&#10;modules.json: 100%|████████████████████| 349/349 [00:00&lt;00:00, 3.81MB/s]&#10;config_sentence_transformers.json: 100%|█| 116/116 [00:00&lt;00:00, 1.29MB/&#10;README.md: 100%|███████████████████| 10.7k/10.7k [00:00&lt;00:00, 48.7MB/s]&#10;sentence_bert_config.json: 100%|██████| 53.0/53.0 [00:00&lt;00:00, 296kB/s]&#10;config.json: 100%|█████████████████████| 612/612 [00:00&lt;00:00, 3.57MB/s]&#10;model.safetensors: 100%|███████████| 90.9M/90.9M [00:02&lt;00:00, 30.4MB/s]&#10;tokenizer_config.json: 100%|███████████| 350/350 [00:00&lt;00:00, 2.90MB/s]&#10;vocab.txt: 100%|█████████████████████| 232k/232k [00:00&lt;00:00, 4.00MB/s]&#10;tokenizer.json: 100%|████████████████| 466k/466k [00:00&lt;00:00, 14.2MB/s]&#10;special_tokens_map.json: 100%|█████████| 112/112 [00:00&lt;00:00, 1.00MB/s]&#10;/home/barthmalemew/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884&#10;  warnings.warn(&#10;1_Pooling/config.json: 100%|███████████| 190/190 [00:00&lt;00:00, 1.56MB/s]&#10;Error initializing LlamaIndex service: ServiceContext is deprecated. Use llama_index.settings.Settings instead, or pass in modules to local functions/methods/interfaces.&#10;See the docs for updated usage/migration: &#10;https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/&#10;Initialization failed: ServiceContext is deprecated. Use llama_index.settings.Settings instead, or pass in modules to local functions/methods/interfaces.&#10;See the docs for updated usage/migration: &#10;https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/&#10;ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 693, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 569, in __aenter__&#10;    await self._router.startup()&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 670, in startup&#10;    await handler()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 359, in startup_event&#10;    await ollama_service.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 278, in initialize&#10;    await self.llama_index.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 182, in initialize&#10;    self.service_context = ServiceContext.from_defaults(&#10;                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/llama_index/core/service_context.py&quot;, line 31, in from_defaults&#10;    raise ValueError(&#10;ValueError: ServiceContext is deprecated. Use llama_index.settings.Settings instead, or pass in modules to local functions/methods/interfaces.&#10;See the docs for updated usage/migration: &#10;https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/&#10;&#10;ERROR:    Application startup failed. Exiting.&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="so it gives empy response,does that mean i need to give it information" />
        <option value="can we read the .txt files now?" />
        <option value="ignore this step just directoly feed the files to the llama index" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Initializing LlamaIndex with documents from: services/data/nasa_docs&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:358: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [24630]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Downloading model llama2...&#10;Found 0 document files&#10;/home/barthmalemew/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884&#10;  warnings.warn(&#10;No documents found in services/data/nasa_docs&#10;Error initializing LlamaIndex service: No documents found in specified directory&#10;Initialization failed: No documents found in specified directory&#10;ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 693, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 569, in __aenter__&#10;    await self._router.startup()&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 670, in startup&#10;    await handler()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 361, in startup_event&#10;    await ollama_service.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 280, in initialize&#10;    await self.llama_index.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 202, in initialize&#10;    raise RuntimeError(&quot;No documents found in specified directory&quot;)&#10;RuntimeError: No documents found in specified directory&#10;&#10;ERROR:    Application startup failed. Exiting.&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ " />
        <option value="dont make new files randomly though " />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Initializing LlamaIndex with documents from: services/data/nasa_docs&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:358: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [24630]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Downloading model llama2...&#10;Found 0 document files&#10;/home/barthmalemew/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884&#10;  warnings.warn(&#10;No documents found in services/data/nasa_docs&#10;Error initializing LlamaIndex service: No documents found in specified directory&#10;Initialization failed: No documents found in specified directory&#10;ERROR:    Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 693, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 569, in __aenter__&#10;    await self._router.startup()&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 670, in startup&#10;    await handler()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 361, in startup_event&#10;    await ollama_service.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 280, in initialize&#10;    await self.llama_index.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 202, in initialize&#10;    raise RuntimeError(&quot;No documents found in specified directory&quot;)&#10;RuntimeError: No documents found in specified directory&#10;&#10;ERROR:    Application startup failed. Exiting.&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ " />
        <option value="the files ARE in this directory thoght" />
        <option value="LlamaIndex initialization complete&#10;Ollama service initialization complete&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;Error in OllamaLLM acomplete: Extra data: line 2 column 1 (char 93)&#10;Error in query: 'list' object has no attribute 'text'&#10;INFO:     127.0.0.1:44836 - &quot;POST /chat HTTP/1.1&quot; 200 OK&#10;Error in OllamaLLM acomplete: &#10;Error in query: 'list' object has no attribute 'text'&#10;INFO:     127.0.0.1:35874 - &quot;POST /chat HTTP/1.1&quot; 200 OK&#10;" />
        <option value="@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Current directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services&#10;Initializing LlamaIndex with documents from: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/data/nasa_docs&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:376: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [28739]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Downloading model llama2...&#10;Absolute docs path: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/data/nasa_docs&#10;Found 9 document files&#10;/home/barthmalemew/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884&#10;  warnings.warn(&#10;Loading 9 documents...&#10;Indexed 9 documents from /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/data/nasa_docs&#10;LlamaIndex initialization complete&#10;Ollama service initialization complete&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:46780 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:46780 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:46786 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:46786 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:41778 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:41778 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;" />
        <option value="please only make chnages to ollama.py " />
        <option value="@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Current directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services&#10;Initializing LlamaIndex with documents from: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/data/nasa_docs&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:376: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [28739]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Downloading model llama2...&#10;Absolute docs path: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/data/nasa_docs&#10;Found 9 document files&#10;/home/barthmalemew/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884&#10;  warnings.warn(&#10;Loading 9 documents...&#10;Indexed 9 documents from /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/data/nasa_docs&#10;LlamaIndex initialization complete&#10;Ollama service initialization complete&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:46780 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:46780 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:46786 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:46786 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:41778 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:41778 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;" />
        <option value="i dont want you to change server.js though, you broke ollama.py yourself" />
        <option value="/**&#10; * Main server application for the NASA Space Duck chatbot&#10; * Handles HTTP requests, serves static files, and manages communication with Ollama API&#10; * Implements error handling and server initialization checks&#10; */&#10;const express = require('express');&#10;const path = require('path');&#10;const axios = require('axios');&#10;const sqlite3 = require('sqlite3').verbose();&#10;const db = new sqlite3.Database(path.join(__dirname, 'db', 'questions.db'));&#10;require('dotenv').config();&#10;&#10;const app = express();&#10;const PORT = process.env.SERVER_PORT || 3000;&#10;&#10;// Middleware&#10;app.use(express.json());&#10;app.use(express.static(path.join(__dirname, 'public')));&#10;app.use(express.urlencoded({ extended: true }));&#10;&#10;// Add rate limiting&#10;const rateLimit = require('express-rate-limit');&#10;const limiter = rateLimit({&#10;    windowMs: 15 * 60 * 1000, // 15 minutes&#10;    max: 100 // limit each IP to 100 requests per windowMs&#10;});&#10;&#10;app.use('/api/', limiter);&#10;&#10;// Configure Ollama service URL&#10;const OLLAMA_SERVICE_URL = process.env.OLLAMA_SERVICE_URL || 'http://localhost:5000';&#10;const ollamaClient = axios.create({ baseURL: OLLAMA_SERVICE_URL });&#10;&#10;let serverReady = false;&#10;let initializationError = null;&#10;const MAX_INIT_RETRIES = 5;&#10;let initRetryCount = 0;&#10;&#10;async function initializeServer() {&#10;    try {&#10;        const statusResponse = await ollamaClient.get('/status');&#10;&#10;        if (!statusResponse.data.initialized) {&#10;            console.log(`Initializing Ollama service (Status: ${statusResponse.data.status})...`);&#10;            await ollamaClient.post('/initialize');&#10;        }&#10;&#10;        serverReady = true;&#10;        initializationError = null;&#10;        console.log('Server initialization complete, ready to handle requests');&#10;    } catch (error) {&#10;        initRetryCount++;&#10;        initializationError = error.response?.data?.error || error.message;&#10;        console.error(`Initialization attempt ${initRetryCount} failed:`, initializationError);&#10;&#10;        if (initRetryCount &lt; MAX_INIT_RETRIES) {&#10;            console.log(`Retrying initialization in 5 seconds...`);&#10;            setTimeout(initializeServer, 5000);&#10;        } else {&#10;            console.error('Max initialization retries reached. Server starting in limited mode.');&#10;            serverReady = false;&#10;        }&#10;    }&#10;}&#10;&#10;// Middleware to check if server is ready&#10;const checkServerReady = (req, res, next) =&gt; {&#10;    if (!serverReady) {&#10;        return res.status(503).json({&#10;            error: 'Server is still initializing',&#10;            details: initializationError || 'Please wait a few moments and try again'&#10;        });&#10;    }&#10;    next();&#10;};&#10;&#10;// Enhanced questions endpoint with error handling and caching&#10;const questionCache = new Map();&#10;const CACHE_DURATION = 5 * 60 * 1000; // 5 minutes&#10;&#10;app.get('/api/questions/:category', async (req, res) =&gt; {&#10;    try {&#10;        const category = req.params.category;&#10;        const cacheKey = `category_${category}`;&#10;        const cachedData = questionCache.get(cacheKey);&#10;        &#10;        if (cachedData &amp;&amp; (Date.now() - cachedData.timestamp) &lt; CACHE_DURATION) {&#10;            return res.json(cachedData.data);&#10;        }&#10;&#10;        db.get(&#10;            'SELECT * FROM questions WHERE category = ? ORDER BY RANDOM() LIMIT 1',&#10;            [category],&#10;            (err, row) =&gt; {&#10;                if (err) throw err;&#10;                if (!row) {&#10;                    return res.status(404).json({ &#10;                        error: 'No questions found for this category' &#10;                    });&#10;                }&#10;                questionCache.set(cacheKey, {&#10;                    data: row,&#10;                    timestamp: Date.now()&#10;                });&#10;                res.json(row);&#10;            });&#10;    } catch (error) {&#10;        console.error('Error fetching questions:', error);&#10;        res.status(500).json({&#10;            error: 'Failed to fetch questions',&#10;            details: error.message&#10;        });&#10;    }&#10;});&#10;&#10;// New endpoint to get categories&#10;app.get('/api/categories', async (req, res) =&gt; {&#10;    try {&#10;        db.all(&#10;            'SELECT DISTINCT category FROM questions',&#10;            [],&#10;            (err, rows) =&gt; {&#10;                if (err) throw err;&#10;                res.json(rows.map(row =&gt; row.category));&#10;            }&#10;        );&#10;    } catch (error) {&#10;        console.error('Error fetching categories:', error);&#10;        res.status(500).json({&#10;            error: 'Failed to fetch categories',&#10;            details: error.message&#10;        });&#10;    }&#10;});&#10;&#10;// New endpoint to get random question&#10;app.get('/api/random-question', async (req, res) =&gt; {&#10;    try {&#10;        db.get(&#10;            'SELECT * FROM questions ORDER BY RANDOM() LIMIT 1',&#10;            [],&#10;            (err, row) =&gt; {&#10;                if (err) throw err;&#10;                res.json(row);&#10;            }&#10;        );&#10;    } catch (error) {&#10;        console.error('Error fetching random question:', error);&#10;        res.status(500).json({&#10;            error: 'Failed to fetch random question',&#10;            details: error.message&#10;        });&#10;    }&#10;});&#10;&#10;// Chat endpoint handler&#10;// Processes incoming chat messages and returns AI responses&#10;// Implements error handling for various failure scenarios&#10;app.post('/api/chat', checkServerReady, async (req, res) =&gt; {&#10;    try {&#10;        const response = await ollamaClient.post('/chat', { prompt: req.body.message });&#10;        const botResponse = response.data.response || response.data.text;&#10;        if (!botResponse) {&#10;            throw new Error('Empty response received from Ollama service');&#10;        }&#10;        res.json({ response: botResponse });&#10;    } catch (error) {&#10;        console.error('Detailed chat error:', error.response?.data || error.message);&#10;        let statusCode = 500;&#10;        let errorMessage = 'An unexpected error occurred';&#10;&#10;        if (error.message.includes('too long')) {&#10;            statusCode = 504;&#10;            errorMessage = 'The request took too long to complete. Please try again.';&#10;        } else if (error.message.includes('not running')) {&#10;            statusCode = 503;&#10;            errorMessage = 'The AI service is currently unavailable. Please try again later.';&#10;        }&#10;&#10;        res.status(statusCode).json({&#10;            error: errorMessage,&#10;            details: error.message&#10;        });&#10;    }&#10;});&#10;&#10;// Graceful shutdown handler&#10;process.on('SIGINT', async () =&gt; {&#10;    console.log('Shutting down server...');&#10;    // Clean shutdown of Express server&#10;    server.close();&#10;    process.exit(0);&#10;});&#10;&#10;// Add error handling for uncaught exceptions&#10;process.on('uncaughtException', (error) =&gt; {&#10;    console.error('Uncaught Exception:', error);&#10;    // Attempt graceful shutdown&#10;    process.exit(1);&#10;});&#10;&#10;// Start server initialization&#10;initializeServer().then(() =&gt; {&#10;    app.listen(PORT, () =&gt; {&#10;        console.log(`Server is running on http://localhost:${PORT}`);&#10;    });&#10;});" />
        <option value="i want you to fix the fucking error" />
        <option value="in/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;node:internal/modules/cjs/loader:1228&#10;  throw err;&#10;  ^&#10;&#10;Error: Cannot find module 'sqlite3'&#10;Require stack:&#10;- /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;    at Module._resolveFilename (node:internal/modules/cjs/loader:1225:15)&#10;    at Module._load (node:internal/modules/cjs/loader:1051:27)&#10;    at Module.require (node:internal/modules/cjs/loader:1311:19)&#10;    at require (node:internal/modules/helpers:179:18)&#10;    at Object.&lt;anonymous&gt; (/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js:9:17)&#10;    at Module._compile (node:internal/modules/cjs/loader:1469:14)&#10;    at Module._extensions..js (node:internal/modules/cjs/loader:1548:10)&#10;    at Module.load (node:internal/modules/cjs/loader:1288:32)&#10;    at Module._load (node:internal/modules/cjs/loader:1104:12)&#10;    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:174:12) {&#10;  code: 'MODULE_NOT_FOUND',&#10;  requireStack: [ '/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js' ]&#10;}&#10;&#10;Node.js v20.17.0&#10;&#10;Process finished with exit code 1&#10;" />
        <option value="db  package.json  package-lock.json  public  server.js  services&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp$ cd services&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ ls&#10;data  ollama.py  __pycache__  requirements.txt&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Current directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services&#10;Initializing LlamaIndex with documents from: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/data/nasa_docs&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:367: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [32023]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Downloading model llama2...&#10;Absolute docs path: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/data/nasa_docs&#10;Found 9 document files&#10;/home/barthmalemew/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884&#10;  warnings.warn(&#10;Loading 9 documents...&#10;Indexed 9 documents from /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/data/nasa_docs&#10;LlamaIndex initialization complete&#10;Ollama service initialization complete&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:59338 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:59338 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:59352 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:59352 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:40940 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:40940 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:40952 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:40952 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:54322 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:54322 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:45104 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:45104 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:45108 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:45108 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;&#10;" />
        <option value="INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:43050 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Error in OllamaLLM acomplete: &#10;Error in query: 'list' object has no attribute 'text'&#10;INFO:     127.0.0.1:54138 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;    &#10;" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 261, in &lt;module&gt;&#10;    class InitializationState(Enum):&#10;                              ^^^^&#10;NameError: name 'Enum' is not defined&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 261, in &lt;module&gt;&#10;    class InitializationState(Enum):&#10;                              ^^^^&#10;NameError: name 'Enum' is not defined&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ " />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 269, in &lt;module&gt;&#10;    class OllamaService:&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 350, in OllamaService&#10;    async def chat(self, prompt: str, model: Optional[str] = None) -&gt; Dict[str, str]:&#10;                                                                      ^^^^&#10;NameError: name 'Dict' is not defined. Did you mean: 'dict'?&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
        <option value="&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 693, in lifespan&#10;    async with self.lifespan_context(app) as maybe_state:&#10;               ^^^^^^^^^^^^^^^^^^^^^^^^^^&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 569, in __aenter__&#10;    await self._router.startup()&#10;  File &quot;/home/barthmalemew/.local/lib/python3.12/site-packages/starlette/routing.py&quot;, line 670, in startup&#10;    await handler()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 424, in startup_event&#10;    await ollama_service.initialize()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 293, in initialize&#10;    await self._check_ollama_service()&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 317, in _check_ollama_service&#10;    raise RuntimeError(f&quot;Ollama service check failed: {str(e)}&quot;)&#10;RuntimeError: Ollama service check failed: Model llama2 not ready within timeout&#10;&#10;ERROR:    Application startup failed. Exiting.&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Current directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services&#10;Initializing LlamaIndex with documents from: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/data/nasa_docs&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:421: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [34021]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Starting initialization process...&#10;Downloading model llama2.." />
        <option value="FO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:60130 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:60130 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:60142 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:60142 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Current directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services&#10;Initializing LlamaIndex with documents from: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/data/nasa_docs&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:369: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 396, in &lt;module&gt;&#10;    async def chat(request: ChatRequest):&#10;                            ^^^^^^^^^^^&#10;NameError: name 'ChatRequest' is not defined&#10;barthmalemew@ladm" />
        <option value="Current directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services&#10;Initializing LlamaIndex with documents from: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/data/nasa_docs&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:369: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 396, in &lt;module&gt;&#10;    async def chat(request: ChatRequest):&#10;                            ^^^^^^^^^^^&#10;NameError: name 'ChatRequest' is not defined&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ ^C&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Traceback (most recent call last):&#10;  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 377, in &lt;module&gt;&#10;    app = FastAPI(lifespan=lifespan)&#10;                           ^^^^^^^^&#10;NameError: name 'lifespan' is not defined&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ \" />
        <option value="/usr/bin/node /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/server.js&#10;Initializing Ollama service (Status: ready)...&#10;Initialization attempt 1 failed: Request failed with status code 404&#10;Retrying initialization in 5 seconds...&#10;Server is running on http://localhost:3000&#10;Initializing Ollama service (Status: ready)...&#10;Initialization attempt 2 failed: Request failed with status code 404&#10;Retrying initialization in 5 seconds...&#10;Initializing Ollama service (Status: ready)...&#10;Initialization attempt 3 failed: Request failed with status code 404&#10;Retrying initialization in 5 seconds...&#10;Initializing Ollama service (Status: ready)...&#10;Initialization attempt 4 failed: Request failed with status code 404&#10;Retrying initialization in 5 seconds...&#10;Initializing Ollama service (Status: ready)...&#10;Initialization attempt 5 failed: Request failed with status code 404&#10;Max initialization retries reached. Server starting in limited mode.&#10;&#10;127.0.0.1:36710 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:36710 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:36714 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:36714 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:54842 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:54842 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;&#10;" />
        <option value="LlamaIndex initialization complete&#10;Ollama service initialization complete&#10;INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:42080 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:42080 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;" />
        <option value="INFO:     127.0.0.1:55572 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Error in OllamaLLM acomplete: &#10;Error in query: 'list' object has no attribute 'text'&#10;INFO:     127.0.0.1:43876 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;&#10;" />
        <option value="INFO:     127.0.0.1:55572 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Error in OllamaLLM acomplete: &#10;Error in query: 'list' object has no attribute 'text'&#10;" />
        <option value="well if you remeber we are trying to use llamaindex" />
        <option value="INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;Error in OllamaLLM acomplete: &#10;Error in query: 'list' object has no attribute 'text'&#10;LlamaIndex response type: &lt;class 'str'&gt;&#10;LlamaIndex response: Error processing query: 'list' object has no attribute 'text'&#10;" />
        <option value="INFO:     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;Error in OllamaLLM acomplete: Extra data: line 2 column 1 (char 94)&#10;Error in query: 'list' object has no attribute 'text'&#10;Falling back to direct Ollama chat with enhanced prompt&#10;INFO:     127.0.0.1:48532 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;" />
        <option value="INFO:     127.0.0.1:54726 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;Error in query: 'list' object has no attribute 'text'&#10;Falling back to direct Ollama chat with enhanced prompt" />
        <option value="rror in OllamaLLM acomplete: &#10;Error in query: 'list' object has no attribute 'text'&#10;Falling back to direct Ollama chat with enhanced prompt&#10;INFO:     127.0.0.1:39304 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;&#10;" />
        <option value="Error in query: 'list' object has no attribute 'text'&#10;Falling back to direct Ollama chat with enhanced prompt&#10;INFO:     127.0.0.1:39304 - &quot;POST /chat HTTP/1.1&quot; 500 Internal Server Error&#10;^Z&#10;[1]+  Stopped                 python ollama.py&#10;" />
        <option value="barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ python ollama.py&#10;Current directory: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services&#10;Initializing LlamaIndex with documents from: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/data/nasa_docs&#10;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py:367: DeprecationWarning: &#10;        on_event is deprecated, use lifespan event handlers instead.&#10;&#10;        Read more about it in the&#10;        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).&#10;        &#10;  @app.on_event(&quot;startup&quot;)&#10;INFO:     Started server process [52263]&#10;INFO:     Waiting for application startup.&#10;Starting Ollama service initialization...&#10;Downloading model llama2...&#10;Absolute docs path: /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/data/nasa_docs&#10;Found 9 document files&#10;/home/barthmalemew/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884&#10;  warnings.warn(&#10;Loading 9 documents...&#10;Indexed 9 documents from /home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/data/nasa_docs&#10;LlamaIndex initialization complete&#10;Ollama service initialization complete&#10;INFO:     Application startup complete.&#10;ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): [errno 98] address already in use&#10;INFO:     Waiting for application shutdown.&#10;INFO:     Application shutdown complete.&#10;" />
        <option value="INFO:     127.0.0.1:39198 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:39198 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:39214 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:39214 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:47244 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:47244 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;&#10;" />
        <option value=":     Application startup complete.&#10;INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)&#10;INFO:     127.0.0.1:38522 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:38522 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;INFO:     127.0.0.1:46150 - &quot;GET /status HTTP/1.1&quot; 200 OK&#10;INFO:     127.0.0.1:46150 - &quot;POST /initialize HTTP/1.1&quot; 404 Not Found&#10;" />
        <option value="  File &quot;/home/barthmalemew/WebstormProjects/SpaceDuck/webapp/services/ollama.py&quot;, line 306, in OllamaService&#10;    async def chat(self, prompt: str, model: Optional[str] = None) -&gt; Dict[str, str]:&#10;                                                                      ^^^^&#10;NameError: name 'Dict' is not defined. Did you mean: 'dict'?&#10;barthmalemew@ladmin:~/WebstormProjects/SpaceDuck/webapp/services$ &#10;" />
      </list>
    </option>
    <option name="selectedModel" value="codebuddy:CLAUDE_3.5_SONNET_V7" />
    <option name="ttsEnabled" value="false" />
  </component>
</project>